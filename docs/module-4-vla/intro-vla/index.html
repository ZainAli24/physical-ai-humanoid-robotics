<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-4-vla/intro-vla" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Introduction to VLA Models | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://zainali24.github.io/physical-ai-humanoid-robotics/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://zainali24.github.io/physical-ai-humanoid-robotics/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://zainali24.github.io/physical-ai-humanoid-robotics/docs/module-4-vla/intro-vla"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Introduction to VLA Models | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Learn about Vision-Language-Action (VLA) models and their role in cutting-edge robotic control"><meta data-rh="true" property="og:description" content="Learn about Vision-Language-Action (VLA) models and their role in cutting-edge robotic control"><link data-rh="true" rel="icon" href="/physical-ai-humanoid-robotics/img/book_robotics_favicon_and_book_logo_image00.png"><link data-rh="true" rel="canonical" href="https://zainali24.github.io/physical-ai-humanoid-robotics/docs/module-4-vla/intro-vla"><link data-rh="true" rel="alternate" href="https://zainali24.github.io/physical-ai-humanoid-robotics/docs/module-4-vla/intro-vla" hreflang="en"><link data-rh="true" rel="alternate" href="https://zainali24.github.io/physical-ai-humanoid-robotics/docs/module-4-vla/intro-vla" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Introduction to VLA Models","item":"https://zainali24.github.io/physical-ai-humanoid-robotics/docs/module-4-vla/intro-vla"}]}</script><link rel="stylesheet" href="/physical-ai-humanoid-robotics/assets/css/styles.c6fa2b59.css">
<script src="/physical-ai-humanoid-robotics/assets/js/runtime~main.2e2f2fcc.js" defer="defer"></script>
<script src="/physical-ai-humanoid-robotics/assets/js/main.7678cec2.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/physical-ai-humanoid-robotics/img/book_robotics_favicon_and_book_logo_image00.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/physical-ai-humanoid-robotics/"><div class="navbar__logo"><img src="/physical-ai-humanoid-robotics/img/book_robotics_favicon_and_book_logo_image00.png" alt="Robotics Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/physical-ai-humanoid-robotics/img/book_robotics_favicon_and_book_logo_image00.png" alt="Robotics Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/physical-ai-humanoid-robotics/docs/preface">Book</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-robotics/docs/preface"><span title="Preface" class="linkLabel_WmDU">Preface</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/physical-ai-humanoid-robotics/docs/intro-physical-ai"><span title="Introduction to Physical AI" class="linkLabel_WmDU">Introduction to Physical AI</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/physical-ai-humanoid-robotics/docs/module-1-ros2/ros2-fundamentals"><span title="Module 1: ROS 2 (The Robotic Nervous System)" class="categoryLinkLabel_W154">Module 1: ROS 2 (The Robotic Nervous System)</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-1-ros2/ros2-fundamentals"><span title="ROS 2 Fundamentals" class="linkLabel_WmDU">ROS 2 Fundamentals</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-1-ros2/nodes-topics-services"><span title="Nodes, Topics, and Services" class="linkLabel_WmDU">Nodes, Topics, and Services</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-1-ros2/building-packages-python"><span title="Building ROS 2 Packages with Python" class="linkLabel_WmDU">Building ROS 2 Packages with Python</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/physical-ai-humanoid-robotics/docs/module-2-gazebo/intro-gazebo"><span title="Module 2: Gazebo &amp; Unity (The Digital Twin)" class="categoryLinkLabel_W154">Module 2: Gazebo &amp; Unity (The Digital Twin)</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-2-gazebo/intro-gazebo"><span title="Introduction to Gazebo" class="linkLabel_WmDU">Introduction to Gazebo</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-2-gazebo/unity-robotics"><span title="Unity for Robotics" class="linkLabel_WmDU">Unity for Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-2-gazebo/simulation-best-practices"><span title="Simulation Best Practices" class="linkLabel_WmDU">Simulation Best Practices</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="true" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/intro-isaac"><span title="Module 3: NVIDIA Isaac (The AI-Robot Brain)" class="categoryLinkLabel_W154">Module 3: NVIDIA Isaac (The AI-Robot Brain)</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/intro-isaac"><span title="Introduction to NVIDIA Isaac" class="linkLabel_WmDU">Introduction to NVIDIA Isaac</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-sim"><span title="Isaac Sim for Robotics" class="linkLabel_WmDU">Isaac Sim for Robotics</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-gym-rl"><span title="Reinforcement Learning with Isaac Gym" class="linkLabel_WmDU">Reinforcement Learning with Isaac Gym</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-sdk-overview"><span title="Isaac Platform Architecture" class="linkLabel_WmDU">Isaac Platform Architecture</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/ai-powered-perception"><span title="AI-Powered Perception" class="linkLabel_WmDU">AI-Powered Perception</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/reinforcement-learning"><span title="Advanced RL and Sim-to-Real Transfer" class="linkLabel_WmDU">Advanced RL and Sim-to-Real Transfer</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/physical-ai-humanoid-robotics/docs/module-4-vla/intro-vla"><span title="Module 4: VLA (Vision-Language-Action)" class="categoryLinkLabel_W154">Module 4: VLA (Vision-Language-Action)</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-4-vla/intro-vla"><span title="Introduction to VLA Models" class="linkLabel_WmDU">Introduction to VLA Models</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-4-vla/vla-architectures"><span title="VLA Architectures: RT-1, RT-2, PaLM-E" class="linkLabel_WmDU">VLA Architectures: RT-1, RT-2, PaLM-E</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-4-vla/vla-training-deployment"><span title="Training and Deploying VLA Systems" class="linkLabel_WmDU">Training and Deploying VLA Systems</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-4-vla/voice-to-action"><span title="Voice-to-Action with OpenAI Whisper" class="linkLabel_WmDU">Voice-to-Action with OpenAI Whisper</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-4-vla/cognitive-planning"><span title="Cognitive Planning with LLMs" class="linkLabel_WmDU">Cognitive Planning with LLMs</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/physical-ai-humanoid-robotics/docs/module-4-vla/multimodal-interaction"><span title="Multi-modal Interaction (Speech, Gesture, Vision)" class="linkLabel_WmDU">Multi-modal Interaction (Speech, Gesture, Vision)</span></a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/physical-ai-humanoid-robotics/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 4: VLA (Vision-Language-Action)</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Introduction to VLA Models</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Introduction to VLA Models</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites" translate="no">​</a></h2>
<p>Before diving into this chapter, ensure you have:</p>
<ul>
<li class="">Completed <strong>Module 3</strong> (NVIDIA Isaac Sim and Isaac Gym)</li>
<li class="">Understanding of <strong>transformer architecture</strong> basics (self-attention, encoder-decoder)</li>
<li class="">Familiarity with <strong>multimodal AI</strong> concepts (vision + language models like CLIP, GPT-4V)</li>
<li class="">Basic knowledge of <strong>deep learning frameworks</strong> (PyTorch or TensorFlow)</li>
<li class="">Experience with <strong>robot control fundamentals</strong> from Module 1</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h2>
<p>Vision-Language-Action (VLA) models represent the cutting edge of physical AI, combining computer vision, natural language understanding, and robot control into unified end-to-end systems. Unlike traditional robotics pipelines that separate perception, planning, and control into discrete modules, VLA models learn direct mappings from visual observations and language instructions to robot actions.</p>
<p>This chapter introduces the foundational concepts of VLA models, their evolution from early multimodal systems to state-of-the-art architectures like RT-1, RT-2, and PaLM-E. You&#x27;ll understand why VLA models are transforming robotics, explore real-world use cases from household tasks to industrial automation, and compare VLA approaches to traditional robotics paradigms.</p>
<p><strong>Learning Objectives:</strong></p>
<ul>
<li class="">Understand what VLA models are and how they unify vision, language, and action</li>
<li class="">Identify use cases where VLA models excel compared to traditional approaches</li>
<li class="">Trace the evolution from early robot learning to modern transformer-based VLA systems</li>
<li class="">Compare VLA advantages and limitations against classical robotics methods</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-are-vla-models">What are VLA Models?<a href="#what-are-vla-models" class="hash-link" aria-label="Direct link to What are VLA Models?" title="Direct link to What are VLA Models?" translate="no">​</a></h2>
<p><strong>Vision-Language-Action (VLA)</strong> models are a class of deep learning architectures that process <strong>multimodal inputs</strong> (images from robot cameras + natural language instructions from humans) and directly output <strong>robot actions</strong> (joint positions, gripper commands, end-effector velocities). VLA models are <strong>embodied AI systems</strong>—they don&#x27;t just perceive and reason about the world; they act in it through physical robot bodies.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="core-components">Core Components<a href="#core-components" class="hash-link" aria-label="Direct link to Core Components" title="Direct link to Core Components" translate="no">​</a></h3>
<p>A typical VLA model consists of three integrated components:</p>
<ol>
<li class="">
<p><strong>Vision Encoder</strong>: Processes camera images (RGB, depth, segmentation) to extract visual features. Modern VLA models use Vision Transformers (ViT) or convolutional backbones pretrained on large-scale datasets (ImageNet, CLIP).</p>
</li>
<li class="">
<p><strong>Language Encoder</strong>: Encodes natural language instructions (e.g., &quot;pick up the red mug&quot;) into semantic embeddings. Leverages pretrained language models like BERT, T5, or GPT to understand task goals and constraints.</p>
</li>
<li class="">
<p><strong>Action Decoder</strong>: Maps fused vision-language representations to robot action sequences. Outputs discrete action tokens (for classification) or continuous action vectors (joint angles, end-effector poses).</p>
</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="multimodal-learning">Multimodal Learning<a href="#multimodal-learning" class="hash-link" aria-label="Direct link to Multimodal Learning" title="Direct link to Multimodal Learning" translate="no">​</a></h3>
<p>VLA models learn <strong>joint embeddings</strong> across vision, language, and action spaces. During training, the model observes:</p>
<ul>
<li class=""><strong>Visual observations</strong>: Camera feeds showing the robot&#x27;s workspace</li>
<li class=""><strong>Language instructions</strong>: Task descriptions like &quot;open the drawer&quot; or &quot;sort objects by color&quot;</li>
<li class=""><strong>Demonstrated actions</strong>: Expert teleoperation trajectories or successful task executions</li>
</ul>
<p>By training on thousands of task demonstrations across diverse environments, VLA models learn generalizable policies that can:</p>
<ul>
<li class=""><strong>Follow new language instructions</strong> not seen during training (zero-shot generalization)</li>
<li class=""><strong>Adapt to novel objects</strong> with similar visual properties (object generalization)</li>
<li class=""><strong>Transfer across environments</strong> with different lighting, backgrounds, or clutter (domain robustness)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="embodied-ai">Embodied AI<a href="#embodied-ai" class="hash-link" aria-label="Direct link to Embodied AI" title="Direct link to Embodied AI" translate="no">​</a></h3>
<p>VLA models are a form of <strong>embodied artificial intelligence</strong>—AI systems that learn through interaction with physical environments. Unlike disembodied language models (ChatGPT, GPT-4) that only process text, embodied AI must:</p>
<ul>
<li class=""><strong>Ground language in physical states</strong>: Understanding &quot;above the table&quot; requires visual perception of table surfaces</li>
<li class=""><strong>Predict action consequences</strong>: Knowing that &quot;grasp&quot; requires closing the gripper around an object</li>
<li class=""><strong>Handle sensor noise and uncertainty</strong>: Robot cameras have occlusions, motion blur, and lighting variations</li>
<li class=""><strong>Operate under real-time constraints</strong>: Actions must be computed within 10-50ms for smooth robot control</li>
</ul>
<p>VLA models bridge the gap between high-level human communication (language) and low-level robot control (motor commands), enabling intuitive human-robot collaboration.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="vla-use-cases">VLA Use Cases<a href="#vla-use-cases" class="hash-link" aria-label="Direct link to VLA Use Cases" title="Direct link to VLA Use Cases" translate="no">​</a></h2>
<p>VLA models excel in scenarios requiring <strong>flexible task specification</strong>, <strong>rapid adaptation</strong>, and <strong>generalization to novel situations</strong>. Here are key application domains:</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="manipulation-tasks">Manipulation Tasks<a href="#manipulation-tasks" class="hash-link" aria-label="Direct link to Manipulation Tasks" title="Direct link to Manipulation Tasks" translate="no">​</a></h3>
<ul>
<li class=""><strong>Pick-and-place operations</strong>: &quot;Move the blue block to the left bin&quot; — VLA models parse language, identify the target object in cluttered scenes, and execute grasp-transport-release sequences.</li>
<li class=""><strong>Assembly tasks</strong>: &quot;Insert the peg into the hole&quot; — Requires precise visual alignment and force-sensitive control, guided by language context.</li>
<li class=""><strong>Sorting and organization</strong>: &quot;Arrange fruits by color&quot; — VLA models generalize across object categories (apples, oranges, bananas) without per-object programming.</li>
</ul>
<p><strong>Example</strong>: Google&#x27;s RT-1 robot successfully performed 700+ different tasks in office kitchens, from &quot;place the sponge in the sink&quot; to &quot;move the Coke can to the top drawer,&quot; achieving 97% success on seen tasks and 76% on novel instructions.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="natural-language-control">Natural Language Control<a href="#natural-language-control" class="hash-link" aria-label="Direct link to Natural Language Control" title="Direct link to Natural Language Control" translate="no">​</a></h3>
<p>VLA models enable <strong>non-expert users</strong> to command robots using everyday language instead of writing code or manually teaching waypoints:</p>
<ul>
<li class=""><strong>Household robots</strong>: &quot;Clean up the toys and put them in the toy box&quot; — The robot interprets multi-step instructions and sequences actions autonomously.</li>
<li class=""><strong>Assistive robotics</strong>: &quot;Fetch my medicine from the bathroom cabinet&quot; — Useful for elderly or mobility-impaired users who can communicate verbally but not perform tasks physically.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="industrial-automation">Industrial Automation<a href="#industrial-automation" class="hash-link" aria-label="Direct link to Industrial Automation" title="Direct link to Industrial Automation" translate="no">​</a></h3>
<p>Beyond research labs, VLA models are being deployed in industrial settings:</p>
<ul>
<li class=""><strong>Bin picking</strong>: &quot;Grasp the largest bolt from the bin&quot; — Replaces rule-based systems with adaptive policies that handle part variations.</li>
<li class=""><strong>Quality inspection</strong>: &quot;Identify defective components and move them to the reject tray&quot; — Combines visual anomaly detection with language-guided categorization.</li>
<li class=""><strong>Flexible manufacturing</strong>: Quickly retool production lines by updating task instructions rather than reprogramming robot controllers.</li>
</ul>
<p><strong>Advantage</strong>: VLA models reduce setup time from hours (traditional programming) to minutes (language specification), enabling agile manufacturing for small-batch production.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="vla-pipeline-architecture">VLA Pipeline Architecture<a href="#vla-pipeline-architecture" class="hash-link" aria-label="Direct link to VLA Pipeline Architecture" title="Direct link to VLA Pipeline Architecture" translate="no">​</a></h2>
<p>The diagram below illustrates the typical VLA inference pipeline, showing how visual and language inputs flow through the model to produce robot actions:</p>
<div class="language-mermaid codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-mermaid codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">flowchart LR</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    A[Camera Image&lt;br/&gt;640×480 RGB] --&gt; B[Vision Encoder&lt;br/&gt;ViT/ResNet]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    C[Language Instruction&lt;br/&gt;&#x27;Pick up red mug&#x27;] --&gt; D[Language Encoder&lt;br/&gt;BERT/T5]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    B --&gt; E[Multimodal Fusion&lt;br/&gt;Cross-Attention]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    D --&gt; E</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    E --&gt; F[Action Decoder&lt;br/&gt;Transformer]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    F --&gt; G[Robot Actions&lt;br/&gt;7 joint angles]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    G --&gt; H[Robot Executes&lt;br/&gt;Action]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style A fill:#e1f5ff</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style C fill:#fff4e1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    style G fill:#e8f5e9</span><br></span></code></pre></div></div>
<p><strong>Alt text</strong>: Flowchart showing VLA pipeline: Camera images and language instructions are encoded separately, fused via cross-attention, decoded into robot actions, and executed by the robot.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="pipeline-steps">Pipeline Steps<a href="#pipeline-steps" class="hash-link" aria-label="Direct link to Pipeline Steps" title="Direct link to Pipeline Steps" translate="no">​</a></h3>
<ol>
<li class=""><strong>Perception</strong>: Robot camera captures RGB image of workspace (640×480 resolution at 10Hz).</li>
<li class=""><strong>Vision encoding</strong>: Image passes through Vision Transformer (ViT-B/16), producing 256-dimensional visual embeddings.</li>
<li class=""><strong>Language encoding</strong>: Instruction &quot;pick up red mug&quot; tokenized and encoded by BERT, producing 512-dimensional language embeddings.</li>
<li class=""><strong>Multimodal fusion</strong>: Cross-attention layers align visual features (red object regions) with language tokens (&quot;red&quot;, &quot;mug&quot;).</li>
<li class=""><strong>Action decoding</strong>: Transformer decoder generates 7-dimensional action vector (joint velocities for 7-DOF arm).</li>
<li class=""><strong>Execution</strong>: Action sent to robot controller at 20Hz; process repeats until task completion.</li>
</ol>
<p><strong>Key insight</strong>: The entire pipeline is <strong>end-to-end differentiable</strong>—gradients flow from action outputs back through fusion, vision, and language encoders, enabling the model to learn optimal representations for robotic control.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="evolution-of-vla-models">Evolution of VLA Models<a href="#evolution-of-vla-models" class="hash-link" aria-label="Direct link to Evolution of VLA Models" title="Direct link to Evolution of VLA Models" translate="no">​</a></h2>
<p>VLA models emerged from decades of research in robot learning, computer vision, and natural language processing. Here&#x27;s a timeline of key milestones:</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="early-approaches-2010-2018">Early Approaches (2010-2018)<a href="#early-approaches-2010-2018" class="hash-link" aria-label="Direct link to Early Approaches (2010-2018)" title="Direct link to Early Approaches (2010-2018)" translate="no">​</a></h3>
<p><strong>Behavioral Cloning</strong>: Neural networks trained to imitate expert demonstrations. Limited generalization—robots could only repeat trained tasks in similar environments.</p>
<ul>
<li class=""><strong>Example</strong>: Learning grasping from 50,000 RGB images (Levine et al., 2016). Required extensive data collection per task.</li>
</ul>
<p><strong>Reinforcement Learning</strong>: Robots learned through trial-and-error in simulation (Gazebo, MuJoCo). Sim-to-real transfer was challenging due to domain gap.</p>
<ul>
<li class=""><strong>Example</strong>: DQN for block stacking (OpenAI, 2017). Took 10 million simulation steps to learn a single task.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="transformer-revolution-2019-2021">Transformer Revolution (2019-2021)<a href="#transformer-revolution-2019-2021" class="hash-link" aria-label="Direct link to Transformer Revolution (2019-2021)" title="Direct link to Transformer Revolution (2019-2021)" translate="no">​</a></h3>
<p>The success of <strong>transformers</strong> in NLP (BERT, GPT) and vision (ViT) sparked new approaches:</p>
<ul>
<li class=""><strong>Decision Transformer</strong> (Chen et al., 2021): Framed RL as sequence modeling—treating states, actions, and rewards as tokens in a sequence.</li>
<li class=""><strong>CLIP</strong> (Radford et al., 2021): Demonstrated powerful vision-language alignment through contrastive learning on 400M image-text pairs.</li>
</ul>
<p>These breakthroughs showed that <strong>large-scale pretraining</strong> on diverse data could produce generalizable representations for downstream tasks.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="modern-vla-era-2022-present">Modern VLA Era (2022-Present)<a href="#modern-vla-era-2022-present" class="hash-link" aria-label="Direct link to Modern VLA Era (2022-Present)" title="Direct link to Modern VLA Era (2022-Present)" translate="no">​</a></h3>
<p><strong>RT-1 (Robotics Transformer 1)</strong> (Brohan et al., 2022):</p>
<ul>
<li class="">First large-scale VLA model trained on 130,000 robot demonstrations across 700 tasks.</li>
<li class="">Achieved 97% success on seen tasks, 76% on novel instructions.</li>
<li class="">Introduced <strong>token learner</strong> to compress visual inputs and <strong>action tokenization</strong> for discrete control.</li>
</ul>
<p><strong>RT-2 (Robotics Transformer 2)</strong> (Brohan et al., 2023):</p>
<ul>
<li class="">Built on top of vision-language model (PaLI-X with 55B parameters).</li>
<li class="">Demonstrated <strong>emergent capabilities</strong>: reasoning about object properties, chain-of-thought planning.</li>
<li class="">Improved generalization: 62% success on novel objects vs. 32% for RT-1.</li>
</ul>
<p><strong>PaLM-E (Embodied Language Model)</strong> (Driess et al., 2023):</p>
<ul>
<li class="">Largest VLA model (562B parameters) integrating PaLM language model with visual observations.</li>
<li class="">Handles <strong>long-horizon tasks</strong>: &quot;Prepare a meal&quot; decomposed into 20+ steps.</li>
<li class="">Multimodal reasoning: &quot;Why is the drawer stuck?&quot; → &quot;Because the spoon is blocking it.&quot;</li>
</ul>
<p><strong>Trend</strong>: VLA models are growing in scale (billions of parameters), training data (millions of trajectories), and capabilities (zero-shot generalization, emergent reasoning).</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="vla-vs-traditional-robotics">VLA vs Traditional Robotics<a href="#vla-vs-traditional-robotics" class="hash-link" aria-label="Direct link to VLA vs Traditional Robotics" title="Direct link to VLA vs Traditional Robotics" translate="no">​</a></h2>
<p>How do VLA models compare to classical robotics approaches? The table below highlights key differences:</p>
<table><thead><tr><th>Aspect</th><th>Traditional Robotics</th><th>VLA Models</th></tr></thead><tbody><tr><td><strong>Task Specification</strong></td><td>Code robot programs (RAPID, KRL, Python scripts)</td><td>Natural language instructions</td></tr><tr><td><strong>Perception</strong></td><td>Hand-engineered features (edge detection, object templates)</td><td>End-to-end learned visual encoders (ViT, ResNet)</td></tr><tr><td><strong>Planning</strong></td><td>Motion planners (RRT, MoveIt) with explicit constraints</td><td>Implicit planning through sequence modeling</td></tr><tr><td><strong>Control</strong></td><td>PID controllers, impedance control</td><td>Learned policies (actions as transformer outputs)</td></tr><tr><td><strong>Generalization</strong></td><td>Task-specific—new tasks require new code</td><td>Few-shot or zero-shot generalization to novel instructions</td></tr><tr><td><strong>Data Requirements</strong></td><td>Minimal (kinematic models, CAD files)</td><td>Large-scale (thousands of task demonstrations)</td></tr><tr><td><strong>Setup Time</strong></td><td>Hours to days (programming + testing)</td><td>Minutes (provide language instruction)</td></tr><tr><td><strong>Failure Modes</strong></td><td>Predictable (sensor failures, kinematic limits)</td><td>Unpredictable (distribution shift, adversarial inputs)</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="advantages-of-vla-models">Advantages of VLA Models<a href="#advantages-of-vla-models" class="hash-link" aria-label="Direct link to Advantages of VLA Models" title="Direct link to Advantages of VLA Models" translate="no">​</a></h3>
<ol>
<li class=""><strong>Rapid task specification</strong>: Describe tasks in language rather than coding low-level behaviors.</li>
<li class=""><strong>Generalization</strong>: Transfer learned knowledge across object categories, environments, and instructions.</li>
<li class=""><strong>Multimodal reasoning</strong>: Leverage language priors (e.g., &quot;fragile objects should be handled gently&quot;) without explicit rules.</li>
<li class=""><strong>Scalability</strong>: Fine-tune a single pretrained model for diverse tasks instead of developing per-task controllers.</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="challenges-of-vla-models">Challenges of VLA Models<a href="#challenges-of-vla-models" class="hash-link" aria-label="Direct link to Challenges of VLA Models" title="Direct link to Challenges of VLA Models" translate="no">​</a></h3>
<ol>
<li class=""><strong>Data hunger</strong>: Require 10,000-1,000,000 demonstrations vs. 10-100 for traditional methods.</li>
<li class=""><strong>Interpretability</strong>: Hard to debug failures—no explicit motion plans or constraint equations.</li>
<li class=""><strong>Safety guarantees</strong>: Difficult to prove safety bounds for learned policies in safety-critical applications (surgery, human-robot collaboration).</li>
<li class=""><strong>Computational cost</strong>: Inference requires GPU (RT-2 runs at 3Hz on NVIDIA A100), traditional controllers run at 1000Hz on microcontrollers.</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="future-directions">Future Directions<a href="#future-directions" class="hash-link" aria-label="Direct link to Future Directions" title="Direct link to Future Directions" translate="no">​</a></h3>
<p>The robotics community is moving toward <strong>hybrid approaches</strong> that combine the best of both worlds:</p>
<ul>
<li class=""><strong>VLA for high-level planning</strong> (&quot;grasp the mug&quot;) + <strong>classical control for low-level execution</strong> (impedance control during contact).</li>
<li class=""><strong>Formal verification</strong> of VLA policies using tools like neural network verification (Marabou, ERAN).</li>
<li class=""><strong>Sim-to-real transfer</strong> using Isaac Gym domain randomization to reduce real-world data requirements.</li>
<li class=""><strong>Foundation models for robotics</strong>: Pretrain VLA models on internet-scale video data (YouTube manipulation videos, WikiHow instructions) to enable better generalization.</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="hands-on-exercises">Hands-On Exercises<a href="#hands-on-exercises" class="hash-link" aria-label="Direct link to Hands-On Exercises" title="Direct link to Hands-On Exercises" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="exercise-1-analyze-a-vla-paper">Exercise 1: Analyze a VLA Paper<a href="#exercise-1-analyze-a-vla-paper" class="hash-link" aria-label="Direct link to Exercise 1: Analyze a VLA Paper" title="Direct link to Exercise 1: Analyze a VLA Paper" translate="no">​</a></h3>
<p>Read the original RT-1 paper (<a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener noreferrer" class="">Brohan et al., 2022</a>) and answer:</p>
<ol>
<li class="">How many demonstration trajectories were collected for training?</li>
<li class="">What is the architecture of the token learner module?</li>
<li class="">How does RT-1 tokenize continuous actions into discrete action tokens?</li>
<li class="">What was the success rate on novel instructions vs. seen instructions?</li>
</ol>
<p><strong>Expected time</strong>: 45 minutes
<strong>Learning goal</strong>: Understand VLA model design decisions and evaluation metrics.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="exercise-2-compare-vla-architectures">Exercise 2: Compare VLA Architectures<a href="#exercise-2-compare-vla-architectures" class="hash-link" aria-label="Direct link to Exercise 2: Compare VLA Architectures" title="Direct link to Exercise 2: Compare VLA Architectures" translate="no">​</a></h3>
<p>Create a comparison table with the following models: RT-1, RT-2, PaLM-E. Include:</p>
<ul>
<li class="">Number of parameters</li>
<li class="">Pretraining dataset size</li>
<li class="">Supported task types (pick-and-place, long-horizon, reasoning)</li>
<li class="">Generalization capabilities (object, instruction, environment)</li>
<li class="">Inference speed (actions per second)</li>
</ul>
<p><strong>Expected time</strong>: 30 minutes
<strong>Learning goal</strong>: Differentiate between VLA model families and their trade-offs.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="exercise-3-identify-vla-use-cases">Exercise 3: Identify VLA Use Cases<a href="#exercise-3-identify-vla-use-cases" class="hash-link" aria-label="Direct link to Exercise 3: Identify VLA Use Cases" title="Direct link to Exercise 3: Identify VLA Use Cases" translate="no">​</a></h3>
<p>For each scenario below, determine if a VLA model is appropriate or if traditional robotics is better. Justify your answer.</p>
<p>Scenarios:</p>
<ol>
<li class=""><strong>Assembly line</strong>: Insert 1000 identical screws per hour with 99.99% precision.</li>
<li class=""><strong>Home cleaning robot</strong>: &quot;Clean up the living room and put toys in the toy box.&quot;</li>
<li class=""><strong>Surgical robot</strong>: Perform laparoscopic surgery with sub-millimeter accuracy.</li>
<li class=""><strong>Warehouse robot</strong>: &quot;Move all red boxes from aisle 3 to the shipping zone.&quot;</li>
<li class=""><strong>Research lab</strong>: Test 50 different grasping strategies on novel objects daily.</li>
</ol>
<p><strong>Expected time</strong>: 20 minutes
<strong>Learning goal</strong>: Apply VLA strengths/weaknesses to real-world scenarios.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways" translate="no">​</a></h2>
<ul>
<li class="">
<p><strong>VLA models unify vision, language, and action</strong> into end-to-end learned systems that map camera images and natural language instructions directly to robot actions, enabling flexible task specification and rapid deployment.</p>
</li>
<li class="">
<p><strong>Multimodal learning</strong> allows VLA models to generalize across object categories, task instructions, and environments by learning joint embeddings of visual features, language semantics, and action sequences from diverse demonstration data.</p>
</li>
<li class="">
<p><strong>VLA use cases</strong> span household robotics (cleaning, fetch tasks), industrial automation (bin picking, assembly), and assistive applications (elderly care, accessibility), excelling where task variety is high and adaptation speed is critical.</p>
</li>
<li class="">
<p><strong>Modern VLA models</strong> (RT-1, RT-2, PaLM-E) leverage transformer architectures and large-scale pretraining on vision-language datasets, achieving emergent capabilities like zero-shot generalization, chain-of-thought reasoning, and long-horizon planning.</p>
</li>
<li class="">
<p><strong>VLA evolution</strong> progressed from early behavioral cloning (narrow task performance) to transformer-based systems (broad generalization), driven by advances in self-attention, vision-language pretraining (CLIP), and massive robot demonstration datasets.</p>
</li>
<li class="">
<p><strong>VLA advantages</strong> include rapid task specification via language, generalization to novel objects and instructions, multimodal reasoning, and scalability through fine-tuning pretrained models, but challenges remain in data efficiency, interpretability, safety guarantees, and computational cost.</p>
</li>
<li class="">
<p><strong>Hybrid approaches</strong> combining VLA high-level planning with classical low-level control, formal verification of learned policies, sim-to-real transfer techniques, and foundation models pretrained on internet-scale data represent the future direction of practical VLA deployment.</p>
</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="navigation">Navigation<a href="#navigation" class="hash-link" aria-label="Direct link to Navigation" title="Direct link to Navigation" translate="no">​</a></h2>
<p><strong>Previous Chapter</strong>: <a class="" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-gym-rl">Reinforcement Learning with Isaac Gym</a>
<strong>Next Chapter</strong>: <a class="" href="/physical-ai-humanoid-robotics/docs/module-4-vla/vla-architectures">VLA Architectures: RT-1, RT-2, PaLM-E</a></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/intro-vla.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/reinforcement-learning"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Advanced RL and Sim-to-Real Transfer</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/physical-ai-humanoid-robotics/docs/module-4-vla/vla-architectures"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">VLA Architectures: RT-1, RT-2, PaLM-E</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#prerequisites" class="table-of-contents__link toc-highlight">Prerequisites</a></li><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#what-are-vla-models" class="table-of-contents__link toc-highlight">What are VLA Models?</a><ul><li><a href="#core-components" class="table-of-contents__link toc-highlight">Core Components</a></li><li><a href="#multimodal-learning" class="table-of-contents__link toc-highlight">Multimodal Learning</a></li><li><a href="#embodied-ai" class="table-of-contents__link toc-highlight">Embodied AI</a></li></ul></li><li><a href="#vla-use-cases" class="table-of-contents__link toc-highlight">VLA Use Cases</a><ul><li><a href="#manipulation-tasks" class="table-of-contents__link toc-highlight">Manipulation Tasks</a></li><li><a href="#natural-language-control" class="table-of-contents__link toc-highlight">Natural Language Control</a></li><li><a href="#industrial-automation" class="table-of-contents__link toc-highlight">Industrial Automation</a></li></ul></li><li><a href="#vla-pipeline-architecture" class="table-of-contents__link toc-highlight">VLA Pipeline Architecture</a><ul><li><a href="#pipeline-steps" class="table-of-contents__link toc-highlight">Pipeline Steps</a></li></ul></li><li><a href="#evolution-of-vla-models" class="table-of-contents__link toc-highlight">Evolution of VLA Models</a><ul><li><a href="#early-approaches-2010-2018" class="table-of-contents__link toc-highlight">Early Approaches (2010-2018)</a></li><li><a href="#transformer-revolution-2019-2021" class="table-of-contents__link toc-highlight">Transformer Revolution (2019-2021)</a></li><li><a href="#modern-vla-era-2022-present" class="table-of-contents__link toc-highlight">Modern VLA Era (2022-Present)</a></li></ul></li><li><a href="#vla-vs-traditional-robotics" class="table-of-contents__link toc-highlight">VLA vs Traditional Robotics</a><ul><li><a href="#advantages-of-vla-models" class="table-of-contents__link toc-highlight">Advantages of VLA Models</a></li><li><a href="#challenges-of-vla-models" class="table-of-contents__link toc-highlight">Challenges of VLA Models</a></li><li><a href="#future-directions" class="table-of-contents__link toc-highlight">Future Directions</a></li></ul></li><li><a href="#hands-on-exercises" class="table-of-contents__link toc-highlight">Hands-On Exercises</a><ul><li><a href="#exercise-1-analyze-a-vla-paper" class="table-of-contents__link toc-highlight">Exercise 1: Analyze a VLA Paper</a></li><li><a href="#exercise-2-compare-vla-architectures" class="table-of-contents__link toc-highlight">Exercise 2: Compare VLA Architectures</a></li><li><a href="#exercise-3-identify-vla-use-cases" class="table-of-contents__link toc-highlight">Exercise 3: Identify VLA Use Cases</a></li></ul></li><li><a href="#key-takeaways" class="table-of-contents__link toc-highlight">Key Takeaways</a></li><li><a href="#navigation" class="table-of-contents__link toc-highlight">Navigation</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Book</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-robotics/docs/preface">Preface</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-robotics/docs/intro-physical-ai">Introduction to Physical AI</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Modules</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-robotics/docs/module-1-ros2/ros2-fundamentals">Module 1: ROS 2</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-robotics/docs/module-2-gazebo/intro-gazebo">Module 2: Gazebo &amp; Unity</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-sdk-overview">Module 3: NVIDIA Isaac</a></li><li class="footer__item"><a class="footer__link-item" href="/physical-ai-humanoid-robotics/docs/module-4-vla/voice-to-action">Module 4: VLA</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Built with Docusaurus for Physical AI & Humanoid Robotics</div></div></div></footer></div>
</body>
</html>