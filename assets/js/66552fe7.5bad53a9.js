"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_01=globalThis.webpackChunkphysical_ai_humanoid_robotics_01||[]).push([[237],{2715:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4-vla/vla-training-deployment","title":"Training and Deploying VLA Systems","description":"Learn how to train, fine-tune, and deploy Vision-Language-Action models for real-world robotics applications","source":"@site/docs/module-4-vla/vla-training-deployment.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/vla-training-deployment","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/vla-training-deployment","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/vla-training-deployment.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Training and Deploying VLA Systems","description":"Learn how to train, fine-tune, and deploy Vision-Language-Action models for real-world robotics applications"},"sidebar":"mainSidebar","previous":{"title":"VLA Architectures: RT-1, RT-2, PaLM-E","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/vla-architectures"},"next":{"title":"Voice-to-Action with OpenAI Whisper","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/voice-to-action"}}');var r=i(4848),t=i(8453);const o={sidebar_position:3,title:"Training and Deploying VLA Systems",description:"Learn how to train, fine-tune, and deploy Vision-Language-Action models for real-world robotics applications"},a="Training and Deploying VLA Systems",l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"VLA Training Data Requirements",id:"vla-training-data-requirements",level:2},{value:"Demonstration Data Collection",id:"demonstration-data-collection",level:3},{value:"Data Augmentation Strategies",id:"data-augmentation-strategies",level:3},{value:"Synthetic Data from Isaac Gym",id:"synthetic-data-from-isaac-gym",level:3},{value:"Fine-Tuning VLA Models",id:"fine-tuning-vla-models",level:2},{value:"Transfer Learning Workflow",id:"transfer-learning-workflow",level:3},{value:"Domain Adaptation",id:"domain-adaptation",level:3},{value:"Hyperparameters",id:"hyperparameters",level:3},{value:"VLA Training Pipeline Diagram",id:"vla-training-pipeline-diagram",level:2},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"Model Size and Inference Time",id:"model-size-and-inference-time",level:3},{value:"Safety Constraints",id:"safety-constraints",level:3},{value:"Failure Modes and Recovery",id:"failure-modes-and-recovery",level:3},{value:"Real-World Case Studies",id:"real-world-case-studies",level:2},{value:"RT-1 in Google&#39;s Everyday Robots",id:"rt-1-in-googles-everyday-robots",level:3},{value:"RT-2 Emergent Reasoning",id:"rt-2-emergent-reasoning",level:3},{value:"Industrial VLA Deployment",id:"industrial-vla-deployment",level:3},{value:"VLA Inference Workflow (Pseudocode)",id:"vla-inference-workflow-pseudocode",level:2},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: Analyze Demonstration Dataset",id:"exercise-1-analyze-demonstration-dataset",level:3},{value:"Exercise 2: Design Fine-Tuning Strategy",id:"exercise-2-design-fine-tuning-strategy",level:3},{value:"Exercise 3: Estimate Inference Time and Optimize",id:"exercise-3-estimate-inference-time-and-optimize",level:3},{value:"Exercise 4: Identify Safety Constraints",id:"exercise-4-identify-safety-constraints",level:3},{value:"Exercise 5: Propose Deployment Architecture",id:"exercise-5-propose-deployment-architecture",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Navigation",id:"navigation",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"training-and-deploying-vla-systems",children:"Training and Deploying VLA Systems"})}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before diving into this chapter, ensure you have:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Completed ",(0,r.jsx)(n.strong,{children:"Chapter 2: VLA Architectures"})," from this module"]}),"\n",(0,r.jsxs)(n.li,{children:["Understanding of ",(0,r.jsx)(n.strong,{children:"VLA model architectures"})," (RT-1, RT-2, PaLM-E)"]}),"\n",(0,r.jsxs)(n.li,{children:["Experience with ",(0,r.jsx)(n.strong,{children:"deep learning frameworks"})," (PyTorch or TensorFlow/JAX)"]}),"\n",(0,r.jsxs)(n.li,{children:["Familiarity with ",(0,r.jsx)(n.strong,{children:"GPU training"})," and distributed systems"]}),"\n",(0,r.jsxs)(n.li,{children:["Knowledge of ",(0,r.jsx)(n.strong,{children:"reinforcement learning"})," or ",(0,r.jsx)(n.strong,{children:"behavioral cloning"})," basics"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(n.p,{children:"While understanding VLA architectures is essential, successfully deploying them in real-world settings requires mastering the full pipeline: collecting high-quality demonstration data, fine-tuning pretrained models for specific tasks, optimizing inference for real-time robot control, and handling failure modes gracefully. This chapter covers practical aspects of VLA training and deployment, from data collection strategies to safety constraints."}),"\n",(0,r.jsx)(n.p,{children:"You'll learn how to leverage synthetic data from Isaac Gym to reduce real-world data requirements, apply domain adaptation techniques to transfer models across robot platforms, optimize models for edge deployment, and implement safety mechanisms to prevent harmful robot behaviors."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Learning Objectives:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand VLA training data requirements and collection strategies"}),"\n",(0,r.jsx)(n.li,{children:"Apply fine-tuning and domain adaptation techniques to pretrained VLA models"}),"\n",(0,r.jsx)(n.li,{children:"Design deployment pipelines for real-time robot control"}),"\n",(0,r.jsx)(n.li,{children:"Implement safety constraints and failure recovery mechanisms"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"vla-training-data-requirements",children:"VLA Training Data Requirements"}),"\n",(0,r.jsx)(n.p,{children:"VLA models require large-scale, diverse demonstration datasets to learn generalizable policies. Unlike traditional supervised learning (image classification with static datasets), robot learning demands:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multimodal alignment"}),": Visual observations synchronized with language instructions and robot actions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Temporal consistency"}),": Sequential data showing state transitions (s_t \u2192 a_t \u2192 s_t+1)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Task diversity"}),": Hundreds to thousands of different tasks to enable generalization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environment diversity"}),": Multiple robot platforms, workspaces, lighting conditions"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"demonstration-data-collection",children:"Demonstration Data Collection"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Teleoperation"})," is the dominant method for collecting robot demonstration data:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Process"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Human operator"})," controls robot using input device (VR controller, keyboard, 3D mouse)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robot executes"})," actions while recording:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Camera images (RGB, depth) at 10-30 Hz"}),"\n",(0,r.jsx)(n.li,{children:'Language instruction (e.g., "pick up red mug")'}),"\n",(0,r.jsx)(n.li,{children:"Robot state (joint angles, gripper position)"}),"\n",(0,r.jsx)(n.li,{children:"Actions (joint velocities or position commands)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quality filtering"}),": Only successful demonstrations are retained (operator marks success/failure)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dataset augmentation"}),": Apply spatial augmentations (random crops, color jitter) to increase diversity"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-1 data collection setup"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"13 robots"})," in office kitchens (mobile manipulators with 7-DOF arms)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"700+ tasks"}),' defined by researchers ("move Coke can to drawer", "place apple in bowl")']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3-5 operators"})," per robot, each completing 10-20 demonstrations per task"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"130,000 demonstrations"})," collected over 17 months"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost estimate"}),": $50K-$100K (operator salaries, robot maintenance)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Challenges"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scalability"}),": Teleoperation is slow (5-10 minutes per demo) and expensive"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quality variance"}),": Different operators have different success rates and motion styles"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Task coverage"}),": Hard to ensure even coverage across all task variations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Distribution shift"}),": Operator demonstrations may differ from autonomous policy behavior"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"data-augmentation-strategies",children:"Data Augmentation Strategies"}),"\n",(0,r.jsx)(n.p,{children:"To maximize data efficiency, apply augmentation techniques:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Spatial augmentations"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torchvision.transforms as T\n\naugmentation = T.Compose([\n    T.RandomResizedCrop(300, scale=(0.8, 1.0)),  # Random crop and resize\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color variation\n    T.RandomHorizontalFlip(p=0.5),  # Flip left-right (with action mirroring)\n])\n\n# Apply to image observations\nimage_aug = augmentation(original_image)\n# Mirror actions for horizontal flip\nif flipped:\n    action_aug[0] *= -1  # Reverse lateral velocity\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Temporal augmentation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action chunking"}),": Predict N-step action sequences (RT-1 predicts 8 future actions) to smooth trajectories"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Frame skipping"}),": Subsample demonstrations at 5 Hz instead of 30 Hz to reduce redundancy"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Trajectory reversal"}),": Reverse pick-and-place demos to generate place-and-pick data"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Language augmentation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Paraphrasing"}),': "pick red mug" \u2192 "grasp the red cup", "grab red mug handle"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Synonyms"}),': "move" \u2192 "transport", "relocate", "shift"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Instruction templates"}),': "pick {object}" \u2192 "pick up the {object}", "grasp {object} and lift it"']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Result"}),": Augmentation increases effective dataset size by 5-10\xd7, improving generalization by 15-20%."]}),"\n",(0,r.jsx)(n.h3,{id:"synthetic-data-from-isaac-gym",children:"Synthetic Data from Isaac Gym"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Isaac Gym"})," enables generating large-scale synthetic demonstrations in parallel:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Approach"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Define robot tasks in Isaac Gym (e.g., Franka pick-and-place)"}),"\n",(0,r.jsx)(n.li,{children:"Train RL policy in simulation using domain randomization (vary object poses, colors, lighting)"}),"\n",(0,r.jsx)(n.li,{children:"Use trained RL policy to generate 100,000+ synthetic demonstrations"}),"\n",(0,r.jsx)(n.li,{children:"Combine synthetic data with 10,000 real-world demos for VLA training"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Synthetic data generation code"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import isaacgym\nfrom rl_games.torch_runner import Runner\n\n# Train RL policy in Isaac Gym (from Module 3)\nrunner = Runner()\nrunner.load_config({\n    'task': 'FrankaPickPlace',\n    'num_envs': 4096,\n    'learning_rate': 0.001,\n    'max_iterations': 5000\n})\nrunner.run()  # Train policy with PPO\n\n# Generate synthetic demonstrations\nenv = isaacgymenvs.make('FrankaPickPlace', num_envs=4096)\npolicy = runner.get_policy()\n\ndemonstrations = []\nfor episode in range(25):  # 25 episodes \xd7 4096 envs = 102,400 demos\n    obs = env.reset()\n    episode_data = []\n    for step in range(50):  # 50 steps per episode\n        action = policy(obs)  # RL policy generates action\n        next_obs, reward, done, info = env.step(action)\n        episode_data.append({\n            'image': obs['camera_rgb'],\n            'language': 'pick up the cube',  # Task instruction\n            'action': action,\n            'success': done and reward > 0.9\n        })\n        obs = next_obs\n    if episode_data[-1]['success']:  # Only keep successful demos\n        demonstrations.append(episode_data)\n\n# Save demonstrations for VLA training\ntorch.save(demonstrations, 'synthetic_demos.pt')\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Benefits"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scale"}),": Generate 100K+ demos in days vs. months for real-world collection"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": $0 (sim-only) vs. $100K (real robots + operators)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Diversity"}),": Automatic domain randomization covers object variations, lighting, clutter"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Limitations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sim-to-real gap"}),": Synthetic data may not capture real-world physics (friction, deformation, contact dynamics)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual domain shift"}),": Simulated images look different from real camera feeds"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Mix 90% synthetic + 10% real data, apply domain adaptation techniques (next section)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"fine-tuning-vla-models",children:"Fine-Tuning VLA Models"}),"\n",(0,r.jsxs)(n.p,{children:["Rather than training VLA models from scratch (requires millions of demos), ",(0,r.jsx)(n.strong,{children:"fine-tuning"})," pretrained models on task-specific data is the standard approach:"]}),"\n",(0,r.jsx)(n.h3,{id:"transfer-learning-workflow",children:"Transfer Learning Workflow"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Step 1: Start with pretrained checkpoint"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nfrom transformers import AutoModel\n\n# Load pretrained RT-2 model (PaLI-X 55B fine-tuned on robot data)\nmodel = AutoModel.from_pretrained('google/rt-2-base')  # Hypothetical API\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Step 2: Freeze backbone, fine-tune head"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Freeze vision encoder and language model (pretrained on web data)\nfor param in model.vision_encoder.parameters():\n    param.requires_grad = False\nfor param in model.language_model.parameters():\n    param.requires_grad = False\n\n# Fine-tune only action decoder (adapts to new robot/task)\nfor param in model.action_decoder.parameters():\n    param.requires_grad = True\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Step 3: Train on task-specific data"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"optimizer = torch.optim.AdamW(model.action_decoder.parameters(), lr=1e-4)\nloss_fn = torch.nn.CrossEntropyLoss()  # For discretized actions\n\nfor epoch in range(50):\n    for batch in dataloader:  # Custom robot demonstrations\n        images = batch['images']  # (B, 3, 300, 300)\n        instructions = batch['instructions']  # (B, max_len)\n        actions = batch['actions']  # (B, 11) - discretized into bins\n\n        # Forward pass\n        action_logits = model(images, instructions)  # (B, 11, 256)\n\n        # Compute loss (cross-entropy over 256 bins for each action dimension)\n        loss = loss_fn(action_logits.view(-1, 256), actions.view(-1))\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Result"}),": Fine-tuning on 1,000-10,000 task-specific demos adapts pretrained VLA to new tasks in hours instead of weeks."]}),"\n",(0,r.jsx)(n.h3,{id:"domain-adaptation",children:"Domain Adaptation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Challenge"}),": VLA model trained on one robot (e.g., Google's mobile manipulator) may fail on different robot (e.g., Universal Robots UR5) due to:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Different action spaces (7-DOF vs. 6-DOF arms)"}),"\n",(0,r.jsx)(n.li,{children:"Different camera viewpoints (wrist-mounted vs. overhead)"}),"\n",(0,r.jsx)(n.li,{children:"Different workspace geometry"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Domain adaptation techniques"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"1. Action Space Mapping"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def map_action_space(action_rt1, robot_type):\n    """Map RT-1 actions (7 arm joints + 3 base + 1 gripper) to target robot"""\n    if robot_type == \'ur5\':\n        # UR5 has 6-DOF arm, no mobile base\n        action_ur5 = action_rt1[:6]  # Take first 6 joints, discard base velocities\n        gripper_ur5 = action_rt1[10]  # Map gripper command\n        return torch.cat([action_ur5, gripper_ur5.unsqueeze(0)])\n    elif robot_type == \'franka\':\n        # Franka has 7-DOF arm, no mobile base\n        action_franka = action_rt1[:7]  # Take first 7 joints\n        gripper_franka = action_rt1[10]\n        return torch.cat([action_franka, gripper_franka.unsqueeze(0)])\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"2. Camera Viewpoint Adaptation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Use ",(0,r.jsx)(n.strong,{children:"image augmentation"})," (random rotations, translations) to simulate viewpoint variations"]}),"\n",(0,r.jsxs)(n.li,{children:["Collect 100-500 ",(0,r.jsx)(n.strong,{children:"calibration demonstrations"})," on target robot to adapt vision encoder"]}),"\n",(0,r.jsxs)(n.li,{children:["Apply ",(0,r.jsx)(n.strong,{children:"CycleGAN"})," to translate source robot images to target robot camera style"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"3. Few-Shot Fine-Tuning"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Collect 100-1,000 demos on target robot/task"}),"\n",(0,r.jsx)(n.li,{children:"Fine-tune only final layers (action decoder + last 2 transformer layers)"}),"\n",(0,r.jsxs)(n.li,{children:["Use ",(0,r.jsx)(n.strong,{children:"meta-learning"})," (MAML) to optimize for fast adaptation with few samples"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),": Domain adaptation achieves 70-85% of source robot performance with 5-10\xd7 less data than training from scratch."]}),"\n",(0,r.jsx)(n.h3,{id:"hyperparameters",children:"Hyperparameters"}),"\n",(0,r.jsx)(n.p,{children:"Key hyperparameters for VLA fine-tuning:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Hyperparameter"}),(0,r.jsx)(n.th,{children:"RT-1"}),(0,r.jsx)(n.th,{children:"RT-2"}),(0,r.jsx)(n.th,{children:"PaLM-E"}),(0,r.jsx)(n.th,{children:"Notes"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Learning rate"})}),(0,r.jsx)(n.td,{children:"1e-4"}),(0,r.jsx)(n.td,{children:"3e-5"}),(0,r.jsx)(n.td,{children:"1e-5"}),(0,r.jsx)(n.td,{children:"Lower LR for larger models"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Batch size"})}),(0,r.jsx)(n.td,{children:"256"}),(0,r.jsx)(n.td,{children:"512"}),(0,r.jsx)(n.td,{children:"1024"}),(0,r.jsx)(n.td,{children:"Limited by GPU memory"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Epochs"})}),(0,r.jsx)(n.td,{children:"100"}),(0,r.jsx)(n.td,{children:"50"}),(0,r.jsx)(n.td,{children:"30"}),(0,r.jsx)(n.td,{children:"Pretrained models need fewer epochs"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Weight decay"})}),(0,r.jsx)(n.td,{children:"1e-4"}),(0,r.jsx)(n.td,{children:"1e-5"}),(0,r.jsx)(n.td,{children:"1e-6"}),(0,r.jsx)(n.td,{children:"Regularization"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Warmup steps"})}),(0,r.jsx)(n.td,{children:"1000"}),(0,r.jsx)(n.td,{children:"500"}),(0,r.jsx)(n.td,{children:"200"}),(0,r.jsx)(n.td,{children:"Gradual LR increase"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Gradient clipping"})}),(0,r.jsx)(n.td,{children:"1.0"}),(0,r.jsx)(n.td,{children:"0.5"}),(0,r.jsx)(n.td,{children:"0.1"}),(0,r.jsx)(n.td,{children:"Prevent exploding gradients"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optimizer"}),": AdamW (Adam with weight decay) is standard for transformer-based VLA models."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Learning rate schedule"}),": Cosine decay with warmup (increase from 0 to max LR over 1000 steps, then decay to 0.1\xd7 max LR)."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"vla-training-pipeline-diagram",children:"VLA Training Pipeline Diagram"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"flowchart LR\n    A[Pretrained Model<br/>RT-2/PaLM-E] --\x3e B[Task-Specific<br/>Demonstrations]\n    B --\x3e C{Data Source}\n    C --\x3e|Real Robot| D[Teleoperation<br/>1K-10K demos]\n    C --\x3e|Simulation| E[Isaac Gym RL<br/>100K demos]\n    D --\x3e F[Data Augmentation<br/>Spatial/Temporal/Language]\n    E --\x3e F\n    F --\x3e G[Fine-Tune<br/>Action Decoder]\n    A --\x3e G\n    G --\x3e H[Evaluate<br/>Success Rate]\n    H --\x3e I{Performance<br/>Acceptable?}\n    I --\x3e|No| J[Collect More Data<br/>or Adjust Hyperparams]\n    I --\x3e|Yes| K[Deploy to Robot]\n    J --\x3e B\n\n    style A fill:#e6f3ff\n    style K fill:#e8f5e9\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Alt text"}),": VLA training pipeline flowchart showing pretrained model fine-tuning on task-specific data from real robot teleoperation or Isaac Gym simulation, followed by data augmentation, fine-tuning, evaluation, and iterative refinement until performance is acceptable for deployment."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,r.jsx)(n.p,{children:"Deploying VLA models on physical robots introduces constraints absent in simulation:"}),"\n",(0,r.jsx)(n.h3,{id:"model-size-and-inference-time",children:"Model Size and Inference Time"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Challenge"}),": Real-time robot control requires actions at 10-20 Hz, but large VLA models (RT-2: 55B params, PaLM-E: 562B params) have slow inference."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-1 Deployment"})," (Edge Device):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware"}),": NVIDIA Jetson AGX Orin (32GB RAM, 275 TOPS AI)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference speed"}),": 5 Hz (200ms per action)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model size"}),": 35M parameters (140 MB in FP32, 70 MB in FP16)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deployment"}),": Full model runs on robot's onboard computer"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-2 Deployment"})," (Cloud Hybrid):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware"}),": NVIDIA A100 GPU in cloud + Jetson Orin on robot"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference speed"}),": 3 Hz (333ms per action)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model size"}),": 55B parameters (220 GB in FP32, 110 GB in FP16)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deployment"}),": Vision encoder on robot (7 Hz) sends features to cloud; cloud returns actions via 5G"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PaLM-E Deployment"})," (Cloud Only):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware"}),": 2048 TPU v4 chips"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inference speed"}),": 0.5 Hz (2 seconds per action)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model size"}),": 562B parameters (2.2 TB in FP32)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deployment"}),": Not practical for real-time robot control; used for offline planning"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optimization techniques"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quantization"}),": Convert FP32 weights to INT8 (4\xd7 smaller, 2-3\xd7 faster inference)","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch.quantization as quant\nmodel_int8 = quant.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pruning"}),": Remove 30-50% of weights with minimal accuracy loss"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Distillation"}),': Train smaller "student" model (1B params) to mimic RT-2 (55B params) behavior']}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"safety-constraints",children:"Safety Constraints"}),"\n",(0,r.jsx)(n.p,{children:"VLA models can generate unsafe actions (e.g., move arm too fast, grasp fragile objects with excessive force). Implement safety checks:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"1. Kinematic limits"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def clip_actions(actions, robot_limits):\n    """Clip joint angles and velocities to safe ranges"""\n    joint_positions = actions[:7]\n    joint_velocities = actions[7:14]\n\n    # Clip positions to joint limits\n    joint_positions = torch.clamp(joint_positions,\n                                   min=robot_limits[\'joint_min\'],\n                                   max=robot_limits[\'joint_max\'])\n\n    # Limit velocities to prevent sudden movements\n    max_velocity = torch.tensor([2.0, 2.0, 2.0, 2.0, 2.5, 2.5, 2.5])  # rad/s\n    joint_velocities = torch.clamp(joint_velocities, min=-max_velocity, max=max_velocity)\n\n    return torch.cat([joint_positions, joint_velocities])\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"2. Collision detection"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use robot's built-in collision detection (force-torque sensors)"}),"\n",(0,r.jsx)(n.li,{children:"If external force exceeds threshold (10N), stop robot and alert operator"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reactive control"}),": If collision detected, VLA generates new action to move away from obstacle"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"3. Workspace boundaries"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def check_workspace_bounds(end_effector_pos, workspace_limits):\n    \"\"\"Verify end-effector stays within safe workspace\"\"\"\n    x, y, z = end_effector_pos\n    if not (workspace_limits['x_min'] <= x <= workspace_limits['x_max'] and\n            workspace_limits['y_min'] <= y <= workspace_limits['y_max'] and\n            workspace_limits['z_min'] <= z <= workspace_limits['z_max']):\n        raise SafetyException(\"End-effector outside safe workspace!\")\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"4. Emergency stop"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Physical e-stop button halts robot immediately"}),"\n",(0,r.jsx)(n.li,{children:"Software watchdog: If no valid action received within 500ms, robot enters safe mode (hold position)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"failure-modes-and-recovery",children:"Failure Modes and Recovery"}),"\n",(0,r.jsx)(n.p,{children:"VLA models can fail in several ways:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"1. Perception failures"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Symptom"}),": Model doesn't detect target object (poor lighting, occlusion)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recovery"}),": Move robot to different viewpoint, request user assistance"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"2. Execution failures"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Symptom"}),": Grasp fails (object slips, gripper misaligned)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recovery"}),": Retry grasp from different angle, adjust gripper force"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"3. Task ambiguity"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Symptom"}),': Instruction unclear ("pick the object" when multiple objects present)']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recovery"}),': Ask clarifying question ("Which object: left or right?")']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Failure detection"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def detect_failure(observation, expected_outcome):\n    \"\"\"Detect if action succeeded or failed\"\"\"\n    # Check if gripper closed around object\n    gripper_closed = observation['gripper_position'] < 0.01  # mm\n    object_in_gripper = observation['force_sensor'] > 0.5  # N\n\n    if expected_outcome == 'grasp':\n        success = gripper_closed and object_in_gripper\n        return not success  # Failure if grasp unsuccessful\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recovery strategy"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def recover_from_failure(failure_type):\n    if failure_type == 'grasp_failed':\n        # Retry grasp with different approach angle\n        return {'action': 'retry_grasp', 'angle_offset': 45}  # degrees\n    elif failure_type == 'object_not_found':\n        # Move to exploration behavior\n        return {'action': 'search', 'head_motion': 'sweep'}\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"real-world-case-studies",children:"Real-World Case Studies"}),"\n",(0,r.jsx)(n.h3,{id:"rt-1-in-googles-everyday-robots",children:"RT-1 in Google's Everyday Robots"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Deployment"}),": Google deployed RT-1 on 13 mobile manipulator robots in office kitchens (2022-2023)."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object retrieval"}),': "Bring me a Coke can from the fridge"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cleaning"}),': "Throw away the trash on the table"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Organization"}),': "Put all fruits in the fruit bowl"']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"700+ tasks"})," learned over 17 months"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"97% success rate"})," on trained tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"76% success rate"}),' on novel instructions (e.g., "move the energy drink to the drawer" when "energy drink" not in training set)']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Insights"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Long-tail failures"}),": 3% failures mostly due to rare edge cases (object stuck under furniture, gripper calibration drift)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Human intervention"}),": Operators intervened every 20-30 tasks to reset environment or fix hardware issues"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generalization limitations"}),': Struggled with novel object categories (e.g., "pick the electronics" failed on headphones, cables)']}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"rt-2-emergent-reasoning",children:"RT-2 Emergent Reasoning"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Experiment"}),": Google tested RT-2's zero-shot reasoning on 100 novel tasks requiring semantic understanding."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:'"Pick the extinct animal"'})," \u2192 Correctly selected toy dinosaur (vs. toy dog, toy cat)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:'"Move the object you\'d use to cut paper"'})," \u2192 Grasped scissors (tool affordance reasoning)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:'"Pick the smallest snack"'})," \u2192 Chose mini candy bar (size comparison + food category)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Results"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"74% success rate"})," on symbolic reasoning tasks (vs. 0% for RT-1)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chain-of-thought"}),': RT-2 can explain reasoning ("I chose the dinosaur because dinosaurs are extinct animals")']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Limitations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Fails on tasks requiring multi-step reasoning ("pick the drink, then the snack" \u2192 only picks drink)'}),"\n",(0,r.jsx)(n.li,{children:'Cannot handle ambiguity ("pick the red one" when 3 red objects present)'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"industrial-vla-deployment",children:"Industrial VLA Deployment"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Company"}),": Agility Robotics (humanoid robots for warehouses)\n",(0,r.jsx)(n.strong,{children:"Task"}),': "Sort packages by size and place on appropriate shelf"']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Setup"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Fine-tuned RT-1 on 5,000 package-sorting demonstrations"}),"\n",(0,r.jsx)(n.li,{children:"Deployed on Digit humanoid robot (biped with 2 arms)"}),"\n",(0,r.jsx)(n.li,{children:"Action space: 26 DOF (12 leg joints, 12 arm joints, 2 grippers)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sorting rate"}),": 60 packages/hour (vs. 120/hour for human workers)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": 92% correct shelf placement"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Failure modes"}),": 8% failures due to package deformation (soft packages collapse during grasp)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cost-benefit"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROI"}),": 18 months (robot cost $150K, saves $100K/year in labor)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Uptime"}),": 85% (15% downtime for charging, maintenance, software updates)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"vla-inference-workflow-pseudocode",children:"VLA Inference Workflow (Pseudocode)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nfrom vla_models import RT2\n\n# Initialize VLA model\nmodel = RT2.from_pretrained('google/rt-2-base')\nmodel.eval()  # Inference mode\n\n# Robot control loop\nwhile True:\n    # 1. Capture observation\n    image = robot.camera.get_rgb()  # (3, 224, 224)\n    instruction = \"pick up the red mug\"  # From user or task planner\n\n    # 2. Preprocess\n    image_tensor = preprocess_image(image)  # Normalize, resize\n    instruction_tokens = tokenize(instruction)  # BERT tokenization\n\n    # 3. VLA inference\n    with torch.no_grad():\n        action_logits = model(image_tensor, instruction_tokens)  # (11, 256)\n\n    # 4. Decode action\n    action_bins = torch.argmax(action_logits, dim=-1)  # (11,) - bin indices\n    action_continuous = detokenize_actions(action_bins)  # Convert bins to joint angles\n\n    # 5. Safety checks\n    action_safe = clip_actions(action_continuous, robot.limits)\n    if not check_workspace_bounds(robot.forward_kinematics(action_safe)):\n        print(\"Warning: Action outside workspace. Clipping.\")\n        action_safe = fallback_action()  # Safe default action\n\n    # 6. Execute action\n    robot.set_joint_positions(action_safe[:7])  # Arm joints\n    robot.set_gripper(action_safe[7])  # Gripper command\n\n    # 7. Detect failures\n    time.sleep(0.2)  # Wait for action to execute (5 Hz control)\n    if detect_failure(robot.get_state(), expected_outcome='grasp'):\n        print(\"Grasp failed. Retrying...\")\n        action_safe = recover_from_failure('grasp_failed')\n\n    # 8. Check task completion\n    if task_complete(robot.get_state(), instruction):\n        print(f\"Task '{instruction}' completed successfully!\")\n        break\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-analyze-demonstration-dataset",children:"Exercise 1: Analyze Demonstration Dataset"}),"\n",(0,r.jsxs)(n.p,{children:["Download a sample robot demonstration dataset (e.g., ",(0,r.jsx)(n.a,{href:"https://robotics-transformer-x.github.io/",children:"Open X-Embodiment"}),") and analyze:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Load dataset and inspect structure (images, actions, language, rewards)"}),"\n",(0,r.jsxs)(n.li,{children:["Compute statistics:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Number of demonstrations per task"}),"\n",(0,r.jsx)(n.li,{children:"Average episode length"}),"\n",(0,r.jsx)(n.li,{children:"Action distribution (mean, std for each joint)"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Visualize 10 random trajectories (plot joint angles over time)"}),"\n",(0,r.jsx)(n.li,{children:"Identify data quality issues (incomplete episodes, sensor failures, outlier actions)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected time"}),": 45 minutes\n",(0,r.jsx)(n.strong,{children:"Learning goal"}),": Understand real-world robot dataset characteristics."]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-design-fine-tuning-strategy",children:"Exercise 2: Design Fine-Tuning Strategy"}),"\n",(0,r.jsx)(n.p,{children:"You have a pretrained RT-2 model trained on Google's mobile manipulator. You want to deploy it on a Franka Emika Panda (7-DOF arm, no mobile base)."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Compare action spaces: Google robot (7 arm + 3 base + 1 gripper = 11D) vs. Franka (7 arm + 1 gripper = 8D)"}),"\n",(0,r.jsx)(n.li,{children:"Design action space mapping function (pseudocode or Python)"}),"\n",(0,r.jsx)(n.li,{children:"Estimate data requirements: How many Franka demonstrations needed for 80% success rate?"}),"\n",(0,r.jsx)(n.li,{children:"Propose domain adaptation techniques to minimize data collection"}),"\n",(0,r.jsx)(n.li,{children:"Identify potential failure modes and propose mitigation strategies"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected time"}),": 40 minutes\n",(0,r.jsx)(n.strong,{children:"Learning goal"}),": Apply transfer learning to new robot platforms."]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-estimate-inference-time-and-optimize",children:"Exercise 3: Estimate Inference Time and Optimize"}),"\n",(0,r.jsx)(n.p,{children:"You're deploying RT-2 (55B parameters) on a robot with NVIDIA Jetson AGX Orin (edge GPU)."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Estimate inference time: Vision encoder (ViT-e 4B params) + language model (UL2 51B params)"}),"\n",(0,r.jsx)(n.li,{children:"Target: 10 Hz control loop (100ms per action). Can you meet this target with full RT-2?"}),"\n",(0,r.jsxs)(n.li,{children:["Propose optimizations:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Model quantization (FP32 \u2192 INT8): 2-3\xd7 speedup"}),"\n",(0,r.jsx)(n.li,{children:"Model pruning: 30% weights removed"}),"\n",(0,r.jsx)(n.li,{children:'Distillation: Train 1B param "student" model'}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["For each optimization, estimate:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Expected inference time"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy trade-off (% drop in success rate)"}),"\n",(0,r.jsx)(n.li,{children:"Implementation effort (hours)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected time"}),": 50 minutes\n",(0,r.jsx)(n.strong,{children:"Learning goal"}),": Understand deployment constraints and optimization strategies."]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-4-identify-safety-constraints",children:"Exercise 4: Identify Safety Constraints"}),"\n",(0,r.jsx)(n.p,{children:"Design a safety validation system for a VLA-controlled robot in a household environment (kitchen)."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"List 10 potential safety hazards (e.g., collision with human, drop fragile object)"}),"\n",(0,r.jsxs)(n.li,{children:["For each hazard, design a safety check:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Input: What sensor data is needed? (joint torques, camera feed, proximity sensors)"}),"\n",(0,r.jsx)(n.li,{children:"Logic: What condition triggers safety violation?"}),"\n",(0,r.jsx)(n.li,{children:"Response: What action should robot take? (stop, retract, alert user)"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Implement 3 safety checks in pseudocode or Python"}),"\n",(0,r.jsx)(n.li,{children:"Estimate false positive rate (safety check triggers when no actual danger exists)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected time"}),": 45 minutes\n",(0,r.jsx)(n.strong,{children:"Learning goal"}),": Proactively design safety mechanisms for VLA deployment."]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-5-propose-deployment-architecture",children:"Exercise 5: Propose Deployment Architecture"}),"\n",(0,r.jsx)(n.p,{children:"Design a deployment architecture for a fleet of 10 robots in a warehouse using RT-2 for package sorting."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Components to specify"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compute"}),": Where does VLA inference run? (edge, cloud, hybrid)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Network"}),": What connectivity is required? (Wi-Fi, 5G, wired Ethernet)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fault tolerance"}),": What happens if cloud connection drops?"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Monitoring"}),": What metrics are logged? (success rate, inference time, failures)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Updates"}),": How are model updates deployed without downtime?"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Deliverable"}),": Architecture diagram (use Mermaid or draw.io) with annotations."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected time"}),": 60 minutes\n",(0,r.jsx)(n.strong,{children:"Learning goal"}),": Design production-ready VLA deployment systems."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"VLA training data requires multimodal alignment"})," (synchronized images, language, actions), temporal consistency (state transition sequences), and diversity (hundreds of tasks, multiple environments), collected primarily through teleoperation at 5-10 minutes per demo and augmented with spatial, temporal, and language variations to increase effective dataset size by 5-10\xd7."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Synthetic data from Isaac Gym reduces real-world collection costs"})," by generating 100K+ demonstrations via RL policies trained with domain randomization, achieving 70-85% of real-data performance when combined with 10% real-world demos, though sim-to-real gap requires domain adaptation (CycleGAN, few-shot fine-tuning) to bridge visual and physics differences."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Fine-tuning pretrained VLA models"})," is more efficient than training from scratch, requiring only 1K-10K task-specific demos and fine-tuning the action decoder while freezing vision/language encoders, using AdamW optimizer with low learning rates (1e-5 to 1e-4), cosine decay schedules, and gradient clipping to prevent instability."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Domain adaptation enables cross-robot transfer"})," by mapping action spaces (7-DOF to 6-DOF arms), adapting camera viewpoints (CycleGAN translation), and few-shot fine-tuning on 100-1,000 calibration demos, achieving 70-85% of source robot performance with 5-10\xd7 less data than training from scratch."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Deployment constraints require optimization"})," because real-time robot control needs 10-20 Hz (50-100ms inference), but large VLA models (RT-2: 55B params) run at 3 Hz on A100 GPU, necessitating quantization (FP32\u2192INT8 for 2-3\xd7 speedup), pruning (remove 30-50% weights), or distillation (1B student model mimics 55B teacher) to meet latency targets on edge devices."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Safety mechanisms prevent harmful actions"})," through kinematic limit clipping (joint angles/velocities), collision detection (force-torque thresholds), workspace boundary checks (end-effector position validation), and emergency stops (physical button + software watchdog), with failure detection and recovery strategies (retry grasp, change viewpoint, request user clarification) to handle perception errors, execution failures, and task ambiguity."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Real-world case studies demonstrate practical impact"}),' with RT-1 achieving 97% success on 700+ tasks in Google\'s office kitchens, RT-2 unlocking 74% success on zero-shot symbolic reasoning ("pick extinct animal"), and industrial deployments (Agility Robotics warehouse sorting) reaching 92% accuracy at 60 packages/hour, though challenges remain in long-tail failures, multi-step reasoning, and ambiguity handling.']}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"navigation",children:"Navigation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,r.jsx)(n.a,{href:"/docs/module-4-vla/vla-architectures",children:"VLA Architectures: RT-1, RT-2, PaLM-E"}),"\n",(0,r.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,r.jsx)(n.a,{href:"/docs/module-4-vla/voice-to-action",children:"Voice-to-Action with OpenAI Whisper"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);