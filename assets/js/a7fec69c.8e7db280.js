"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_01=globalThis.webpackChunkphysical_ai_humanoid_robotics_01||[]).push([[511],{2163:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/voice-to-action","title":"Voice-to-Action with OpenAI Whisper","description":"Implement voice-controlled robotics using speech recognition and natural language processing","source":"@site/docs/module-4-vla/voice-to-action.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/voice-to-action","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/voice-to-action.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Voice-to-Action with OpenAI Whisper","description":"Implement voice-controlled robotics using speech recognition and natural language processing"},"sidebar":"mainSidebar","previous":{"title":"Training and Deploying VLA Systems","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/vla-training-deployment"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/cognitive-planning"}}');var o=i(4848),r=i(8453);const s={sidebar_position:4,title:"Voice-to-Action with OpenAI Whisper",description:"Implement voice-controlled robotics using speech recognition and natural language processing"},a="Voice-to-Action with OpenAI Whisper",l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Voice-to-Action Architecture",id:"voice-to-action-architecture",level:2},{value:"Setting Up the Development Environment",id:"setting-up-the-development-environment",level:2},{value:"1. Install Dependencies",id:"1-install-dependencies",level:3},{value:"2. Configure OpenAI API",id:"2-configure-openai-api",level:3},{value:"3. Test Microphone Access",id:"3-test-microphone-access",level:3},{value:"Building the Speech Recognition Module",id:"building-the-speech-recognition-module",level:2},{value:"Core Whisper Integration",id:"core-whisper-integration",level:3},{value:"Real-Time Audio Capture",id:"real-time-audio-capture",level:3},{value:"Natural Language Understanding with GPT-4",id:"natural-language-understanding-with-gpt-4",level:2},{value:"Command Extraction Pipeline",id:"command-extraction-pipeline",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"Action Server for Robot Control",id:"action-server-for-robot-control",level:3},{value:"Complete Voice Control System",id:"complete-voice-control-system",level:2},{value:"Main Application",id:"main-application",level:3},{value:"Exercise 1: Voice-Controlled Navigation",id:"exercise-1-voice-controlled-navigation",level:2},{value:"Exercise 2: Multi-Lingual Voice Control",id:"exercise-2-multi-lingual-voice-control",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-to-action-with-openai-whisper",children:"Voice-to-Action with OpenAI Whisper"})}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before starting this chapter, you should have:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\u2705 Completed Modules 1-3 (ROS 2, Simulation, NVIDIA Isaac)"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Understanding of audio processing basics"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Familiarity with large language models (LLMs)"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Python programming experience with async/await"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 OpenAI API access (for Whisper and GPT models)"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Estimated Reading Time"}),": 25-30 minutes"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(n.p,{children:["Voice interaction represents the most natural interface between humans and robots. While traditional robot control requires joysticks, keyboards, or programmed sequences, voice commands enable intuitive, hands-free operation. Imagine instructing a humanoid robot: ",(0,o.jsx)(n.em,{children:'"Pick up the red cup from the table and place it in the recycling bin"'})," \u2014 and watching the robot execute the task autonomously."]}),"\n",(0,o.jsxs)(n.p,{children:["This chapter demonstrates how to build a complete voice-to-action pipeline using ",(0,o.jsx)(n.strong,{children:"OpenAI Whisper"})," for speech recognition, ",(0,o.jsx)(n.strong,{children:"GPT-4"})," for natural language understanding, and ",(0,o.jsx)(n.strong,{children:"ROS 2"})," for robot control. You'll learn to process audio input, translate speech to text, extract actionable commands, and trigger robot behaviors."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Why Whisper?"})," OpenAI's Whisper is a state-of-the-art automatic speech recognition (ASR) model trained on 680,000 hours of multilingual data. Unlike traditional ASR systems that require wake words or limited vocabulary, Whisper handles natural speech, accents, background noise, and 99 languages. This makes it ideal for real-world robotics applications where environmental conditions vary."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Learning Objectives"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate OpenAI Whisper for real-time speech recognition"}),"\n",(0,o.jsx)(n.li,{children:"Process voice commands and extract robot actions"}),"\n",(0,o.jsx)(n.li,{children:"Implement natural language understanding (NLU) pipelines"}),"\n",(0,o.jsx)(n.li,{children:"Handle multi-lingual voice interactions"}),"\n",(0,o.jsx)(n.li,{children:"Build error handling and confirmation workflows"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"voice-to-action-architecture",children:"Voice-to-Action Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The voice-to-action system consists of four main stages:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Microphone Audio] --\x3e B[Whisper ASR]\n    B --\x3e C[GPT-4 NLU]\n    C --\x3e D[Action Executor]\n    D --\x3e E[ROS 2 Robot Control]\n    E --\x3e F[Humanoid Robot]\n    F --\x3e G[Speech Feedback]\n    G --\x3e A\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Pipeline Stages"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Capture"}),": Record user speech via microphone (ReSpeaker, USB mic)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition (ASR)"}),": Convert audio \u2192 text using Whisper"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Extract intent and parameters using GPT-4"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Execution"}),": Map commands to ROS 2 actions (move, grasp, navigate)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feedback Loop"}),": Confirm execution via text-to-speech (TTS)"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"setting-up-the-development-environment",children:"Setting Up the Development Environment"}),"\n",(0,o.jsx)(n.h3,{id:"1-install-dependencies",children:"1. Install Dependencies"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Create a ROS 2 workspace for voice control\nmkdir -p ~/robot_ws/src\ncd ~/robot_ws/src\n\n# Install Python dependencies\npip install openai pyaudio numpy soundfile\n\n# Install ROS 2 audio packages\nsudo apt install ros-humble-audio-common portaudio19-dev\n\n# Install Whisper (local option - optional)\npip install openai-whisper\n"})}),"\n",(0,o.jsx)(n.h3,{id:"2-configure-openai-api",children:"2. Configure OpenAI API"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# config.py\nimport os\n\nOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # Set via: export OPENAI_API_KEY="sk-..."\nWHISPER_MODEL = "whisper-1"  # API model\nGPT_MODEL = "gpt-4"\n\n# Audio settings\nSAMPLE_RATE = 16000  # 16 kHz (Whisper requirement)\nCHUNK_SIZE = 1024\nCHANNELS = 1  # Mono\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-test-microphone-access",children:"3. Test Microphone Access"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# test_mic.py\nimport pyaudio\nimport numpy as np\n\ndef test_microphone():\n    p = pyaudio.PyAudio()\n\n    # List available devices\n    print("Available audio devices:")\n    for i in range(p.get_device_count()):\n        info = p.get_device_info_by_index(i)\n        print(f"{i}: {info[\'name\']} (Channels: {info[\'maxInputChannels\']})")\n\n    # Record 3 seconds\n    stream = p.open(format=pyaudio.paInt16, channels=1, rate=16000,\n                    input=True, frames_per_buffer=1024)\n    print("\\nRecording for 3 seconds...")\n    frames = [stream.read(1024) for _ in range(int(16000 / 1024 * 3))]\n    stream.close()\n    p.terminate()\n\n    audio = np.frombuffer(b\'\'.join(frames), dtype=np.int16)\n    print(f"Recorded {len(audio)} samples. Max amplitude: {np.max(np.abs(audio))}")\n\nif __name__ == "__main__":\n    test_microphone()\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"building-the-speech-recognition-module",children:"Building the Speech Recognition Module"}),"\n",(0,o.jsx)(n.h3,{id:"core-whisper-integration",children:"Core Whisper Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# whisper_asr.py\nimport openai\nfrom openai import OpenAI\nimport soundfile as sf\nimport io\n\nclass WhisperASR:\n    def __init__(self, api_key: str):\n        self.client = OpenAI(api_key=api_key)\n\n    def transcribe(self, audio_data: bytes, language: str = "en") -> str:\n        """\n        Convert audio bytes to text using Whisper API.\n\n        Args:\n            audio_data: Raw audio bytes (WAV format)\n            language: ISO language code (en, ur, es, fr, etc.)\n\n        Returns:\n            Transcribed text string\n        """\n        try:\n            # Create in-memory file-like object\n            audio_file = io.BytesIO(audio_data)\n            audio_file.name = "audio.wav"\n\n            # Call Whisper API\n            transcript = self.client.audio.transcriptions.create(\n                model="whisper-1",\n                file=audio_file,\n                language=language,\n                response_format="text"\n            )\n\n            return transcript.strip()\n\n        except Exception as e:\n            print(f"Whisper API error: {e}")\n            return ""\n\n    def transcribe_with_timestamps(self, audio_data: bytes):\n        """Get word-level timestamps (useful for alignment)"""\n        audio_file = io.BytesIO(audio_data)\n        audio_file.name = "audio.wav"\n\n        response = self.client.audio.transcriptions.create(\n            model="whisper-1",\n            file=audio_file,\n            response_format="verbose_json",\n            timestamp_granularities=["word"]\n        )\n\n        return response  # Contains words array with timestamps\n'})}),"\n",(0,o.jsx)(n.h3,{id:"real-time-audio-capture",children:"Real-Time Audio Capture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# audio_capture.py\nimport pyaudio\nimport wave\nimport numpy as np\nfrom collections import deque\n\nclass AudioCapture:\n    def __init__(self, sample_rate=16000, chunk_size=1024):\n        self.sample_rate = sample_rate\n        self.chunk_size = chunk_size\n        self.p = pyaudio.PyAudio()\n        self.stream = None\n        self.buffer = deque(maxlen=int(sample_rate * 10))  # 10-sec buffer\n\n    def start_stream(self):\n        self.stream = self.p.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size,\n            stream_callback=self._audio_callback\n        )\n        self.stream.start_stream()\n\n    def _audio_callback(self, in_data, frame_count, time_info, status):\n        audio_chunk = np.frombuffer(in_data, dtype=np.int16)\n        self.buffer.extend(audio_chunk)\n        return (in_data, pyaudio.paContinue)\n\n    def get_audio_segment(self, duration_sec=5):\n        """Extract last N seconds of audio"""\n        samples = int(self.sample_rate * duration_sec)\n        audio = np.array(list(self.buffer)[-samples:], dtype=np.int16)\n\n        # Convert to WAV bytes\n        wav_buffer = io.BytesIO()\n        with wave.open(wav_buffer, \'wb\') as wf:\n            wf.setnchannels(1)\n            wf.setsampwidth(2)  # 16-bit\n            wf.setframerate(self.sample_rate)\n            wf.writeframes(audio.tobytes())\n\n        wav_buffer.seek(0)\n        return wav_buffer.read()\n\n    def stop(self):\n        if self.stream:\n            self.stream.stop_stream()\n            self.stream.close()\n        self.p.terminate()\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"natural-language-understanding-with-gpt-4",children:"Natural Language Understanding with GPT-4"}),"\n",(0,o.jsxs)(n.p,{children:["Once we have the transcribed text, we need to extract ",(0,o.jsx)(n.strong,{children:"actionable commands"}),". For example:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["User says: ",(0,o.jsx)(n.em,{children:'"Move forward 2 meters and turn left"'})]}),"\n",(0,o.jsxs)(n.li,{children:["Expected output: ",(0,o.jsx)(n.code,{children:'[{action: "move", distance: 2.0, direction: "forward"}, {action: "turn", angle: 90, direction: "left"}]'})]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"command-extraction-pipeline",children:"Command Extraction Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# nlu_processor.py\nfrom openai import OpenAI\nimport json\n\nclass NLUProcessor:\n    def __init__(self, api_key: str):\n        self.client = OpenAI(api_key=api_key)\n        self.system_prompt = """You are a robot command interpreter.\nExtract robot actions from natural language commands.\n\nAvailable actions:\n- move(direction, distance_meters)\n- turn(direction, angle_degrees)\n- grasp(object_name, hand)\n- release(hand)\n- navigate_to(location_name)\n- wait(seconds)\n\nOutput JSON array of actions. Example:\nUser: "Go forward 3 meters then pick up the red cup"\nOutput: [{"action": "move", "direction": "forward", "distance": 3.0},\n         {"action": "grasp", "object": "red cup", "hand": "right"}]\n"""\n\n    def extract_actions(self, text: str) -> list:\n        """Convert natural language \u2192 structured actions"""\n        try:\n            response = self.client.chat.completions.create(\n                model="gpt-4",\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": text}\n                ],\n                temperature=0.0,  # Deterministic output\n                response_format={"type": "json_object"}\n            )\n\n            result = json.loads(response.choices[0].message.content)\n            return result.get("actions", [])\n\n        except Exception as e:\n            print(f"NLU error: {e}")\n            return []\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,o.jsx)(n.h3,{id:"action-server-for-robot-control",children:"Action Server for Robot Control"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# robot_action_server.py\nimport rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nimport time\n\nclass RobotActionServer(Node):\n    def __init__(self):\n        super().__init__(\'robot_action_server\')\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.status_pub = self.create_publisher(String, \'/robot/status\', 10)\n\n    def execute_move(self, direction: str, distance: float):\n        """Execute linear movement"""\n        twist = Twist()\n\n        # Map direction to velocity\n        speed = 0.5  # m/s\n        if direction == "forward":\n            twist.linear.x = speed\n        elif direction == "backward":\n            twist.linear.x = -speed\n\n        # Calculate duration\n        duration = distance / speed\n\n        # Publish velocity\n        start_time = time.time()\n        while time.time() - start_time < duration:\n            self.cmd_vel_pub.publish(twist)\n            time.sleep(0.1)\n\n        # Stop\n        self.cmd_vel_pub.publish(Twist())\n        self.get_logger().info(f"Moved {direction} {distance}m")\n\n    def execute_turn(self, direction: str, angle: float):\n        """Execute rotation"""\n        twist = Twist()\n        angular_speed = 0.5  # rad/s\n\n        if direction == "left":\n            twist.angular.z = angular_speed\n        elif direction == "right":\n            twist.angular.z = -angular_speed\n\n        # Convert degrees to radians and calculate duration\n        angle_rad = angle * (3.14159 / 180.0)\n        duration = angle_rad / angular_speed\n\n        start_time = time.time()\n        while time.time() - start_time < duration:\n            self.cmd_vel_pub.publish(twist)\n            time.sleep(0.1)\n\n        self.cmd_vel_pub.publish(Twist())\n        self.get_logger().info(f"Turned {direction} {angle}\xb0")\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"complete-voice-control-system",children:"Complete Voice Control System"}),"\n",(0,o.jsx)(n.h3,{id:"main-application",children:"Main Application"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# voice_robot_control.py\nimport rclpy\nfrom whisper_asr import WhisperASR\nfrom nlu_processor import NLUProcessor\nfrom robot_action_server import RobotActionServer\nfrom audio_capture import AudioCapture\nimport os\n\ndef main():\n    # Initialize ROS 2\n    rclpy.init()\n    robot = RobotActionServer()\n\n    # Initialize AI models\n    api_key = os.getenv("OPENAI_API_KEY")\n    whisper = WhisperASR(api_key)\n    nlu = NLUProcessor(api_key)\n\n    # Start audio capture\n    audio = AudioCapture()\n    audio.start_stream()\n\n    print("\ud83c\udfa4 Voice control active. Say commands (Ctrl+C to exit):")\n\n    try:\n        while rclpy.ok():\n            input("Press Enter to record command...")\n\n            # Capture 5 seconds of audio\n            audio_data = audio.get_audio_segment(duration_sec=5)\n\n            # Speech \u2192 Text\n            text = whisper.transcribe(audio_data)\n            print(f"\ud83d\udcdd Heard: {text}")\n\n            if not text:\n                continue\n\n            # Text \u2192 Actions\n            actions = nlu.extract_actions(text)\n            print(f"\ud83e\udd16 Actions: {actions}")\n\n            # Execute actions\n            for action in actions:\n                if action["action"] == "move":\n                    robot.execute_move(action["direction"], action["distance"])\n                elif action["action"] == "turn":\n                    robot.execute_turn(action["direction"], action["angle"])\n                # Add more action handlers...\n\n            print("\u2705 Command completed\\n")\n\n    except KeyboardInterrupt:\n        print("\\nShutting down...")\n    finally:\n        audio.stop()\n        robot.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"exercise-1-voice-controlled-navigation",children:"Exercise 1: Voice-Controlled Navigation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Task"}),": Create a voice-controlled robot that can navigate a simple environment."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:'Accept commands: "Go to the kitchen", "Move to the bedroom"'}),"\n",(0,o.jsx)(n.li,{children:"Use Nav2 for path planning"}),"\n",(0,o.jsx)(n.li,{children:'Provide voice feedback: "Navigating to kitchen..."'}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Starter Code"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def navigate_to_location(location: str):\n    """Navigate to named location using Nav2"""\n    # TODO: Implement Nav2 goal publisher\n    # TODO: Add location name \u2192 coordinates mapping\n    pass\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,o.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Robot responds to at least 5 location names"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Handles obstacles using Nav2"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Provides audio confirmation"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"exercise-2-multi-lingual-voice-control",children:"Exercise 2: Multi-Lingual Voice Control"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Task"}),": Extend the system to support Urdu voice commands."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Example Commands"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Urdu: ",(0,o.jsx)(n.em,{children:'"\u0622\u06af\u06d2 \u0628\u0691\u06be\u0648"'})," (Move forward)"]}),"\n",(0,o.jsxs)(n.li,{children:["Urdu: ",(0,o.jsx)(n.em,{children:'"\u062f\u0627\u0626\u06cc\u06ba \u0645\u0691\u0648"'})," (Turn right)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Implementation Hints"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Detect language and transcribe\ntranscript = whisper.transcribe(audio_data, language="ur")\n\n# Add Urdu action mapping\nURDU_ACTIONS = {\n    "\u0622\u06af\u06d2": "forward",\n    "\u067e\u06cc\u0686\u06be\u06d2": "backward",\n    "\u062f\u0627\u0626\u06cc\u06ba": "right",\n    "\u0628\u0627\u0626\u06cc\u06ba": "left"\n}\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Issue"}),(0,o.jsx)(n.th,{children:"Cause"}),(0,o.jsx)(n.th,{children:"Solution"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"High latency (>5s)"}),(0,o.jsx)(n.td,{children:"Cloud API round-trip"}),(0,o.jsxs)(n.td,{children:["Use local Whisper model: ",(0,o.jsx)(n.code,{children:'whisper.load_model("base")'})]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Background noise"}),(0,o.jsx)(n.td,{children:"Poor microphone"}),(0,o.jsxs)(n.td,{children:["Use noise cancellation: ",(0,o.jsx)(n.code,{children:"noisereduce"})," library"]})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Misrecognition"}),(0,o.jsx)(n.td,{children:"Accent/unclear speech"}),(0,o.jsx)(n.td,{children:'Add confirmation step: "Did you say: move forward?"'})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Command timeouts"}),(0,o.jsx)(n.td,{children:"Network issues"}),(0,o.jsx)(n.td,{children:"Implement retry logic with exponential backoff"})]})]})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Whisper"})," provides state-of-the-art speech recognition with 99-language support"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"GPT-4"})," enables flexible natural language understanding without hard-coded grammars"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2"})," provides the robot control layer for action execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Confirmation loops"})," prevent dangerous misinterpretations"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Local models"})," (Whisper base) reduce latency for real-time applications"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Voice control transforms robot interaction from technical to conversational. By combining ASR, NLU, and robot control, you create systems that feel truly intelligent."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,o.jsx)(n.a,{href:"/docs/module-4-vla/vla-training-deployment",children:"Training and Deploying VLA Systems"}),"\n",(0,o.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,o.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/cognitive-planning",children:"Cognitive Planning with LLMs"})]}),"\n",(0,o.jsx)(n.p,{children:"In the next chapter, we'll explore how LLMs can reason about complex tasks, break them into subtasks, and adapt plans when the environment changes."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);