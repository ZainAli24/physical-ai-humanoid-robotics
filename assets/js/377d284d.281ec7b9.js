"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_01=globalThis.webpackChunkphysical_ai_humanoid_robotics_01||[]).push([[602],{3672:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-3-isaac/ai-powered-perception","title":"AI-Powered Perception","description":"Implement object detection, segmentation, pose estimation, and depth perception for robotic manipulation using Isaac Sim","source":"@site/docs/module-3-isaac/ai-powered-perception.md","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/ai-powered-perception","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ai-powered-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/ai-powered-perception.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"AI-Powered Perception","description":"Implement object detection, segmentation, pose estimation, and depth perception for robotic manipulation using Isaac Sim"},"sidebar":"mainSidebar","previous":{"title":"Isaac Platform Architecture","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-sdk-overview"},"next":{"title":"Advanced RL and Sim-to-Real Transfer","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/reinforcement-learning"}}');var o=i(4848),r=i(8453);const s={sidebar_position:5,title:"AI-Powered Perception",description:"Implement object detection, segmentation, pose estimation, and depth perception for robotic manipulation using Isaac Sim"},a="AI-Powered Perception",c={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:2},{value:"Why Synthetic Data?",id:"why-synthetic-data",level:3},{value:"Setting Up a Data Generation Scene",id:"setting-up-a-data-generation-scene",level:3},{value:"Data Output Format",id:"data-output-format",level:3},{value:"Object Detection with Isaac Sim",id:"object-detection-with-isaac-sim",level:2},{value:"Using Pre-Trained DetectNet",id:"using-pre-trained-detectnet",level:3},{value:"6D Pose Estimation",id:"6d-pose-estimation",level:2},{value:"What is 6D Pose?",id:"what-is-6d-pose",level:3},{value:"DOPE (Deep Object Pose Estimation)",id:"dope-deep-object-pose-estimation",level:3},{value:"Depth Perception",id:"depth-perception",level:2},{value:"Processing Depth Images",id:"processing-depth-images",level:3},{value:"Complete Perception Pipeline: Pick and Place",id:"complete-perception-pipeline-pick-and-place",level:2},{value:"Integration with Manipulation",id:"integration-with-manipulation",level:3},{value:"Domain Randomization for Sim-to-Real Transfer",id:"domain-randomization-for-sim-to-real-transfer",level:2},{value:"Why Randomization?",id:"why-randomization",level:3},{value:"Implementing Domain Randomization",id:"implementing-domain-randomization",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"ai-powered-perception",children:"AI-Powered Perception"})}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before starting this chapter, you should have:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"\u2705 Completed Isaac Platform Architecture chapter"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Understanding of computer vision fundamentals (CNNs, image classification)"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Isaac Sim 2023.1.0 or newer installed"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Python deep learning frameworks (PyTorch or TensorFlow)"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 Familiarity with ROS 2 sensor messages"}),"\n",(0,o.jsx)(n.li,{children:"\u2705 NVIDIA RTX GPU (8GB+ VRAM for inference)"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Estimated Reading Time"}),": 30-35 minutes"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(n.p,{children:["Robots must ",(0,o.jsx)(n.strong,{children:"perceive"})," their environment to act intelligently. Unlike traditional rule-based vision (template matching, color thresholding), ",(0,o.jsx)(n.strong,{children:"AI-powered perception"})," uses deep learning models to understand complex, unstructured scenes. Isaac Sim provides:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Photorealistic images with automatic ground-truth labeling"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pre-trained Models"}),": NVIDIA-optimized models for object detection, segmentation, pose estimation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Domain Randomization"}),": Sim-to-real transfer techniques to reduce the reality gap"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor Simulation"}),": RGB, depth, segmentation, lidar for comprehensive perception"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This chapter demonstrates how to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Generate synthetic training data with Isaac Sim"}),"\n",(0,o.jsx)(n.li,{children:"Deploy pre-trained vision models (DOPE, FoundationPose, DetectNet)"}),"\n",(0,o.jsx)(n.li,{children:"Implement 6D pose estimation for manipulation"}),"\n",(0,o.jsx)(n.li,{children:"Process depth data for obstacle avoidance"}),"\n",(0,o.jsx)(n.li,{children:"Create perception pipelines for pick-and-place tasks"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Why AI-Powered Perception?"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Handles occlusions, lighting variations, object diversity"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Generalization"}),": Trained models work on unseen object instances"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speed"}),": GPU-accelerated inference (30-240 FPS)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration"}),": Easy connection to manipulation planners (MoveIt, Isaac manipulators)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Learning Objectives"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Generate synthetic datasets with semantic labels in Isaac Sim"}),"\n",(0,o.jsx)(n.li,{children:"Deploy object detection models (YOLO, DetectNet, GroundingDINO)"}),"\n",(0,o.jsx)(n.li,{children:"Estimate 6D object poses using DOPE and FoundationPose"}),"\n",(0,o.jsx)(n.li,{children:"Process depth images for 3D scene reconstruction"}),"\n",(0,o.jsx)(n.li,{children:"Build complete perception-action pipelines for manipulation"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,o.jsx)(n.h3,{id:"why-synthetic-data",children:"Why Synthetic Data?"}),"\n",(0,o.jsx)(n.p,{children:"Real-world data collection is:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Expensive"}),": Manual labeling costs $0.10-$1.00 per bounding box"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Slow"}),": 1000-image dataset = weeks of collection + annotation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Limited"}),": Hard to cover edge cases (unusual poses, lighting, occlusions)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Isaac Sim generates ",(0,o.jsx)(n.strong,{children:"millions of labeled images"})," automatically:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Automatic ground-truth (bounding boxes, segmentation masks, 3D poses)"}),"\n",(0,o.jsx)(n.li,{children:"Infinite scene variations (lighting, camera angles, object arrangements)"}),"\n",(0,o.jsx)(n.li,{children:"Rare event simulation (objects falling, extreme occlusions)"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"setting-up-a-data-generation-scene",children:"Setting Up a Data Generation Scene"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# synthetic_data_gen.py\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.replicator.core import Writer, AnnotatorRegistry\nimport omni.replicator.core as rep\nimport random\n\n# Initialize world\nworld = World(stage_units_in_meters=1.0)\nworld.scene.add_default_ground_plane()\n\n# Add camera\ncamera = rep.create.camera(position=(1.5, 1.5, 1.0), look_at=(0, 0, 0.5))\n\n# Load object assets\nassets_root = get_assets_root_path()\nobject_paths = [\n    f"{assets_root}/Isaac/Props/YCB/Axis_Aligned/003_cracker_box.usd",\n    f"{assets_root}/Isaac/Props/YCB/Axis_Aligned/004_sugar_box.usd",\n    f"{assets_root}/Isaac/Props/YCB/Axis_Aligned/005_tomato_soup_can.usd",\n    f"{assets_root}/Isaac/Props/YCB/Axis_Aligned/006_mustard_bottle.usd"\n]\n\n# Randomizer function\ndef randomize_scene():\n    """Place objects in random positions with random orientations"""\n    for i, obj_path in enumerate(object_paths):\n        x = random.uniform(-0.3, 0.3)\n        y = random.uniform(-0.3, 0.3)\n        z = random.uniform(0.2, 0.8)\n\n        rotation = (random.uniform(0, 360), random.uniform(0, 360), random.uniform(0, 360))\n\n        obj_prim = add_reference_to_stage(\n            obj_path,\n            f"/World/Object_{i}"\n        )\n\n        # Set transform\n        rep.modify.pose(\n            obj_prim,\n            position=(x, y, z),\n            rotation=rotation\n        )\n\n    # Randomize lighting\n    light_intensity = random.uniform(500, 2000)\n    rep.modify.attribute(\n        "/World/defaultLight",\n        "intensity",\n        light_intensity\n    )\n\n# Register randomizer\nrep.randomizer.register(randomize_scene)\n\n# Define output writers\noutput_dir = "./synthetic_data"\n\n# RGB writer\nrgb_writer = rep.WriterRegistry.get("BasicWriter")\nrgb_writer.initialize(\n    output_dir=f"{output_dir}/rgb",\n    rgb=True,\n    bounding_box_2d_tight=True,  # Object detection labels\n    semantic_segmentation=True,   # Segmentation masks\n    distance_to_camera=True       # Depth images\n)\n\n# Generate 1000 frames\nwith rep.trigger.on_frame(num_frames=1000):\n    rep.randomizer.randomize_scene()\n\nrep.orchestrator.run()\nprint(f"Generated 1000 synthetic images in {output_dir}")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"data-output-format",children:"Data Output Format"}),"\n",(0,o.jsx)(n.p,{children:"Isaac Replicator generates:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"synthetic_data/\n\u251c\u2500\u2500 rgb/\n\u2502   \u251c\u2500\u2500 rgb_0000.png\n\u2502   \u251c\u2500\u2500 rgb_0001.png\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 bounding_box_2d_tight/\n\u2502   \u251c\u2500\u2500 bbox_0000.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 semantic_segmentation/\n\u2502   \u251c\u2500\u2500 seg_0000.png\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 distance_to_camera/\n    \u251c\u2500\u2500 depth_0000.npy\n    \u2514\u2500\u2500 ...\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Bounding Box JSON Format"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "objects": [\n    {\n      "class": "cracker_box",\n      "bbox": [120, 85, 245, 310],\n      "confidence": 1.0,\n      "occlusion": 0.15\n    }\n  ]\n}\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"object-detection-with-isaac-sim",children:"Object Detection with Isaac Sim"}),"\n",(0,o.jsx)(n.h3,{id:"using-pre-trained-detectnet",children:"Using Pre-Trained DetectNet"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"DetectNet"})," is NVIDIA's object detection model optimized for robotics."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# detectnet_inference.py\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.extensions import enable_extension\nimport torch\nimport torchvision\nimport numpy as np\n\n# Enable Isaac sensor extension\nenable_extension("omni.isaac.sensor")\n\nfrom omni.isaac.sensor import Camera\n\nclass ObjectDetector:\n    def __init__(self, model_path="detectnet_v2.pth", confidence_threshold=0.5):\n        """Initialize object detector"""\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        self.model = torch.load(model_path).to(self.device)\n        self.model.eval()\n        self.confidence_threshold = confidence_threshold\n\n    def detect(self, image):\n        """\n        Detect objects in image.\n\n        Args:\n            image: NumPy array (H, W, 3) in RGB format\n\n        Returns:\n            List of detections: [{"class": str, "bbox": [x1,y1,x2,y2], "confidence": float}]\n        """\n        # Preprocess\n        image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n        image_tensor = image_tensor.unsqueeze(0).to(self.device)\n\n        # Inference\n        with torch.no_grad():\n            predictions = self.model(image_tensor)\n\n        # Post-process\n        detections = []\n        boxes = predictions[0]["boxes"].cpu().numpy()\n        scores = predictions[0]["scores"].cpu().numpy()\n        labels = predictions[0]["labels"].cpu().numpy()\n\n        for box, score, label in zip(boxes, scores, labels):\n            if score > self.confidence_threshold:\n                detections.append({\n                    "class": self.idx_to_class(label),\n                    "bbox": box.tolist(),\n                    "confidence": float(score)\n                })\n\n        return detections\n\n    def idx_to_class(self, idx):\n        """Map class index to name"""\n        class_names = ["background", "cracker_box", "sugar_box", "soup_can", "mustard_bottle"]\n        return class_names[idx]\n\n# Usage in Isaac Sim\nworld = World()\ncamera = Camera(\n    prim_path="/World/Camera",\n    position=[1.5, 0, 1.0],\n    frequency=30  # 30 Hz\n)\n\ndetector = ObjectDetector("detectnet_v2.pth")\n\nfor _ in range(100):\n    world.step()\n\n    # Get camera image\n    rgb_image = camera.get_rgba()[:, :, :3]  # Drop alpha channel\n\n    # Detect objects\n    detections = detector.detect(rgb_image)\n\n    for det in detections:\n        print(f"Detected {det[\'class\']} at {det[\'bbox\']} (confidence: {det[\'confidence\']:.2f})")\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"6d-pose-estimation",children:"6D Pose Estimation"}),"\n",(0,o.jsx)(n.h3,{id:"what-is-6d-pose",children:"What is 6D Pose?"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"6D Pose"})," = 3D position (x, y, z) + 3D orientation (roll, pitch, yaw)"]}),"\n",(0,o.jsx)(n.p,{children:"Critical for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grasping"}),": Align gripper with object orientation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Placement"}),": Put objects in specific poses"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation"}),": Track object movement during contact"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"dope-deep-object-pose-estimation",children:"DOPE (Deep Object Pose Estimation)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"DOPE"})," estimates 6D poses from RGB images using CNN keypoints."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# dope_pose_estimation.py\nfrom omni.isaac.dope import DOPE\nfrom omni.isaac.core import World\nimport numpy as np\n\nclass PoseEstimator:\n    def __init__(self):\n        """Initialize DOPE model"""\n        self.dope = DOPE()\n        self.dope.load_model("dope_cracker_box.pth")\n\n    def estimate_pose(self, rgb_image, camera_intrinsics):\n        """\n        Estimate 6D pose from RGB image.\n\n        Args:\n            rgb_image: (H, W, 3) RGB image\n            camera_intrinsics: 3x3 camera matrix\n\n        Returns:\n            {\n                "position": [x, y, z],\n                "rotation": [qw, qx, qy, qz],  # Quaternion\n                "confidence": float\n            }\n        """\n        # Run DOPE inference\n        detections = self.dope.infer(rgb_image)\n\n        if len(detections) == 0:\n            return None\n\n        # Take highest-confidence detection\n        best_det = max(detections, key=lambda d: d["score"])\n\n        # Extract 2D keypoints\n        keypoints_2d = best_det["projected_points"]\n\n        # PnP (Perspective-n-Point) to recover 3D pose\n        pose_3d = self.solve_pnp(keypoints_2d, camera_intrinsics)\n\n        return {\n            "position": pose_3d["translation"],\n            "rotation": pose_3d["quaternion"],\n            "confidence": best_det["score"]\n        }\n\n    def solve_pnp(self, keypoints_2d, camera_matrix):\n        """Solve PnP problem to get 6D pose"""\n        import cv2\n\n        # 3D model keypoints (object-centric coordinates)\n        object_points = np.array([\n            [0.0, 0.0, 0.0],      # Center\n            [0.1, 0.0, 0.0],      # +X\n            [0.0, 0.1, 0.0],      # +Y\n            [0.0, 0.0, 0.1],      # +Z\n            # ... (more keypoints)\n        ], dtype=np.float32)\n\n        image_points = np.array(keypoints_2d, dtype=np.float32)\n\n        # Solve PnP\n        success, rvec, tvec = cv2.solvePnP(\n            object_points,\n            image_points,\n            camera_matrix,\n            distCoeffs=None\n        )\n\n        if not success:\n            return None\n\n        # Convert rotation vector to quaternion\n        rotation_matrix, _ = cv2.Rodrigues(rvec)\n        quaternion = self.rotation_matrix_to_quaternion(rotation_matrix)\n\n        return {\n            "translation": tvec.flatten().tolist(),\n            "quaternion": quaternion\n        }\n\n    def rotation_matrix_to_quaternion(self, R):\n        """Convert 3x3 rotation matrix to quaternion"""\n        trace = np.trace(R)\n        if trace > 0:\n            s = 0.5 / np.sqrt(trace + 1.0)\n            w = 0.25 / s\n            x = (R[2, 1] - R[1, 2]) * s\n            y = (R[0, 2] - R[2, 0]) * s\n            z = (R[1, 0] - R[0, 1]) * s\n        else:\n            # Handle degenerate cases\n            # (simplified for brevity)\n            w, x, y, z = 1, 0, 0, 0\n\n        return [w, x, y, z]\n\n# Usage\nworld = World()\ncamera = world.scene.add_camera("/World/Camera")\npose_estimator = PoseEstimator()\n\nfor _ in range(100):\n    world.step()\n\n    rgb_image = camera.get_rgba()[:, :, :3]\n    camera_matrix = camera.get_intrinsics_matrix()\n\n    pose = pose_estimator.estimate_pose(rgb_image, camera_matrix)\n\n    if pose:\n        print(f"Object at position: {pose[\'position\']}")\n        print(f"Object orientation (quaternion): {pose[\'rotation\']}")\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"depth-perception",children:"Depth Perception"}),"\n",(0,o.jsx)(n.h3,{id:"processing-depth-images",children:"Processing Depth Images"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# depth_processing.py\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.extensions import enable_extension\nimport numpy as np\n\nenable_extension("omni.isaac.sensor")\nfrom omni.isaac.sensor import Camera\n\nclass DepthProcessor:\n    def __init__(self, camera):\n        self.camera = camera\n\n    def get_depth_map(self):\n        """Get depth map from camera (in meters)"""\n        depth_image = self.camera.get_depth()  # Shape: (H, W)\n        return depth_image\n\n    def find_nearest_obstacle(self, depth_map):\n        """Find closest obstacle in depth map"""\n        min_depth = np.min(depth_map[depth_map > 0])  # Ignore zero (invalid)\n        return min_depth\n\n    def segment_foreground_background(self, depth_map, threshold=2.0):\n        """Separate foreground (< threshold) from background"""\n        foreground_mask = depth_map < threshold\n        return foreground_mask\n\n    def point_cloud_from_depth(self, depth_map, camera_intrinsics):\n        """\n        Convert depth map to 3D point cloud.\n\n        Args:\n            depth_map: (H, W) depth in meters\n            camera_intrinsics: 3x3 camera matrix\n\n        Returns:\n            (N, 3) point cloud in camera frame\n        """\n        H, W = depth_map.shape\n        fx, fy = camera_intrinsics[0, 0], camera_intrinsics[1, 1]\n        cx, cy = camera_intrinsics[0, 2], camera_intrinsics[1, 2]\n\n        # Create pixel grid\n        u, v = np.meshgrid(np.arange(W), np.arange(H))\n\n        # Back-project to 3D\n        z = depth_map\n        x = (u - cx) * z / fx\n        y = (v - cy) * z / fy\n\n        # Stack into (H*W, 3) array\n        points = np.stack([x, y, z], axis=-1).reshape(-1, 3)\n\n        # Filter invalid points\n        valid_mask = points[:, 2] > 0\n        points = points[valid_mask]\n\n        return points\n\n# Usage\nworld = World()\ncamera = Camera(\n    prim_path="/World/Camera",\n    position=[2.0, 0, 1.0]\n)\n\nprocessor = DepthProcessor(camera)\n\nfor _ in range(100):\n    world.step()\n\n    depth_map = processor.get_depth_map()\n\n    # Find nearest obstacle\n    min_dist = processor.find_nearest_obstacle(depth_map)\n    print(f"Nearest obstacle: {min_dist:.2f}m")\n\n    # Generate point cloud\n    intrinsics = camera.get_intrinsics_matrix()\n    point_cloud = processor.point_cloud_from_depth(depth_map, intrinsics)\n    print(f"Point cloud: {point_cloud.shape[0]} points")\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"complete-perception-pipeline-pick-and-place",children:"Complete Perception Pipeline: Pick and Place"}),"\n",(0,o.jsx)(n.h3,{id:"integration-with-manipulation",children:"Integration with Manipulation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# pick_and_place_perception.py\nfrom omni.isaac.core import World\nfrom omni.isaac.franka import Franka\nfrom omni.isaac.core.objects import DynamicCuboid\nimport numpy as np\n\nclass PickAndPlaceController:\n    def __init__(self):\n        # Initialize world\n        self.world = World()\n        self.world.scene.add_default_ground_plane()\n\n        # Add robot\n        self.robot = self.world.scene.add(Franka(\n            prim_path="/World/Franka",\n            name="franka"\n        ))\n\n        # Add camera mounted on robot\n        from omni.isaac.sensor import Camera\n        self.camera = Camera(\n            prim_path="/World/Franka/panda_hand/camera",\n            position=[0, 0, 0.1],  # Relative to end-effector\n            orientation=[0.707, 0, 0.707, 0]  # Look downward\n        )\n\n        # Initialize perception\n        self.object_detector = ObjectDetector("detectnet.pth")\n        self.pose_estimator = PoseEstimator()\n\n    def detect_and_pick(self):\n        """Main perception-action loop"""\n        # 1. Move to observation pose\n        self.robot.move_to_observation_pose()\n        self.world.step()\n\n        # 2. Capture image\n        rgb_image = self.camera.get_rgba()[:, :, :3]\n\n        # 3. Detect objects\n        detections = self.object_detector.detect(rgb_image)\n\n        if len(detections) == 0:\n            print("No objects detected")\n            return False\n\n        # 4. Estimate pose of first detected object\n        target_object = detections[0]\n        pose_6d = self.pose_estimator.estimate_pose(\n            rgb_image,\n            self.camera.get_intrinsics_matrix()\n        )\n\n        if pose_6d is None:\n            print("Pose estimation failed")\n            return False\n\n        # 5. Plan grasp\n        grasp_pose = self.compute_grasp_pose(pose_6d)\n\n        # 6. Execute grasp\n        success = self.robot.grasp_at_pose(grasp_pose)\n\n        if success:\n            print(f"Successfully grasped {target_object[\'class\']}")\n\n            # 7. Place at target location\n            place_pose = [0.5, 0.3, 0.2]  # Target position\n            self.robot.place_at_pose(place_pose)\n            print("Object placed")\n\n        return success\n\n    def compute_grasp_pose(self, object_pose):\n        """Convert object 6D pose to grasp pose"""\n        # Simple top-down grasp\n        grasp_position = object_pose["position"]\n        grasp_position[2] += 0.1  # Approach from above\n\n        grasp_orientation = [1, 0, 0, 0]  # Vertical gripper\n\n        return {\n            "position": grasp_position,\n            "orientation": grasp_orientation\n        }\n\n    def run(self, num_cycles=10):\n        """Run pick-and-place for multiple objects"""\n        for i in range(num_cycles):\n            print(f"\\n=== Cycle {i+1}/{num_cycles} ===")\n            success = self.detect_and_pick()\n\n            if not success:\n                print("Cycle failed, retrying...")\n                continue\n\n# Run the pipeline\ncontroller = PickAndPlaceController()\ncontroller.run(num_cycles=5)\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"domain-randomization-for-sim-to-real-transfer",children:"Domain Randomization for Sim-to-Real Transfer"}),"\n",(0,o.jsx)(n.h3,{id:"why-randomization",children:"Why Randomization?"}),"\n",(0,o.jsxs)(n.p,{children:["Models trained on synthetic data often fail on real robots due to the ",(0,o.jsx)(n.strong,{children:"reality gap"})," (differences in textures, lighting, physics)."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Domain Randomization"})," bridges this gap by training on ",(0,o.jsx)(n.strong,{children:"diverse"})," synthetic data:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Random textures for objects and backgrounds"}),"\n",(0,o.jsx)(n.li,{children:"Random lighting (intensity, color, shadows)"}),"\n",(0,o.jsx)(n.li,{children:"Random camera parameters (position, exposure, noise)"}),"\n",(0,o.jsx)(n.li,{children:"Random object positions and orientations"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"implementing-domain-randomization",children:"Implementing Domain Randomization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# domain_randomization.py\nimport omni.replicator.core as rep\nimport random\n\ndef randomize_textures():\n    """Apply random materials to objects"""\n    objects = ["/World/Object_0", "/World/Object_1", "/World/Object_2"]\n\n    for obj_path in objects:\n        # Random PBR material\n        rep.randomizer.materials(\n            obj_path,\n            metallic=random.uniform(0, 1),\n            roughness=random.uniform(0.2, 1.0),\n            base_color=(\n                random.uniform(0, 1),\n                random.uniform(0, 1),\n                random.uniform(0, 1)\n            )\n        )\n\ndef randomize_lighting():\n    """Randomize scene lighting"""\n    intensity = random.uniform(500, 3000)\n    color_temp = random.randint(2000, 8000)  # Kelvin\n\n    rep.modify.attribute("/World/Light", "intensity", intensity)\n    rep.modify.attribute("/World/Light", "colorTemperature", color_temp)\n\ndef randomize_camera():\n    """Add realistic camera noise and distortion"""\n    camera_path = "/World/Camera"\n\n    # Motion blur\n    rep.modify.attribute(camera_path, "motion:motionBlurScale", random.uniform(0, 0.5))\n\n    # Exposure\n    exposure = random.uniform(0.5, 2.0)\n    rep.modify.attribute(camera_path, "exposure", exposure)\n\n# Register randomizers\nwith rep.trigger.on_frame():\n    randomize_textures()\n    randomize_lighting()\n    randomize_camera()\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Synthetic data generation"})," in Isaac Sim provides unlimited labeled training data with automatic ground-truth"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object detection models"})," (DetectNet, YOLO) enable robots to identify objects in cluttered scenes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"6D pose estimation"})," (DOPE, FoundationPose) is critical for precise manipulation tasks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Depth perception"})," enables obstacle avoidance and 3D scene understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Complete perception pipelines"})," integrate detection, pose estimation, and manipulation for autonomous pick-and-place"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Domain randomization"})," bridges the sim-to-real gap by training on diverse synthetic data"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"AI-powered perception transforms robots from blind actuators into intelligent agents that understand and interact with complex, unstructured environments."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,o.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-sdk-overview",children:"Isaac Platform Architecture"}),"\n",(0,o.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,o.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-3-isaac/reinforcement-learning",children:"Advanced RL and Sim-to-Real Transfer"})]}),"\n",(0,o.jsx)(n.p,{children:"In the next chapter, we'll explore advanced reinforcement learning techniques and strategies for transferring policies trained in simulation to physical robots."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);