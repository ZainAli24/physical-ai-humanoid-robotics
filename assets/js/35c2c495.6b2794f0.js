"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_01=globalThis.webpackChunkphysical_ai_humanoid_robotics_01||[]).push([[887],{8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var i=t(6540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},9166:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4-vla/multimodal-interaction","title":"Multi-modal Interaction (Speech, Gesture, Vision)","description":"Integrate speech, gesture recognition, and vision to create natural human-robot interaction systems","source":"@site/docs/module-4-vla/multimodal-interaction.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/multimodal-interaction","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/multimodal-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/multimodal-interaction.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Multi-modal Interaction (Speech, Gesture, Vision)","description":"Integrate speech, gesture recognition, and vision to create natural human-robot interaction systems"},"sidebar":"mainSidebar","previous":{"title":"Cognitive Planning with LLMs","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/cognitive-planning"}}');var s=t(4848),o=t(8453);const r={sidebar_position:6,title:"Multi-modal Interaction (Speech, Gesture, Vision)",description:"Integrate speech, gesture recognition, and vision to create natural human-robot interaction systems"},l="Multi-modal Interaction (Speech, Gesture, Vision)",a={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Multi-Modal Architecture",id:"multi-modal-architecture",level:2},{value:"Gesture Recognition with MediaPipe",id:"gesture-recognition-with-mediapipe",level:2},{value:"Setting Up MediaPipe Hands",id:"setting-up-mediapipe-hands",level:3},{value:"Hand Gesture Detector",id:"hand-gesture-detector",level:3},{value:"Vision Integration: Object Detection",id:"vision-integration-object-detection",level:2},{value:"YOLO-Based Object Detector",id:"yolo-based-object-detector",level:3},{value:"Multi-Modal Fusion Layer",id:"multi-modal-fusion-layer",level:2},{value:"Temporal Alignment and Fusion",id:"temporal-alignment-and-fusion",level:3},{value:"Intent Resolution with Context",id:"intent-resolution-with-context",level:2},{value:"Resolving Multi-Modal Commands",id:"resolving-multi-modal-commands",level:3},{value:"Complete Multi-Modal System",id:"complete-multi-modal-system",level:2},{value:"ROS 2 Integration Node",id:"ros-2-integration-node",level:3},{value:"Exercise 1: Pointing-Based Object Selection",id:"exercise-1-pointing-based-object-selection",level:2},{value:"Exercise 2: Gesture-Controlled Navigation",id:"exercise-2-gesture-controlled-navigation",level:2},{value:"Capstone Project: The Autonomous Humanoid",id:"capstone-project-the-autonomous-humanoid",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"Key Components Integration",id:"key-components-integration",level:3},{value:"Implementation Steps",id:"implementation-steps",level:3},{value:"Sample Scenarios",id:"sample-scenarios",level:3},{value:"Common Challenges and Solutions",id:"common-challenges-and-solutions",level:2},{value:"Advanced: Cross-Modal Attention",id:"advanced-cross-modal-attention",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Congratulations!",id:"congratulations",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"multi-modal-interaction-speech-gesture-vision",children:"Multi-modal Interaction (Speech, Gesture, Vision)"})}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting this chapter, you should have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Completed Voice-to-Action and Cognitive Planning chapters"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Understanding of computer vision basics (OpenCV, object detection)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Familiarity with sensor fusion concepts"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Experience with real-time systems and asynchronous processing"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 ROS 2 tf2 (transformations) knowledge"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Estimated Reading Time"}),": 25-30 minutes"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:["Human communication is inherently ",(0,s.jsx)(n.strong,{children:"multi-modal"}),'. When we ask someone to "pick up that red cup over there," we combine ',(0,s.jsx)(n.strong,{children:"speech"}),' ("pick up"), ',(0,s.jsx)(n.strong,{children:"gesture"})," (pointing), and ",(0,s.jsx)(n.strong,{children:"visual reference"}),' ("that red cup"). Single-modality interaction\u2014voice-only or gesture-only\u2014feels unnatural and error-prone.']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-modal interaction"})," systems fuse multiple sensory inputs to understand user intent with higher accuracy, robustness, and naturalness. A humanoid robot equipped with multi-modal perception can:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disambiguate commands"}),': "Grab this" (speech) + pointing gesture (vision) \u2192 identifies specific object']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Confirm understanding"}),': "Move there" + hand wave \u2192 visual confirmation before navigation']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Handle failures gracefully"}),": If speech recognition fails due to noise, gesture recognition provides backup"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enable richer interactions"}),": Combine voice commands with gaze tracking, facial expressions, and body language"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This chapter demonstrates how to build a ",(0,s.jsx)(n.strong,{children:"unified multi-modal interaction framework"})," that integrates:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech"})," (from Whisper + GPT-4 in Chapter 1)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gesture recognition"})," (hand tracking, pose estimation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision"})," (object detection, scene understanding)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor fusion"})," (combining modalities for robust decision-making)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Learning Objectives"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement gesture recognition using MediaPipe and OpenCV"}),"\n",(0,s.jsx)(n.li,{children:"Integrate vision models (YOLO, SAM) for object detection"}),"\n",(0,s.jsx)(n.li,{children:"Design sensor fusion architectures for multi-modal input"}),"\n",(0,s.jsx)(n.li,{children:"Build temporal alignment systems for speech-gesture synchronization"}),"\n",(0,s.jsx)(n.li,{children:"Create a capstone project: The Autonomous Humanoid"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"multi-modal-architecture",children:"Multi-Modal Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[User Input] --\x3e B[Speech Module: Whisper]\n    A --\x3e C[Gesture Module: MediaPipe]\n    A --\x3e D[Vision Module: YOLO/SAM]\n\n    B --\x3e E[Multi-Modal Fusion Layer]\n    C --\x3e E\n    D --\x3e E\n\n    E --\x3e F[Intent Resolver]\n    F --\x3e G[Action Planner: LLM]\n    G --\x3e H[Robot Executor]\n    H --\x3e I[Humanoid Robot]\n\n    I --\x3e J[Feedback: TTS + Visual Signals]\n    J --\x3e A\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Components"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Modules"}),": Parallel processing of speech, gestures, and vision"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fusion Layer"}),": Combines modalities with temporal alignment and confidence scoring"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intent Resolver"}),": Extracts unified command from fused data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Planner"}),": Uses LLM (from Chapter 2) to generate robot actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Executor"}),": Executes actions via ROS 2 (from previous modules)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"gesture-recognition-with-mediapipe",children:"Gesture Recognition with MediaPipe"}),"\n",(0,s.jsx)(n.h3,{id:"setting-up-mediapipe-hands",children:"Setting Up MediaPipe Hands"}),"\n",(0,s.jsx)(n.p,{children:"MediaPipe provides real-time hand tracking with 21 3D landmarks per hand."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install dependencies\npip install mediapipe opencv-python numpy\n\n# Install ROS 2 vision packages\nsudo apt install ros-humble-vision-opencv ros-humble-cv-bridge\n"})}),"\n",(0,s.jsx)(n.h3,{id:"hand-gesture-detector",children:"Hand Gesture Detector"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# gesture_detector.py\nimport cv2\nimport mediapipe as mp\nimport numpy as np\nfrom enum import Enum\n\nclass GestureType(Enum):\n    POINT = "point"\n    GRAB = "grab"\n    WAVE = "wave"\n    STOP = "stop"\n    THUMBS_UP = "thumbs_up"\n    UNKNOWN = "unknown"\n\nclass GestureDetector:\n    def __init__(self):\n        self.mp_hands = mp.solutions.hands\n        self.hands = self.mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            min_detection_confidence=0.7,\n            min_tracking_confidence=0.5\n        )\n        self.mp_draw = mp.solutions.drawing_utils\n\n    def detect_gesture(self, frame):\n        """\n        Detect hand gestures in video frame.\n\n        Returns:\n            {\n                "gesture": GestureType,\n                "hand": "left" | "right",\n                "position": (x, y, z),\n                "confidence": float\n            }\n        """\n        # Convert BGR to RGB\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        results = self.hands.process(rgb_frame)\n\n        if not results.multi_hand_landmarks:\n            return {"gesture": GestureType.UNKNOWN, "confidence": 0.0}\n\n        # Process first detected hand\n        hand_landmarks = results.multi_hand_landmarks[0]\n        hand_label = results.multi_handedness[0].classification[0].label  # "Left" or "Right"\n\n        # Extract landmark positions\n        landmarks = []\n        for lm in hand_landmarks.landmark:\n            landmarks.append((lm.x, lm.y, lm.z))\n\n        # Classify gesture\n        gesture = self._classify_gesture(landmarks)\n\n        # Get pointing direction (index finger tip)\n        pointing_pos = landmarks[8]  # Index finger tip\n\n        return {\n            "gesture": gesture,\n            "hand": hand_label.lower(),\n            "position": pointing_pos,\n            "confidence": results.multi_handedness[0].classification[0].score,\n            "landmarks": landmarks\n        }\n\n    def _classify_gesture(self, landmarks):\n        """Classify gesture from hand landmarks"""\n        # Extract finger tip and base positions\n        thumb_tip = np.array(landmarks[4])\n        index_tip = np.array(landmarks[8])\n        middle_tip = np.array(landmarks[12])\n        ring_tip = np.array(landmarks[16])\n        pinky_tip = np.array(landmarks[20])\n\n        # Finger base positions (MCP joints)\n        index_mcp = np.array(landmarks[5])\n        middle_mcp = np.array(landmarks[9])\n        ring_mcp = np.array(landmarks[13])\n        pinky_mcp = np.array(landmarks[17])\n\n        # Check which fingers are extended\n        def is_extended(tip, mcp):\n            return tip[1] < mcp[1]  # y-axis inverted in image\n\n        index_extended = is_extended(index_tip, index_mcp)\n        middle_extended = is_extended(middle_tip, middle_mcp)\n        ring_extended = is_extended(ring_tip, ring_mcp)\n        pinky_extended = is_extended(pinky_tip, pinky_mcp)\n\n        # Gesture classification rules\n        if index_extended and not middle_extended and not ring_extended:\n            return GestureType.POINT\n\n        elif not index_extended and not middle_extended and not ring_extended and not pinky_extended:\n            return GestureType.GRAB\n\n        elif index_extended and middle_extended and ring_extended and pinky_extended:\n            return GestureType.WAVE\n\n        elif all([not index_extended, not middle_extended, not ring_extended, not pinky_extended]):\n            # Closed fist\n            return GestureType.STOP\n\n        elif thumb_tip[1] < index_mcp[1] and not index_extended:\n            return GestureType.THUMBS_UP\n\n        return GestureType.UNKNOWN\n\n    def draw_landmarks(self, frame, landmarks):\n        """Visualize hand landmarks on frame"""\n        # Draw connections between landmarks\n        for hand_landmarks in landmarks:\n            self.mp_draw.draw_landmarks(\n                frame,\n                hand_landmarks,\n                self.mp_hands.HAND_CONNECTIONS\n            )\n        return frame\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"vision-integration-object-detection",children:"Vision Integration: Object Detection"}),"\n",(0,s.jsx)(n.h3,{id:"yolo-based-object-detector",children:"YOLO-Based Object Detector"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# object_detector.py\nfrom ultralytics import YOLO\nimport cv2\nimport numpy as np\n\nclass ObjectDetector:\n    def __init__(self, model_path="yolov8n.pt"):\n        """Initialize YOLO model"""\n        self.model = YOLO(model_path)\n        self.class_names = self.model.names\n\n    def detect_objects(self, frame, confidence_threshold=0.5):\n        """\n        Detect objects in frame.\n\n        Returns:\n            List of {\n                "class": str,\n                "confidence": float,\n                "bbox": (x1, y1, x2, y2),\n                "center": (x, y)\n            }\n        """\n        results = self.model(frame, conf=confidence_threshold)\n\n        detections = []\n        for result in results:\n            boxes = result.boxes\n            for box in boxes:\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                conf = box.conf[0].cpu().numpy()\n                cls = int(box.cls[0].cpu().numpy())\n\n                center_x = int((x1 + x2) / 2)\n                center_y = int((y1 + y2) / 2)\n\n                detections.append({\n                    "class": self.class_names[cls],\n                    "confidence": float(conf),\n                    "bbox": (int(x1), int(y1), int(x2), int(y2)),\n                    "center": (center_x, center_y)\n                })\n\n        return detections\n\n    def find_object_at_position(self, detections, point_x, point_y, tolerance=50):\n        """\n        Find object closest to pointed position.\n\n        Args:\n            detections: List of detected objects\n            point_x, point_y: Pointing coordinates (normalized 0-1)\n            tolerance: Distance tolerance in pixels\n\n        Returns:\n            Best matching object or None\n        """\n        best_match = None\n        min_distance = float(\'inf\')\n\n        for obj in detections:\n            obj_x, obj_y = obj["center"]\n            distance = np.sqrt((obj_x - point_x)**2 + (obj_y - point_y)**2)\n\n            if distance < min_distance and distance < tolerance:\n                min_distance = distance\n                best_match = obj\n\n        return best_match\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"multi-modal-fusion-layer",children:"Multi-Modal Fusion Layer"}),"\n",(0,s.jsx)(n.h3,{id:"temporal-alignment-and-fusion",children:"Temporal Alignment and Fusion"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# multimodal_fusion.py\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass MultiModalInput:\n    timestamp: float\n    speech_text: Optional[str] = None\n    gesture: Optional[dict] = None\n    vision_objects: Optional[list] = None\n    confidence: float = 0.0\n\nclass MultiModalFusion:\n    def __init__(self, temporal_window=2.0):\n        """\n        Fuse multi-modal inputs within temporal window.\n\n        Args:\n            temporal_window: Time window (seconds) for aligning inputs\n        """\n        self.temporal_window = temporal_window\n        self.speech_buffer = deque(maxlen=10)\n        self.gesture_buffer = deque(maxlen=30)  # Higher framerate\n        self.vision_buffer = deque(maxlen=30)\n\n    def add_speech(self, text: str):\n        """Add speech input"""\n        self.speech_buffer.append({\n            "timestamp": time.time(),\n            "text": text\n        })\n\n    def add_gesture(self, gesture_data: dict):\n        """Add gesture input"""\n        self.gesture_buffer.append({\n            "timestamp": time.time(),\n            **gesture_data\n        })\n\n    def add_vision(self, detections: list):\n        """Add vision input"""\n        self.vision_buffer.append({\n            "timestamp": time.time(),\n            "objects": detections\n        })\n\n    def fuse_inputs(self):\n        """\n        Fuse inputs within temporal window.\n\n        Returns:\n            MultiModalInput with fused data\n        """\n        current_time = time.time()\n\n        # Get recent inputs within window\n        recent_speech = self._get_recent(self.speech_buffer, current_time)\n        recent_gestures = self._get_recent(self.gesture_buffer, current_time)\n        recent_vision = self._get_recent(self.vision_buffer, current_time)\n\n        if not recent_speech and not recent_gestures:\n            return None\n\n        # Combine modalities\n        fused = MultiModalInput(timestamp=current_time)\n\n        # Speech component\n        if recent_speech:\n            fused.speech_text = recent_speech[-1]["text"]\n\n        # Gesture component\n        if recent_gestures:\n            # Get most confident gesture\n            best_gesture = max(recent_gestures, key=lambda g: g.get("confidence", 0))\n            fused.gesture = best_gesture\n\n        # Vision component\n        if recent_vision:\n            fused.vision_objects = recent_vision[-1]["objects"]\n\n        # Calculate overall confidence\n        fused.confidence = self._calculate_confidence(fused)\n\n        return fused\n\n    def _get_recent(self, buffer, current_time):\n        """Get entries within temporal window"""\n        return [\n            entry for entry in buffer\n            if current_time - entry["timestamp"] <= self.temporal_window\n        ]\n\n    def _calculate_confidence(self, fused: MultiModalInput):\n        """Calculate confidence score for fused input"""\n        scores = []\n\n        if fused.speech_text:\n            scores.append(0.8)  # Speech is highly reliable\n\n        if fused.gesture:\n            scores.append(fused.gesture.get("confidence", 0.5))\n\n        if fused.vision_objects:\n            avg_vision_conf = sum(obj["confidence"] for obj in fused.vision_objects) / len(fused.vision_objects)\n            scores.append(avg_vision_conf)\n\n        return sum(scores) / len(scores) if scores else 0.0\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"intent-resolution-with-context",children:"Intent Resolution with Context"}),"\n",(0,s.jsx)(n.h3,{id:"resolving-multi-modal-commands",children:"Resolving Multi-Modal Commands"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# intent_resolver.py\nfrom openai import OpenAI\nimport json\n\nclass IntentResolver:\n    def __init__(self, api_key: str):\n        self.client = OpenAI(api_key=api_key)\n\n    def resolve_intent(self, fused_input: MultiModalInput) -> dict:\n        """\n        Resolve user intent from multi-modal input.\n\n        Returns:\n            {\n                "action": str,\n                "target_object": str,\n                "location": (x, y, z),\n                "parameters": dict\n            }\n        """\n        # Build context from fused input\n        context = self._build_context(fused_input)\n\n        prompt = f"""You are a multi-modal robot command interpreter.\n\nUser provided these inputs simultaneously:\n{json.dumps(context, indent=2)}\n\nInterpret the user\'s intent and output a structured robot command.\n\nAvailable actions: navigate_to, grasp, place, point_at, wait, confirm\n\nOutput JSON format:\n{{\n  "action": "action_name",\n  "target_object": "object name or null",\n  "location": [x, y, z] or null,\n  "parameters": {{}}\n}}\n\nExample:\nInput: Speech: "pick up the cup", Gesture: pointing at (0.5, 0.3), Vision: [cup, plate]\nOutput: {{"action": "grasp", "target_object": "cup", "location": [0.5, 0.3, 0], "parameters": {{"hand": "right"}}}}\n"""\n\n        response = self.client.chat.completions.create(\n            model="gpt-4",\n            messages=[{"role": "user", "content": prompt}],\n            temperature=0.0,\n            response_format={"type": "json_object"}\n        )\n\n        result = json.loads(response.choices[0].message.content)\n        return result\n\n    def _build_context(self, fused_input: MultiModalInput) -> dict:\n        """Build context dictionary from fused input"""\n        context = {}\n\n        if fused_input.speech_text:\n            context["speech"] = fused_input.speech_text\n\n        if fused_input.gesture:\n            context["gesture"] = {\n                "type": fused_input.gesture["gesture"].value,\n                "position": fused_input.gesture["position"]\n            }\n\n        if fused_input.vision_objects:\n            context["visible_objects"] = [\n                obj["class"] for obj in fused_input.vision_objects\n            ]\n\n        return context\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"complete-multi-modal-system",children:"Complete Multi-Modal System"}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-integration-node",children:"ROS 2 Integration Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# multimodal_robot_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport os\n\nfrom gesture_detector import GestureDetector\nfrom object_detector import ObjectDetector\nfrom multimodal_fusion import MultiModalFusion\nfrom intent_resolver import IntentResolver\nfrom whisper_asr import WhisperASR  # From Chapter 1\nfrom robot_action_server import RobotActionServer  # From Chapter 1\n\nclass MultiModalRobotNode(Node):\n    def __init__(self):\n        super().__init__(\'multimodal_robot\')\n\n        # Initialize components\n        self.gesture_detector = GestureDetector()\n        self.object_detector = ObjectDetector()\n        self.fusion = MultiModalFusion(temporal_window=2.0)\n\n        api_key = os.getenv("OPENAI_API_KEY")\n        self.whisper = WhisperASR(api_key)\n        self.intent_resolver = IntentResolver(api_key)\n\n        # ROS 2 subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.bridge = CvBridge()\n\n        # Create timer for fusion and execution\n        self.create_timer(0.5, self.process_multimodal_input)\n\n        self.get_logger().info("Multi-modal robot node initialized")\n\n    def image_callback(self, msg):\n        """Process camera images for vision and gesture"""\n        frame = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n        # Gesture detection\n        gesture_result = self.gesture_detector.detect_gesture(frame)\n        if gesture_result["confidence"] > 0.7:\n            self.fusion.add_gesture(gesture_result)\n\n        # Object detection\n        detections = self.object_detector.detect_objects(frame)\n        if detections:\n            self.fusion.add_vision(detections)\n\n    def process_speech(self, audio_data: bytes):\n        """Process speech input (called externally)"""\n        text = self.whisper.transcribe(audio_data)\n        if text:\n            self.get_logger().info(f"Speech: {text}")\n            self.fusion.add_speech(text)\n\n    def process_multimodal_input(self):\n        """Fuse inputs and execute actions"""\n        fused = self.fusion.fuse_inputs()\n\n        if not fused or fused.confidence < 0.5:\n            return  # Not enough confident input\n\n        self.get_logger().info(f"Fused input (confidence: {fused.confidence:.2f})")\n\n        # Resolve intent\n        intent = self.intent_resolver.resolve_intent(fused)\n        self.get_logger().info(f"Resolved intent: {intent}")\n\n        # Execute action\n        self.execute_action(intent)\n\n    def execute_action(self, intent: dict):\n        """Execute robot action from resolved intent"""\n        action = intent.get("action")\n\n        if action == "grasp":\n            target = intent.get("target_object")\n            self.get_logger().info(f"Grasping {target}")\n            # Call robot action server\n\n        elif action == "navigate_to":\n            location = intent.get("location")\n            self.get_logger().info(f"Navigating to {location}")\n\n        # Add more action handlers...\n\ndef main():\n    rclpy.init()\n    node = MultiModalRobotNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"exercise-1-pointing-based-object-selection",children:"Exercise 1: Pointing-Based Object Selection"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),': Build a system where users point at objects and say "pick up this" to select targets.']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Detect pointing gesture with MediaPipe"}),"\n",(0,s.jsx)(n.li,{children:"Project pointing ray onto camera frame"}),"\n",(0,s.jsx)(n.li,{children:"Find nearest object to pointing direction"}),"\n",(0,s.jsx)(n.li,{children:"Confirm selection with visual feedback (bounding box highlight)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Starter Code"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def find_pointed_object(gesture_position, detections, frame_width, frame_height):\n    """\n    Find object user is pointing at.\n\n    Args:\n        gesture_position: (x, y, z) normalized coordinates\n        detections: List of detected objects\n        frame_width, frame_height: Camera resolution\n\n    Returns:\n        Selected object or None\n    """\n    # Convert normalized to pixel coordinates\n    point_x = int(gesture_position[0] * frame_width)\n    point_y = int(gesture_position[1] * frame_height)\n\n    # TODO: Find closest object to pointing position\n    # TODO: Add visual confirmation (draw bounding box)\n    pass\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Accurately selects objects within 100px of pointing direction"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Provides visual feedback (green bounding box)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Handles multiple objects in scene"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Works in real-time (under 100ms latency)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"exercise-2-gesture-controlled-navigation",children:"Exercise 2: Gesture-Controlled Navigation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Control robot navigation using hand gestures."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Gesture Mappings"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Point"}),": Navigate to pointed location"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Wave"}),": Stop current motion"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Thumbs up"}),": Confirm destination"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stop (fist)"}),": Emergency stop"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implementation Hints"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def gesture_navigation_control(gesture_type, position):\n    """Map gestures to navigation commands"""\n    if gesture_type == GestureType.POINT:\n        # Convert 2D point to 3D world coordinates\n        world_coords = camera_to_world(position)\n        return {"action": "navigate_to", "location": world_coords}\n\n    elif gesture_type == GestureType.WAVE:\n        return {"action": "stop"}\n\n    # TODO: Add more gesture mappings\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Project Goal"}),": Build a fully autonomous humanoid robot that integrates all four modules of this textbook."]}),"\n",(0,s.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[Human User] --\x3e B[Multi-Modal Interface]\n    B --\x3e C[Speech: Whisper]\n    B --\x3e D[Gesture: MediaPipe]\n    B --\x3e E[Vision: YOLO]\n\n    C --\x3e F[Multi-Modal Fusion]\n    D --\x3e F\n    E --\x3e F\n\n    F --\x3e G[Cognitive Planner: GPT-4]\n    G --\x3e H[ROS 2 Action Server]\n\n    H --\x3e I[NVIDIA Isaac Sim]\n    I --\x3e J[Simulated Humanoid]\n\n    H --\x3e K[Physical Robot: Gazebo/Unity]\n    K --\x3e L[Real Humanoid Hardware]\n\n    J --\x3e M[Feedback Loop]\n    L --\x3e M\n    M --\x3e A\n"})}),"\n",(0,s.jsx)(n.h3,{id:"key-components-integration",children:"Key Components Integration"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Module"}),(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"Integration Point"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Module 1: ROS 2"})}),(0,s.jsx)(n.td,{children:"Action servers, tf2 transforms"}),(0,s.jsx)(n.td,{children:"Robot control layer"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Module 2: Simulation"})}),(0,s.jsx)(n.td,{children:"Gazebo/Unity environments"}),(0,s.jsx)(n.td,{children:"Testing and validation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Module 3: NVIDIA Isaac"})}),(0,s.jsx)(n.td,{children:"Isaac Sim, Isaac Gym"}),(0,s.jsx)(n.td,{children:"RL training, physics simulation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Module 4: VLA"})}),(0,s.jsx)(n.td,{children:"Multi-modal interaction"}),(0,s.jsx)(n.td,{children:"User interface and cognitive planning"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Environment Setup"})," (Week 1)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure ROS 2 workspace with all dependencies"}),"\n",(0,s.jsx)(n.li,{children:"Set up Gazebo/Unity simulation environment"}),"\n",(0,s.jsx)(n.li,{children:"Install NVIDIA Isaac Sim and Isaac Gym"}),"\n",(0,s.jsx)(n.li,{children:"Configure camera, microphone, and sensors"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Module Integration"})," (Week 2)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate speech recognition (Whisper)"}),"\n",(0,s.jsx)(n.li,{children:"Add gesture detection (MediaPipe)"}),"\n",(0,s.jsx)(n.li,{children:"Implement object detection (YOLO)"}),"\n",(0,s.jsx)(n.li,{children:"Create multi-modal fusion layer"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cognitive Planning"})," (Week 3)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Connect LLM planner from Chapter 2"}),"\n",(0,s.jsx)(n.li,{children:"Implement task decomposition"}),"\n",(0,s.jsx)(n.li,{children:"Add failure handling and replanning"}),"\n",(0,s.jsx)(n.li,{children:"Create self-critique loops"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Robot Execution"})," (Week 4)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement navigation using Nav2"}),"\n",(0,s.jsx)(n.li,{children:"Add manipulation with MoveIt 2"}),"\n",(0,s.jsx)(n.li,{children:"Integrate with Isaac Sim for training"}),"\n",(0,s.jsx)(n.li,{children:"Deploy to physical hardware (if available)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Testing and Refinement"})," (Week 5)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Test in simulation with various scenarios"}),"\n",(0,s.jsx)(n.li,{children:"Collect user feedback"}),"\n",(0,s.jsx)(n.li,{children:"Optimize latency and accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Create demonstration videos"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"sample-scenarios",children:"Sample Scenarios"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scenario 1: Household Assistant"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["User: ",(0,s.jsx)(n.em,{children:'"Clean up the living room"'})," (speech)"]}),"\n",(0,s.jsx)(n.li,{children:"Robot: Uses cognitive planner to decompose task"}),"\n",(0,s.jsx)(n.li,{children:"Robot: Navigates, detects objects (vision), picks up items (manipulation)"}),"\n",(0,s.jsxs)(n.li,{children:["User: Points at specific object + ",(0,s.jsx)(n.em,{children:'"put this in the drawer"'})," (gesture + speech)"]}),"\n",(0,s.jsx)(n.li,{children:"Robot: Confirms with visual feedback, executes command"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scenario 2: Collaborative Cooking"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["User: Gestures at ingredients + ",(0,s.jsx)(n.em,{children:'"pass me the tomatoes"'})]}),"\n",(0,s.jsx)(n.li,{children:"Robot: Detects tomatoes (vision), grasps, hands to user"}),"\n",(0,s.jsxs)(n.li,{children:["User: ",(0,s.jsx)(n.em,{children:'"chop these vegetables"'})," (speech)"]}),"\n",(0,s.jsx)(n.li,{children:"Robot: Explains it cannot use knives (safety), suggests alternative"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scenario 3: Industrial Inspection"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["User: Points at machinery + ",(0,s.jsx)(n.em,{children:'"inspect this component"'})]}),"\n",(0,s.jsx)(n.li,{children:"Robot: Navigates to location, captures images"}),"\n",(0,s.jsx)(n.li,{children:"Robot: Uses vision model to detect defects"}),"\n",(0,s.jsx)(n.li,{children:"Robot: Reports findings via speech synthesis"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"common-challenges-and-solutions",children:"Common Challenges and Solutions"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Challenge"}),(0,s.jsx)(n.th,{children:"Cause"}),(0,s.jsx)(n.th,{children:"Solution"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Modality desynchronization"})}),(0,s.jsx)(n.td,{children:"Different sensor framerates"}),(0,s.jsx)(n.td,{children:"Use temporal alignment with buffering"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Conflicting inputs"})}),(0,s.jsx)(n.td,{children:'Speech says "left", gesture points right'}),(0,s.jsx)(n.td,{children:"Implement confidence-weighted fusion"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Latency issues"})}),(0,s.jsx)(n.td,{children:"Processing multiple AI models"}),(0,s.jsx)(n.td,{children:"Parallelize inference, use edge TPU/GPU"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Gesture ambiguity"})}),(0,s.jsx)(n.td,{children:"Similar hand poses for different gestures"}),(0,s.jsx)(n.td,{children:"Add temporal context, require confirmation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Lighting variations"})}),(0,s.jsx)(n.td,{children:"Vision and gesture fail in poor light"}),(0,s.jsx)(n.td,{children:"Add infrared camera, use robust models"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"advanced-cross-modal-attention",children:"Advanced: Cross-Modal Attention"}),"\n",(0,s.jsxs)(n.p,{children:["For state-of-the-art systems, implement ",(0,s.jsx)(n.strong,{children:"cross-modal attention"})," to learn which modality to prioritize:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class CrossModalAttention:\n    def __init__(self):\n        # Learnable attention weights\n        self.speech_weight = 0.5\n        self.gesture_weight = 0.3\n        self.vision_weight = 0.2\n\n    def attend(self, speech_features, gesture_features, vision_features):\n        """\n        Compute attention-weighted fusion.\n\n        Returns:\n            Weighted combination of features\n        """\n        attended = (\n            self.speech_weight * speech_features +\n            self.gesture_weight * gesture_features +\n            self.vision_weight * vision_features\n        )\n        return attended\n\n    def update_weights(self, feedback: float):\n        """Update attention weights based on task success"""\n        # TODO: Implement reinforcement learning update\n        pass\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-modal systems"})," are more robust than single-modality interfaces"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temporal alignment"})," is critical for fusing asynchronous sensor inputs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gesture recognition"})," enables intuitive spatial commands (pointing, waving)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision integration"})," grounds language in the physical world (object reference)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor fusion"})," combines modalities for higher accuracy and fault tolerance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM-based intent resolution"})," handles ambiguity in natural human communication"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Multi-modal interaction transforms humanoid robots from tools into collaborative partners. By understanding speech, gestures, and visual context, robots can participate in natural human workflows."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"congratulations",children:"Congratulations!"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"You've completed the Physical AI & Humanoid Robotics textbook!"})," \ud83c\udf89"]}),"\n",(0,s.jsx)(n.p,{children:"You've learned:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Module 1"}),": ROS 2 fundamentals (nodes, topics, actions, lifecycle management)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Module 2"}),": Simulation environments (Gazebo, Unity, physics engines)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Module 3"}),": NVIDIA Isaac platform (Isaac Sim, Isaac Gym, reinforcement learning)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Module 4"}),": Vision-Language-Action models (speech recognition, cognitive planning, multi-modal interaction)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next Steps"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Build your capstone project: The Autonomous Humanoid"}),"\n",(0,s.jsx)(n.li,{children:"Contribute to open-source robotics projects (ROS 2, MoveIt, Nav2)"}),"\n",(0,s.jsx)(n.li,{children:"Explore research papers on VLA models (RT-1, RT-2, PaLM-E, Octo)"}),"\n",(0,s.jsx)(n.li,{children:"Join robotics communities (ROS Discourse, Humanoids subreddit, AI alignment forums)"}),"\n",(0,s.jsx)(n.li,{children:"Apply for robotics internships or research positions"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Resources for Further Learning"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"ROS 2 Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/",children:"NVIDIA Isaac Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://openai.com/research/robotics",children:"OpenAI Robotics Research"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/",children:"Google DeepMind Robotics"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,s.jsx)(n.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/cognitive-planning",children:"Cognitive Planning with LLMs"}),"\n",(0,s.jsx)(n.strong,{children:"Return to"}),": ",(0,s.jsx)(n.a,{href:"/docs/preface",children:"Preface"})," | ",(0,s.jsx)(n.a,{href:"/docs/intro-physical-ai",children:"Introduction to Physical AI"})]}),"\n",(0,s.jsx)(n.p,{children:"Thank you for learning with us. Go build amazing robots! \ud83e\udd16"})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);