"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_01=globalThis.webpackChunkphysical_ai_humanoid_robotics_01||[]).push([[963],{6568:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Use large language models for high-level task planning, reasoning, and decision-making in autonomous robotics","source":"@site/docs/module-4-vla/cognitive-planning.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/cognitive-planning","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/cognitive-planning.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Cognitive Planning with LLMs","description":"Use large language models for high-level task planning, reasoning, and decision-making in autonomous robotics"},"sidebar":"mainSidebar","previous":{"title":"Voice-to-Action with OpenAI Whisper","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/voice-to-action"},"next":{"title":"Multi-modal Interaction (Speech, Gesture, Vision)","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/multimodal-interaction"}}');var s=t(4848),a=t(8453);const o={sidebar_position:5,title:"Cognitive Planning with LLMs",description:"Use large language models for high-level task planning, reasoning, and decision-making in autonomous robotics"},r="Cognitive Planning with LLMs",l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Cognitive Planning Architecture",id:"cognitive-planning-architecture",level:2},{value:"Prompt Engineering for Robot Planning",id:"prompt-engineering-for-robot-planning",level:2},{value:"Baseline Prompt (Naive Approach)",id:"baseline-prompt-naive-approach",level:3},{value:"Improved Prompt with Robot Constraints",id:"improved-prompt-with-robot-constraints",level:3},{value:"Implementing the Planning System",id:"implementing-the-planning-system",level:2},{value:"1. LLM Planner Class",id:"1-llm-planner-class",level:3},{value:"2. Chain-of-Thought (CoT) Reasoning",id:"2-chain-of-thought-cot-reasoning",level:3},{value:"Execution and Monitoring",id:"execution-and-monitoring",level:2},{value:"Plan Executor with Failure Handling",id:"plan-executor-with-failure-handling",level:3},{value:"Replanning Strategy",id:"replanning-strategy",level:2},{value:"Exercise 1: &quot;Tidy the Room&quot; Task",id:"exercise-1-tidy-the-room-task",level:2},{value:"Exercise 2: Self-Critique Loop",id:"exercise-2-self-critique-loop",level:2},{value:"Common Planning Failures",id:"common-planning-failures",level:2},{value:"Advanced: Hierarchical Task Networks (HTN)",id:"advanced-hierarchical-task-networks-htn",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"})}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(e.p,{children:"Before starting this chapter, you should have:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"\u2705 Completed Voice-to-Action chapter"}),"\n",(0,s.jsx)(e.li,{children:"\u2705 Understanding of LLM capabilities (GPT-4, Claude, Llama)"}),"\n",(0,s.jsx)(e.li,{children:"\u2705 Experience with prompt engineering"}),"\n",(0,s.jsx)(e.li,{children:"\u2705 Knowledge of task planning concepts (helpful but not required)"}),"\n",(0,s.jsx)(e.li,{children:"\u2705 Python async programming basics"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Estimated Reading Time"}),": 20-25 minutes"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(e.p,{children:['Traditional robot programming requires developers to manually script every step: "if sensor reads X, do Y; else do Z." This approach breaks down when robots encounter novel situations. ',(0,s.jsx)(e.strong,{children:"Cognitive planning"})," with large language models (LLMs) changes this paradigm: instead of rigid rules, robots use LLMs to ",(0,s.jsx)(e.em,{children:"reason"})," about tasks, adapt to unexpected conditions, and generate action sequences on the fly."]}),"\n",(0,s.jsxs)(e.p,{children:["Consider this instruction: ",(0,s.jsx)(e.em,{children:'"Clean the living room."'})," A traditional robot needs explicit steps: navigate to living room, identify objects, categorize trash vs. belongings, pick up items, etc. An LLM-powered robot can ",(0,s.jsx)(e.strong,{children:"decompose"})," this high-level goal into sub-tasks, ",(0,s.jsx)(e.strong,{children:"reason"}),' about object relationships ("don\'t vacuum before picking up toys"), and ',(0,s.jsx)(e.strong,{children:"adapt"}),' when plans fail ("cup is too heavy, request human help").']}),"\n",(0,s.jsx)(e.p,{children:"This chapter demonstrates how to integrate GPT-4 as a cognitive planning layer for humanoid robots. You'll learn to prompt LLMs for task decomposition, implement chain-of-thought reasoning, handle failures, and create self-improving planning loops."}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Why LLMs for Planning?"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Zero-shot generalization"}),": Handle tasks never seen during training"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Common-sense reasoning"}),': Understand implicit constraints ("don\'t place hot pan on wooden table")']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Natural language interface"}),": Accept vague instructions and ask clarifying questions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-step decomposition"}),": Break complex goals into achievable primitives"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Learning Objectives"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Integrate LLMs (GPT-4, Claude) for robot task planning"}),"\n",(0,s.jsx)(e.li,{children:"Implement chain-of-thought (CoT) reasoning"}),"\n",(0,s.jsx)(e.li,{children:"Design prompt engineering strategies for robotics"}),"\n",(0,s.jsx)(e.li,{children:"Handle planning failures and replanning"}),"\n",(0,s.jsx)(e.li,{children:"Build self-critique and verification loops"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"cognitive-planning-architecture",children:"Cognitive Planning Architecture"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-mermaid",children:"graph TD\n    A[User Instruction: Clean room] --\x3e B[LLM Task Decomposer]\n    B --\x3e C[Sub-tasks List]\n    C --\x3e D{Execute Task 1}\n    D --\x3e|Success| E{More Tasks?}\n    D --\x3e|Failure| F[LLM Replanner]\n    F --\x3e D\n    E --\x3e|Yes| D\n    E --\x3e|No| G[Report Completion]\n\n    B --\x3e H[Knowledge Base]\n    H --\x3e |Room layout, Object locations| B\n    F --\x3e H\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Key Components"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Task Decomposer"}),": LLM breaks high-level goals into atomic actions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Executor"}),": ROS 2 action servers execute primitives"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Monitor"}),": Tracks success/failure of each step"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Replanner"}),": Generates alternative plans when steps fail"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Knowledge Base"}),": World state, object locations, constraints"]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"prompt-engineering-for-robot-planning",children:"Prompt Engineering for Robot Planning"}),"\n",(0,s.jsx)(e.h3,{id:"baseline-prompt-naive-approach",children:"Baseline Prompt (Naive Approach)"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'user_instruction = "Clean the kitchen table"\n\nprompt = f"Break down this task into steps: {user_instruction}"\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Problem"}),": Vague, no robot context, no constraints."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Output"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"1. Remove items from table\n2. Wipe surface\n3. Put items back\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Issue"}),': Robot doesn\'t have "wipe" primitive. Steps assume human capabilities.']}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h3,{id:"improved-prompt-with-robot-constraints",children:"Improved Prompt with Robot Constraints"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'SYSTEM_PROMPT = """You are a task planner for a humanoid robot with these capabilities:\n\nAvailable Actions:\n- navigate_to(location: str) - Move to a named location\n- grasp(object: str, hand: str) - Pick up object with left/right hand\n- place(object: str, location: str) - Put held object at location\n- open(object: str) - Open door/drawer\n- close(object: str) - Close door/drawer\n- detect_objects(area: str) - Scan area and list objects\n- request_help(reason: str) - Ask human for assistance\n\nPhysical Constraints:\n- Can carry max 2 objects (one per hand)\n- Cannot lift objects >5kg\n- Must navigate before grasping objects\n- Cannot grasp if hands full\n\nWorld Knowledge:\n- Kitchen has: table, sink, trash_bin, counter\n- Living room has: sofa, tv, bookshelf\n\nTask: {user_task}\n\nOutput:\n1. List of sequential actions in JSON format\n2. Preconditions for each action\n3. Expected outcome after completion\n\nExample output:\n{\n  "plan": [\n    {"action": "navigate_to", "params": {"location": "kitchen"}, "precondition": "none"},\n    {"action": "detect_objects", "params": {"area": "table"}, "precondition": "at_kitchen"}\n  ],\n  "expected_outcome": "Kitchen table is clear"\n}\n"""\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"implementing-the-planning-system",children:"Implementing the Planning System"}),"\n",(0,s.jsx)(e.h3,{id:"1-llm-planner-class",children:"1. LLM Planner Class"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# llm_planner.py\nfrom openai import OpenAI\nimport json\nfrom typing import List, Dict\n\nclass CognitivePlanner:\n    def __init__(self, api_key: str, model: str = "gpt-4"):\n        self.client = OpenAI(api_key=api_key)\n        self.model = model\n        self.action_primitives = [\n            "navigate_to", "grasp", "place", "open", "close",\n            "detect_objects", "request_help"\n        ]\n\n    def create_plan(self, user_task: str, world_state: Dict) -> List[Dict]:\n        """\n        Generate action plan from high-level task.\n\n        Args:\n            user_task: Natural language instruction\n            world_state: Current robot knowledge (locations, objects)\n\n        Returns:\n            List of action dictionaries\n        """\n        system_prompt = self._build_system_prompt(world_state)\n\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": f"Task: {user_task}"}\n            ],\n            temperature=0.0,\n            response_format={"type": "json_object"}\n        )\n\n        result = json.loads(response.choices[0].message.content)\n        return result.get("plan", [])\n\n    def _build_system_prompt(self, world_state: Dict) -> str:\n        """Construct prompt with current world knowledge"""\n        return f"""You are a robot task planner.\n\nAvailable actions: {\', \'.join(self.action_primitives)}\n\nCurrent world state:\n- Robot location: {world_state.get(\'robot_location\', \'unknown\')}\n- Known objects: {world_state.get(\'objects\', [])}\n- Hand status: {world_state.get(\'hands\', \'both empty\')}\n\nPlan the task step-by-step using ONLY available actions.\nOutput valid JSON with this format:\n{{\n  "plan": [\n    {{"action": "action_name", "params": {{}}, "rationale": "why this step"}},\n    ...\n  ]\n}}\n"""\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h3,{id:"2-chain-of-thought-cot-reasoning",children:"2. Chain-of-Thought (CoT) Reasoning"}),"\n",(0,s.jsx)(e.p,{children:'Chain-of-Thought prompting makes LLMs "think out loud," improving plan quality.'}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'COT_PROMPT = """Let\'s think step by step:\n\nTask: {task}\n\nStep 1: Analyze the goal\n- What is the end state we want?\n- What are the preconditions?\n\nStep 2: Identify sub-goals\n- What intermediate states are needed?\n- Are there dependencies between steps?\n\nStep 3: Check constraints\n- Physical limitations (weight, reach, hands)?\n- Environmental obstacles?\n\nStep 4: Generate action sequence\nBased on the above reasoning, create the plan.\n\nOutput your reasoning, then the final JSON plan.\n"""\n\ndef plan_with_cot(self, task: str, world_state: Dict):\n    """Generate plan with explicit reasoning"""\n    prompt = COT_PROMPT.format(task=task)\n\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=[\n            {"role": "system", "content": self._build_system_prompt(world_state)},\n            {"role": "user", "content": prompt}\n        ],\n        temperature=0.2\n    )\n\n    # Extract JSON from response (may include reasoning text)\n    content = response.choices[0].message.content\n    # Parse JSON from content...\n    return parsed_plan\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Example Output"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:'Reasoning:\n- Goal: Clean table means remove clutter\n- Preconditions: Robot must be at table, hands empty\n- Sub-goals: Navigate, detect objects, move each to trash/storage\n- Constraint: Can only carry 2 items at once\n\nFinal Plan:\n{\n  "plan": [\n    {"action": "navigate_to", "params": {"location": "kitchen_table"}},\n    {"action": "detect_objects", "params": {"area": "table"}},\n    {"action": "grasp", "params": {"object": "plate", "hand": "left"}},\n    ...\n  ]\n}\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"execution-and-monitoring",children:"Execution and Monitoring"}),"\n",(0,s.jsx)(e.h3,{id:"plan-executor-with-failure-handling",children:"Plan Executor with Failure Handling"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# plan_executor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom enum import Enum\n\nclass ExecutionStatus(Enum):\n    SUCCESS = "success"\n    FAILURE = "failure"\n    BLOCKED = "blocked"\n\nclass PlanExecutor(Node):\n    def __init__(self, planner: CognitivePlanner):\n        super().__init__(\'plan_executor\')\n        self.planner = planner\n        self.action_handlers = {\n            "navigate_to": self.execute_navigate,\n            "grasp": self.execute_grasp,\n            "place": self.execute_place,\n            # ... register all action handlers\n        }\n\n    def execute_plan(self, plan: List[Dict], max_retries: int = 3):\n        """\n        Execute action plan with monitoring and replanning.\n        """\n        for step_idx, action in enumerate(plan):\n            self.get_logger().info(f"Step {step_idx + 1}/{len(plan)}: {action}")\n\n            # Execute action\n            status = self._execute_action(action)\n\n            if status == ExecutionStatus.SUCCESS:\n                continue\n\n            elif status == ExecutionStatus.FAILURE:\n                # Attempt replanning\n                self.get_logger().warn(f"Action failed: {action[\'action\']}")\n\n                if max_retries > 0:\n                    # Ask LLM for alternative\n                    new_plan = self.planner.replan(\n                        failed_action=action,\n                        remaining_plan=plan[step_idx + 1:],\n                        failure_reason="action_failed"\n                    )\n\n                    return self.execute_plan(new_plan, max_retries - 1)\n                else:\n                    self.get_logger().error("Max retries exceeded")\n                    return False\n\n            elif status == ExecutionStatus.BLOCKED:\n                # Environment changed, full replan needed\n                self.get_logger().warn("Environment blocked, replanning...")\n                # Get updated world state and replan entire task\n                pass\n\n        self.get_logger().info("\u2705 Plan completed successfully")\n        return True\n\n    def _execute_action(self, action: Dict) -> ExecutionStatus:\n        """Execute single action via registered handler"""\n        action_name = action["action"]\n        params = action.get("params", {})\n\n        if action_name not in self.action_handlers:\n            self.get_logger().error(f"Unknown action: {action_name}")\n            return ExecutionStatus.FAILURE\n\n        handler = self.action_handlers[action_name]\n\n        try:\n            result = handler(**params)\n            return ExecutionStatus.SUCCESS if result else ExecutionStatus.FAILURE\n        except Exception as e:\n            self.get_logger().error(f"Execution error: {e}")\n            return ExecutionStatus.FAILURE\n\n    def execute_grasp(self, object: str, hand: str) -> bool:\n        """Example action handler for grasping"""\n        # TODO: Call MoveIt or Isaac manipulation\n        self.get_logger().info(f"Grasping {object} with {hand} hand")\n        # Simulate action\n        import time\n        time.sleep(1)\n        return True  # or False if grasp failed\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"replanning-strategy",children:"Replanning Strategy"}),"\n",(0,s.jsx)(e.p,{children:"When actions fail, the LLM generates alternative approaches:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'def replan(self, failed_action: Dict, remaining_plan: List[Dict],\n           failure_reason: str) -> List[Dict]:\n    """\n    Generate alternative plan after failure.\n\n    Args:\n        failed_action: The action that failed\n        remaining_plan: Actions that weren\'t executed yet\n        failure_reason: Why the action failed\n\n    Returns:\n        New action plan\n    """\n    replan_prompt = f"""The robot attempted this action but it failed:\nFailed action: {json.dumps(failed_action)}\nFailure reason: {failure_reason}\n\nOriginal remaining plan:\n{json.dumps(remaining_plan, indent=2)}\n\nGenerate an ALTERNATIVE approach to complete the task.\nConsider:\n- Why did the action fail?\n- What\'s a different way to achieve the same goal?\n- Should we request human help?\n\nOutput new plan in JSON format.\n"""\n\n    response = self.client.chat.completions.create(\n        model=self.model,\n        messages=[\n            {"role": "system", "content": self._build_system_prompt({})},\n            {"role": "user", "content": replan_prompt}\n        ],\n        temperature=0.3  # Slightly higher for creative alternatives\n    )\n\n    result = json.loads(response.choices[0].message.content)\n    return result.get("plan", [])\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"exercise-1-tidy-the-room-task",children:'Exercise 1: "Tidy the Room" Task'}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Objective"}),": Build a complete cognitive planner that tidies a simulated room."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Scenario"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Room contains: book (on floor), cup (on table), toy (on sofa)"}),"\n",(0,s.jsx)(e.li,{children:"Goal: Put books on bookshelf, cups in sink, toys in toy_box"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"LLM generates multi-step plan"}),"\n",(0,s.jsx)(e.li,{children:"Robot executes using Gazebo simulation"}),"\n",(0,s.jsx)(e.li,{children:"Handle at least one failure (object too heavy, location blocked)"}),"\n",(0,s.jsx)(e.li,{children:"Replan and complete task"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Starter Code"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'def tidy_room_task():\n    # Define world state\n    world_state = {\n        "robot_location": "living_room",\n        "objects": [\n            {"name": "book1", "location": "floor", "weight": 0.5},\n            {"name": "cup1", "location": "table", "weight": 0.2},\n            {"name": "toy1", "location": "sofa", "weight": 0.3}\n        ],\n        "hands": {"left": "empty", "right": "empty"}\n    }\n\n    # TODO: Initialize planner\n    # TODO: Generate plan\n    # TODO: Execute with failure handling\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","All objects placed in correct locations"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","At least one simulated failure recovered via replanning"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Execution completes within 10 action steps"]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"exercise-2-self-critique-loop",children:"Exercise 2: Self-Critique Loop"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Objective"}),': Add a "critic" LLM that reviews plans before execution.']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Approach"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'def critique_plan(plan: List[Dict]) -> Dict:\n    """\n    Review plan for logical errors, unsafe actions, or inefficiencies.\n\n    Returns:\n        {"approved": bool, "issues": [str], "suggestions": [str]}\n    """\n    critic_prompt = f"""Review this robot action plan:\n{json.dumps(plan, indent=2)}\n\nCheck for:\n1. Unsafe actions (e.g., moving with hands full near obstacles)\n2. Logical errors (e.g., grasping before navigating)\n3. Inefficiencies (e.g., unnecessary back-and-forth movements)\n\nOutput JSON: {{"approved": true/false, "issues": [], "suggestions": []}}\n"""\n    # Call LLM and return critique\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Integration"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Generate plan"}),"\n",(0,s.jsx)(e.li,{children:"Critique plan"}),"\n",(0,s.jsx)(e.li,{children:"If not approved, regenerate with critique feedback"}),"\n",(0,s.jsx)(e.li,{children:"Execute approved plan"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"common-planning-failures",children:"Common Planning Failures"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Failure Type"}),(0,s.jsx)(e.th,{children:"Example"}),(0,s.jsx)(e.th,{children:"Mitigation"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Hallucination"})}),(0,s.jsx)(e.td,{children:'LLM invents non-existent action "teleport_to"'}),(0,s.jsx)(e.td,{children:"Strict output validation, function calling"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Impossible sequence"})}),(0,s.jsx)(e.td,{children:'"grasp cup" before "navigate_to table"'}),(0,s.jsx)(e.td,{children:"Add precondition checker"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Infinite loops"})}),(0,s.jsx)(e.td,{children:"Replanning generates same failed plan"}),(0,s.jsx)(e.td,{children:"Track attempted solutions, force diversity"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Overly complex"})}),(0,s.jsx)(e.td,{children:"50-step plan for simple task"}),(0,s.jsx)(e.td,{children:'Add "simplicity" to prompt criteria'})]})]})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"advanced-hierarchical-task-networks-htn",children:"Advanced: Hierarchical Task Networks (HTN)"}),"\n",(0,s.jsx)(e.p,{children:"For complex tasks, use hierarchical decomposition:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'HIERARCHICAL_PROMPT = """Decompose task into hierarchy:\n\nLevel 1 (Abstract): High-level goals\nLevel 2 (Intermediate): Sub-tasks\nLevel 3 (Primitive): Robot actions\n\nExample:\nTask: "Prepare breakfast"\n\nLevel 1: [Get ingredients, Cook food, Set table]\nLevel 2: [Open fridge, Grasp eggs, Close fridge, ...]\nLevel 3: [navigate_to(fridge), open(fridge), ...]\n\nYour task: {task}\nOutput all 3 levels.\n"""\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"LLMs enable zero-shot task planning"})," without manual programming for each scenario"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Chain-of-thought prompting"})," improves plan quality by forcing explicit reasoning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Replanning loops"})," make robots resilient to environmental changes and action failures"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Prompt engineering"})," is critical\u2014specify robot capabilities, constraints, and output format"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Self-critique"})," (using a second LLM call) catches logical errors before execution"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Cognitive planning transforms robots from rigid automata into adaptive systems that reason about goals, constraints, and failures\u2014much like human problem-solving."}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Previous Chapter"}),": ",(0,s.jsx)(e.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/voice-to-action",children:"Voice-to-Action with OpenAI Whisper"}),"\n",(0,s.jsx)(e.strong,{children:"Next Chapter"}),": ",(0,s.jsx)(e.a,{href:"/physical-ai-humanoid-robotics/docs/module-4-vla/multimodal-interaction",children:"Multi-modal Interaction (Speech, Gesture, Vision)"})]}),"\n",(0,s.jsx)(e.p,{children:"In the final chapter, we'll integrate voice, vision, and gesture recognition to create truly natural human-robot interaction systems."})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var i=t(6540);const s={},a=i.createContext(s);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);