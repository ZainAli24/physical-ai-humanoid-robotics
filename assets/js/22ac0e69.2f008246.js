"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_01=globalThis.webpackChunkphysical_ai_humanoid_robotics_01||[]).push([[377],{934:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-3-isaac/isaac-gym-rl","title":"Reinforcement Learning with Isaac Gym","description":"Learn to train robot policies using GPU-accelerated parallel RL environments in Isaac Gym","source":"@site/docs/module-3-isaac/isaac-gym-rl.md","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/isaac-gym-rl","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-gym-rl","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/isaac-gym-rl.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Reinforcement Learning with Isaac Gym","description":"Learn to train robot policies using GPU-accelerated parallel RL environments in Isaac Gym"},"sidebar":"mainSidebar","previous":{"title":"Isaac Sim for Robotics","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-sim"},"next":{"title":"Isaac Platform Architecture","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/isaac-sdk-overview"}}');var r=i(4848),a=i(8453);const o={sidebar_position:3,title:"Reinforcement Learning with Isaac Gym",description:"Learn to train robot policies using GPU-accelerated parallel RL environments in Isaac Gym"},t="Reinforcement Learning with Isaac Gym",l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Isaac Gym Overview",id:"isaac-gym-overview",level:2},{value:"GPU-Parallel RL Architecture",id:"gpu-parallel-rl-architecture",level:3},{value:"Key Features",id:"key-features",level:3},{value:"RL Training Workflow",id:"rl-training-workflow",level:2},{value:"Defining an RL Task",id:"defining-an-rl-task",level:3},{value:"Example Task: Franka Arm Reaching",id:"example-task-franka-arm-reaching",level:3},{value:"Isaac Gym RL Code Example",id:"isaac-gym-rl-code-example",level:2},{value:"Step 1: Define the Task Class",id:"step-1-define-the-task-class",level:3},{value:"Step 2: Configure RL Training",id:"step-2-configure-rl-training",level:3},{value:"Step 3: Train the Policy",id:"step-3-train-the-policy",level:3},{value:"Reward Shaping Best Practices",id:"reward-shaping-best-practices",level:2},{value:"Principles of Good Reward Design",id:"principles-of-good-reward-design",level:3},{value:"Example: Improved Franka Reach Reward",id:"example-improved-franka-reach-reward",level:3},{value:"Sim-to-Real Transfer with Domain Randomization",id:"sim-to-real-transfer-with-domain-randomization",level:2},{value:"Randomization Strategies",id:"randomization-strategies",level:3},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: Set Up Isaac Gym Environment",id:"exercise-1-set-up-isaac-gym-environment",level:3},{value:"Exercise 2: Define a Custom RL Task",id:"exercise-2-define-a-custom-rl-task",level:3},{value:"Exercise 3: Train a Reaching Policy",id:"exercise-3-train-a-reaching-policy",level:3},{value:"Exercise 4: Implement Reward Shaping",id:"exercise-4-implement-reward-shaping",level:3},{value:"Exercise 5: Apply Domain Randomization",id:"exercise-5-apply-domain-randomization",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Navigation",id:"navigation",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"reinforcement-learning-with-isaac-gym",children:"Reinforcement Learning with Isaac Gym"})}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before starting this chapter, you should have:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Completed Chapter 1: Introduction to NVIDIA Isaac"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Completed Chapter 2: Isaac Sim for Robotics"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 NVIDIA RTX GPU (8GB+ VRAM recommended for RL training)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Isaac Sim 2023.1.0 (or newer) installed and verified"}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"PyTorch installed"})," (1.13.0+ with CUDA support)"]}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Basic understanding of reinforcement learning (policies, rewards, value functions)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Familiarity with Python and NumPy for tensor operations"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Understanding of robot control concepts (joint positions, velocities, torques)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Reading Time"}),": 20-25 minutes"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["Traditional reinforcement learning (RL) for robotics faces a critical bottleneck: simulation speed. Training a robotic manipulation policy from scratch can require millions of environment interactions. On CPU-based simulators, collecting this data might take weeks or months\u2014making research impractical and hyperparameter tuning infeasible. Isaac Gym solves this problem with ",(0,r.jsx)(n.strong,{children:"massively parallel GPU simulation"}),", running thousands of robot environments simultaneously on a single GPU."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why RL for Robotics?"})," Reinforcement learning enables robots to learn complex behaviors through trial and error, discovering strategies that are difficult or impossible to hand-program. From dexterous manipulation (in-hand object rotation, tool use) to dynamic locomotion (quadruped gaits, bipedal walking), RL has achieved state-of-the-art results. However, the key challenge is sample efficiency\u2014RL algorithms require vast amounts of experience to learn."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Isaac Gym's Breakthrough"}),": By parallelizing physics simulation on the GPU, Isaac Gym accelerates RL training by ",(0,r.jsx)(n.strong,{children:"10-100x"})," compared to CPU-based approaches. What previously took weeks can now complete in hours. This speed enables:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rapid prototyping"}),": Test reward functions and hyperparameters in minutes instead of days"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Complex tasks"}),": Train policies for high-DoF robots (humanoids, multi-fingered hands)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Curriculum learning"}),": Progressively increase task difficulty as the policy improves"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robust policies"}),": Train with domain randomization (varying masses, friction, sensor noise) for sim-to-real transfer"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Chapter Goals"}),": By the end of this chapter, you will understand Isaac Gym's architecture for GPU-parallelized RL, define custom RL tasks with observations/actions/rewards, train a robot manipulation policy using PPO (Proximal Policy Optimization), evaluate policy performance, and apply domain randomization for sim-to-real transfer. You'll gain hands-on experience with the tools that power cutting-edge robot learning research."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Learning Objectives"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Understand GPU-parallel RL and Isaac Gym's tensor-based API"}),"\n",(0,r.jsx)(n.li,{children:"Define RL tasks: observation space, action space, reward functions"}),"\n",(0,r.jsx)(n.li,{children:"Train policies using RL Games (PPO algorithm)"}),"\n",(0,r.jsx)(n.li,{children:"Implement reward shaping for effective learning"}),"\n",(0,r.jsx)(n.li,{children:"Apply domain randomization to improve sim-to-real transfer"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate and visualize trained policies"}),"\n",(0,r.jsx)(n.li,{children:"Deploy policies from simulation to real robots"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"isaac-gym-overview",children:"Isaac Gym Overview"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Isaac Gym"})," is not a separate simulator\u2014it's a mode of Isaac Sim optimized for massively parallel reinforcement learning."]}),"\n",(0,r.jsx)(n.h3,{id:"gpu-parallel-rl-architecture",children:"GPU-Parallel RL Architecture"}),"\n",(0,r.jsx)(n.p,{children:"Traditional RL training loop (CPU-based):"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Collect experience from ",(0,r.jsx)(n.strong,{children:"1 environment"})," (slow)"]}),"\n",(0,r.jsx)(n.li,{children:"Update policy using collected data"}),"\n",(0,r.jsx)(n.li,{children:"Repeat millions of times"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Isaac Gym RL training loop (GPU-based):"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Collect experience from ",(0,r.jsx)(n.strong,{children:"thousands of environments in parallel"})," (fast)"]}),"\n",(0,r.jsx)(n.li,{children:"Update policy using batched GPU data (no CPU-GPU data transfer)"}),"\n",(0,r.jsx)(n.li,{children:"Converge 10-100x faster"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"sequenceDiagram\n    participant Env as Parallel Environments<br/>(4096 robots on GPU)\n    participant Phys as PhysX 5<br/>GPU Physics\n    participant Obs as Observations<br/>(PyTorch Tensors)\n    participant Policy as Policy Network<br/>(Neural Network)\n    participant Reward as Reward Function<br/>(GPU Kernels)\n    participant Algo as RL Algorithm<br/>(PPO/SAC)\n\n    loop Every Simulation Step\n        Env->>Phys: Step physics (all envs)\n        Phys->>Obs: Compute observations (batched)\n        Obs->>Policy: Forward pass (batch)\n        Policy->>Env: Apply actions (all robots)\n        Env->>Reward: Compute rewards (batched)\n        Reward->>Algo: Accumulate experience\n    end\n\n    Algo->>Policy: Update policy (gradient descent)\n    Policy->>Env: Deploy updated policy\n\n    Note over Env,Algo: All operations on GPU<br/>No CPU bottleneck\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Figure 1: Isaac Gym RL training loop. All operations (physics, observations, policy inference, rewards) execute on the GPU, eliminating CPU-GPU data transfer overhead."})}),"\n",(0,r.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Tensor-Based API"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"All data (observations, actions, rewards) are PyTorch tensors on GPU"}),"\n",(0,r.jsx)(n.li,{children:"Direct integration with PyTorch-based RL libraries (RL Games, Stable Baselines3)"}),"\n",(0,r.jsx)(n.li,{children:"No CPU-GPU memory copies (10-100x speedup)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Vectorized Environments"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Thousands of identical robots training simultaneously"}),"\n",(0,r.jsx)(n.li,{children:"Each environment has independent state (joint positions, object poses)"}),"\n",(0,r.jsx)(n.li,{children:"Shared policy learns from diverse experiences in parallel"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Built-In Domain Randomization"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Randomize physics parameters (mass, friction, damping) per environment"}),"\n",(0,r.jsx)(n.li,{children:"Randomize visual properties (lighting, textures, camera position)"}),"\n",(0,r.jsx)(n.li,{children:"Improves generalization to real-world deployment"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Efficient Memory Layout"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Environments stored in contiguous GPU memory (cache-efficient)"}),"\n",(0,r.jsx)(n.li,{children:"Batch operations leverage GPU tensor cores (fast matrix multiplication)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"rl-training-workflow",children:"RL Training Workflow"}),"\n",(0,r.jsx)(n.h3,{id:"defining-an-rl-task",children:"Defining an RL Task"}),"\n",(0,r.jsx)(n.p,{children:"An RL task in Isaac Gym requires defining:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Observation Space"}),': What the policy "sees" (joint positions, object poses, goal, etc.)']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action Space"}),": What the policy controls (joint velocities, torques, end-effector poses)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reward Function"}),": What the policy optimizes for (reach target, minimize energy, avoid collisions)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reset Conditions"}),": When to reset an environment (task success, timeout, failure)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-task-franka-arm-reaching",children:"Example Task: Franka Arm Reaching"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Control a 7-DoF Franka Panda arm to reach a target position in 3D space."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Observation"})," (17-dimensional vector):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Franka joint positions (7 values)"}),"\n",(0,r.jsx)(n.li,{children:"Franka joint velocities (7 values)"}),"\n",(0,r.jsx)(n.li,{children:"Target position relative to end-effector (3 values: x, y, z)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Action"})," (7-dimensional vector):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Joint velocity commands for 7 Franka joints (continuous values in [-1, 1])"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Reward"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"-distance_to_target"})," (negative distance encourages reaching)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"-0.01 * action_penalty"})," (small penalty for large actions, encourages smooth motion)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"+10"})," bonus if end-effector within 5cm of target (task success)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Reset"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Reset if end-effector reaches target (success)"}),"\n",(0,r.jsx)(n.li,{children:"Reset if 500 steps elapsed (timeout)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"isaac-gym-rl-code-example",children:"Isaac Gym RL Code Example"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-define-the-task-class",children:"Step 1: Define the Task Class"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from omni.isaac.gym.vec_env import VecEnvBase\nimport torch\nimport numpy as np\n\nclass FrankaReachTask(VecEnvBase):\n    def __init__(self, cfg, sim_device, graphics_device_id, headless):\n        # Configuration\n        self.num_envs = cfg["num_envs"]  # e.g., 4096 parallel environments\n        self.num_obs = 17  # Observation dimension\n        self.num_actions = 7  # Action dimension (7 Franka joints)\n\n        # Initialize base class\n        super().__init__(cfg, sim_device, graphics_device_id, headless)\n\n        # Define observation and action spaces\n        self.obs_buf = torch.zeros((self.num_envs, self.num_obs), device=self.device)\n        self.rew_buf = torch.zeros(self.num_envs, device=self.device)\n        self.reset_buf = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)\n\n        # Target positions for each environment (randomized)\n        self.targets = torch.zeros((self.num_envs, 3), device=self.device)\n\n    def create_sim(self):\n        """Create the simulation with Franka robots and targets."""\n        # Add Franka robot for each environment\n        for i in range(self.num_envs):\n            # Create environment at offset position\n            env_pos = torch.tensor([i % 64, i // 64, 0.0]) * 2.0  # Grid layout\n\n            # Add Franka arm\n            self.add_franka(env_pos)\n\n            # Add target sphere (visual marker)\n            self.add_target_sphere(env_pos + torch.tensor([0.5, 0.0, 0.3]))\n\n    def compute_observations(self):\n        """Compute observations for all environments."""\n        # Get Franka joint positions and velocities (batched)\n        joint_pos = self.get_joint_positions()  # Shape: (num_envs, 7)\n        joint_vel = self.get_joint_velocities()  # Shape: (num_envs, 7)\n\n        # Get end-effector position\n        ee_pos = self.get_end_effector_position()  # Shape: (num_envs, 3)\n\n        # Compute relative target position\n        target_rel = self.targets - ee_pos  # Shape: (num_envs, 3)\n\n        # Concatenate into observation vector\n        self.obs_buf = torch.cat([joint_pos, joint_vel, target_rel], dim=-1)\n\n        return self.obs_buf\n\n    def compute_reward(self):\n        """Compute rewards for all environments."""\n        # Get end-effector position\n        ee_pos = self.get_end_effector_position()\n\n        # Distance to target\n        distance = torch.norm(self.targets - ee_pos, dim=-1)\n\n        # Reward: negative distance\n        self.rew_buf = -distance\n\n        # Action penalty (encourage smooth motion)\n        action_penalty = torch.norm(self.actions, dim=-1)\n        self.rew_buf -= 0.01 * action_penalty\n\n        # Success bonus (if within 5cm of target)\n        success_mask = distance < 0.05\n        self.rew_buf[success_mask] += 10.0\n\n        # Mark successful environments for reset\n        self.reset_buf[success_mask] = 1\n\n        return self.rew_buf\n\n    def reset_idx(self, env_ids):\n        """Reset specified environments."""\n        # Randomize Franka joint positions\n        self.set_joint_positions(env_ids, torch.rand((len(env_ids), 7)) * 0.5)\n\n        # Randomize target positions\n        self.targets[env_ids] = torch.rand((len(env_ids), 3)) * 0.6 + torch.tensor([0.3, -0.3, 0.2])\n\n        # Reset buffers\n        self.reset_buf[env_ids] = 0\n\n    def step(self, actions):\n        """Apply actions and step simulation."""\n        self.actions = actions.clone()\n\n        # Apply joint velocity commands\n        self.set_joint_velocities(actions)\n\n        # Step physics simulation\n        self.simulate()\n        self.render()\n\n        # Compute observations and rewards\n        obs = self.compute_observations()\n        rewards = self.compute_reward()\n\n        # Check for timeouts\n        self.progress_buf += 1\n        timeout_mask = self.progress_buf >= 500  # 500 steps = timeout\n        self.reset_buf[timeout_mask] = 1\n\n        # Reset environments that need it\n        reset_env_ids = self.reset_buf.nonzero(as_tuple=False).squeeze(-1)\n        if len(reset_env_ids) > 0:\n            self.reset_idx(reset_env_ids)\n\n        return obs, rewards, self.reset_buf, {}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-configure-rl-training",children:"Step 2: Configure RL Training"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsxs)(n.strong,{children:["Configuration File (",(0,r.jsx)(n.code,{children:"franka_reach.yaml"}),")"]}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# Environment settings\nnum_envs: 4096  # Number of parallel environments\nenv_spacing: 2.0  # Spacing between environments (meters)\n\n# RL algorithm (PPO)\nppo:\n  num_iterations: 1000  # Training iterations\n  horizon_length: 16  # Steps per environment before policy update\n  minibatch_size: 32768  # Samples per gradient update\n  learning_rate: 0.001\n  gamma: 0.99  # Discount factor\n  lam: 0.95  # GAE lambda\n  clip_param: 0.2  # PPO clipping\n  entropy_coef: 0.0\n  value_loss_coef: 2.0\n  max_grad_norm: 1.0\n\n# Network architecture\nnetwork:\n  name: "ActorCritic"\n  separate: False  # Shared trunk for actor and critic\n  mlp:\n    units: [256, 128, 64]  # Hidden layer sizes\n    activation: "elu"\n    initializer: "default"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-train-the-policy",children:"Step 3: Train the Policy"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from rl_games.torch_runner import Runner\nfrom rl_games.algos_torch import torch_ext\n\n# Load task configuration\ntask_cfg = load_yaml("franka_reach.yaml")\n\n# Create vectorized environment\nenv = FrankaReachTask(task_cfg, sim_device="cuda:0", graphics_device_id=0, headless=False)\n\n# Configure RL Games runner\nrunner = Runner()\nrunner.load(task_cfg)\nrunner.reset()\n\n# Train policy\nrunner.run({\n    "train": True,\n    "play": False,\n    "num_actors": task_cfg["num_envs"],\n    "max_iterations": task_cfg["ppo"]["num_iterations"]\n})\n\n# Save trained policy\nrunner.save("franka_reach_policy.pth")\nprint("Training complete! Policy saved.")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Training Time"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX 4090"}),": ~15-30 minutes for 1000 iterations (4096 envs)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX 3080"}),": ~30-60 minutes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTX 2080 Ti"}),": ~60-90 minutes"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Training Metrics"})," (printed during training):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Iteration: 100/1000\nMean Reward: -0.45 (improving from -1.2)\nSuccess Rate: 12% (improving from 0%)\nFPS: 85000 (steps per second across all envs)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"reward-shaping-best-practices",children:"Reward Shaping Best Practices"}),"\n",(0,r.jsx)(n.p,{children:"Designing effective reward functions is critical for RL success. Poor rewards lead to slow learning or undesirable behaviors."}),"\n",(0,r.jsx)(n.h3,{id:"principles-of-good-reward-design",children:"Principles of Good Reward Design"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Dense Rewards > Sparse Rewards"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sparse"}),": ",(0,r.jsx)(n.code,{children:"+1"})," if task succeeds, ",(0,r.jsx)(n.code,{children:"0"})," otherwise (slow learning)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Dense"}),": ",(0,r.jsx)(n.code,{children:"-distance_to_goal"})," (provides gradient, faster learning)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Scale Rewards Appropriately"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Keep rewards in range ",(0,r.jsx)(n.code,{children:"[-10, +10]"})," for numerical stability"]}),"\n",(0,r.jsx)(n.li,{children:"Success bonuses should be significant but not dominating"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Avoid Unintended Behaviors"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Bad"}),': Reward only "reach target" \u2192 policy might reach in dangerous way']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Good"}),': Reward "reach target" + penalize collisions + penalize large actions']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"4. Use Curriculum Learning"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Start with easier tasks (target close to robot)"}),"\n",(0,r.jsx)(n.li,{children:"Gradually increase difficulty as success rate improves"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-improved-franka-reach-reward",children:"Example: Improved Franka Reach Reward"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def compute_reward_improved(self):\n    """Improved reward with shaping and safety constraints."""\n    ee_pos = self.get_end_effector_position()\n    distance = torch.norm(self.targets - ee_pos, dim=-1)\n\n    # Dense reward: negative distance (range: [-1, 0])\n    reward = -distance\n\n    # Success bonus (scaled)\n    success_mask = distance < 0.05\n    reward[success_mask] += 5.0\n\n    # Action penalty (encourage smooth motion)\n    action_magnitude = torch.norm(self.actions, dim=-1)\n    reward -= 0.01 * action_magnitude\n\n    # Collision penalty (if robot hits itself or environment)\n    collision_mask = self.check_collisions()\n    reward[collision_mask] -= 2.0\n    self.reset_buf[collision_mask] = 1  # Reset on collision\n\n    # Joint limit penalty (discourage near-limit configurations)\n    joint_pos = self.get_joint_positions()\n    near_limit_penalty = torch.sum(torch.abs(joint_pos) > 0.9, dim=-1) * 0.1\n    reward -= near_limit_penalty\n\n    self.rew_buf = reward\n    return self.rew_buf\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"sim-to-real-transfer-with-domain-randomization",children:"Sim-to-Real Transfer with Domain Randomization"}),"\n",(0,r.jsxs)(n.p,{children:["Policies trained purely in simulation often fail when deployed to real robots due to the ",(0,r.jsx)(n.strong,{children:"sim-to-real gap"}),": differences in physics, sensor noise, and environmental conditions."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Domain Randomization"})," mitigates this gap by training on diverse simulation parameters, forcing the policy to be robust to variations."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"flowchart LR\n    A[Simulation Training<br/>with Randomization] --\x3e B[Train Policy<br/>1000+ Iterations]\n    B --\x3e C[Evaluate in Sim<br/>with Randomization]\n    C --\x3e D{Success Rate<br/>&gt;90%?}\n\n    D --\x3e|No| E[Adjust Rewards<br/>or Network]\n    E --\x3e B\n\n    D --\x3e|Yes| F[Domain Randomization<br/>Testing]\n    F --\x3e G[Deploy to<br/>Real Robot]\n\n    G --\x3e H{Real-World<br/>Performance?}\n    H --\x3e|Success| I[Production Ready]\n    H --\x3e|Partial Success| J[Fine-Tune in Sim<br/>with Real Data]\n    H --\x3e|Failure| K[Increase Randomization<br/>or Collect Real Data]\n\n    J --\x3e B\n    K --\x3e B\n\n    style A fill:#4CAF50,stroke:#2E7D32,color:#fff\n    style I fill:#76B900,stroke:#5A8E00,color:#fff\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Figure 2: Sim-to-real transfer workflow with domain randomization. Policies are trained on randomized simulation parameters, then deployed to real robots after achieving high success rates."})}),"\n",(0,r.jsx)(n.h3,{id:"randomization-strategies",children:"Randomization Strategies"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Physics Randomization"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def randomize_physics(self, env_ids):\n    """Randomize physics parameters for specified environments."""\n    # Randomize robot link masses (\xb120%)\n    base_mass = 2.0\n    mass_scale = torch.rand(len(env_ids)) * 0.4 + 0.8  # [0.8, 1.2]\n    self.set_link_mass(env_ids, base_mass * mass_scale)\n\n    # Randomize joint friction (0.01 to 0.1)\n    friction = torch.rand(len(env_ids)) * 0.09 + 0.01\n    self.set_joint_friction(env_ids, friction)\n\n    # Randomize surface friction (table, floor)\n    surface_friction = torch.rand(len(env_ids)) * 0.5 + 0.5  # [0.5, 1.0]\n    self.set_surface_friction(env_ids, surface_friction)\n\n    # Randomize joint damping\n    damping = torch.rand(len(env_ids)) * 0.5 + 0.1  # [0.1, 0.6]\n    self.set_joint_damping(env_ids, damping)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Observation Noise"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def add_observation_noise(self, obs):\n    """Add realistic sensor noise to observations."""\n    # Joint position noise (encoders have ~0.001 rad error)\n    joint_pos_noise = torch.randn_like(obs[:, :7]) * 0.001\n    obs[:, :7] += joint_pos_noise\n\n    # Joint velocity noise (higher than position)\n    joint_vel_noise = torch.randn_like(obs[:, 7:14]) * 0.01\n    obs[:, 7:14] += joint_vel_noise\n\n    # Vision-based target position noise (camera has ~1cm error)\n    target_noise = torch.randn_like(obs[:, 14:17]) * 0.01\n    obs[:, 14:17] += target_noise\n\n    return obs\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"3. Visual Randomization"})," (for vision-based policies)"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def randomize_visuals(self, env_ids):\n    """Randomize visual properties (lighting, textures, colors)."""\n    # Randomize lighting intensity\n    light_intensity = torch.rand(len(env_ids)) * 2000 + 1000  # [1000, 3000]\n    self.set_light_intensity(env_ids, light_intensity)\n\n    # Randomize object colors\n    colors = torch.rand(len(env_ids), 3)  # RGB [0, 1]\n    self.set_object_color(env_ids, colors)\n\n    # Randomize camera position (small perturbations)\n    cam_offset = (torch.rand(len(env_ids), 3) - 0.5) * 0.1  # \xb15cm\n    self.set_camera_offset(env_ids, cam_offset)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-set-up-isaac-gym-environment",children:"Exercise 1: Set Up Isaac Gym Environment"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Verify Isaac Gym is ready for RL training."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Instructions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Install PyTorch with CUDA support:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Install RL Games:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip3 install rl-games\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Run a sample Isaac Gym example (if available in your installation):","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ~/.local/share/ov/pkg/isaac_sim-2023.1.0/\n./python.sh standalone_examples/api/omni.isaac.gym/cartpole.py\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Outcome"}),": Cartpole example runs with thousands of parallel environments training in real-time."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-define-a-custom-rl-task",children:"Exercise 2: Define a Custom RL Task"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Create a simple RL task from scratch."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Instructions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:'Define a task: "Balance a pole on a cart" (classic Cartpole problem)'}),"\n",(0,r.jsx)(n.li,{children:"Observation: cart position, cart velocity, pole angle, pole angular velocity (4D)"}),"\n",(0,r.jsx)(n.li,{children:"Action: force applied to cart (1D, continuous)"}),"\n",(0,r.jsxs)(n.li,{children:["Reward: ",(0,r.jsx)(n.code,{children:"+1"})," per timestep if pole angle < 12\xb0, ",(0,r.jsx)(n.code,{children:"-10"})," if pole falls"]}),"\n",(0,r.jsx)(n.li,{children:"Implement the task class following the Franka Reach example structure"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Outcome"}),": A functional RL task that can be trained with PPO."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-train-a-reaching-policy",children:"Exercise 3: Train a Reaching Policy"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Train a robot arm to reach a target using PPO."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Instructions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Use the Franka Reach code example from this chapter"}),"\n",(0,r.jsx)(n.li,{children:"Configure training for 500 iterations with 2048 environments (adjust based on GPU VRAM)"}),"\n",(0,r.jsx)(n.li,{children:"Monitor training metrics (mean reward, success rate, FPS)"}),"\n",(0,r.jsx)(n.li,{children:"Save the trained policy checkpoint"}),"\n",(0,r.jsxs)(n.li,{children:["Visualize the trained policy by setting ",(0,r.jsx)(n.code,{children:"headless=False"})," and running inference"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Outcome"}),": After 500 iterations, success rate >80%, mean reward > -0.2."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"exercise-4-implement-reward-shaping",children:"Exercise 4: Implement Reward Shaping"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Improve learning speed with dense rewards."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Instructions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Start with a sparse reward: ",(0,r.jsx)(n.code,{children:"+10"})," if within 5cm of target, ",(0,r.jsx)(n.code,{children:"0"})," otherwise"]}),"\n",(0,r.jsx)(n.li,{children:"Train for 200 iterations and record success rate"}),"\n",(0,r.jsxs)(n.li,{children:["Replace with dense reward: ",(0,r.jsx)(n.code,{children:"-distance + success_bonus"})]}),"\n",(0,r.jsx)(n.li,{children:"Train for 200 iterations and compare success rate"}),"\n",(0,r.jsx)(n.li,{children:"Analyze which reward converges faster"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Outcome"}),": Dense reward achieves >50% success rate in 200 iterations, sparse reward <10%."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"exercise-5-apply-domain-randomization",children:"Exercise 5: Apply Domain Randomization"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Make policies robust to real-world variations."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Instructions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Train a policy without randomization (fixed physics parameters)"}),"\n",(0,r.jsx)(n.li,{children:"Test on randomized physics (vary mass \xb130%, friction \xb150%)"}),"\n",(0,r.jsx)(n.li,{children:"Record success rate drop (e.g., 90% \u2192 40%)"}),"\n",(0,r.jsx)(n.li,{children:"Retrain with domain randomization enabled"}),"\n",(0,r.jsx)(n.li,{children:"Test on randomized physics again and compare"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Outcome"}),": Policy trained with randomization maintains >70% success rate under variations."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsx)(n.p,{children:"After completing this chapter, you should understand:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"GPU-Parallel RL"}),": Isaac Gym runs thousands of robot environments in parallel on a single GPU, accelerating RL training by 10-100x compared to CPU-based simulators."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tensor-Based API"}),": All data (observations, actions, rewards) are PyTorch tensors on GPU, eliminating CPU-GPU data transfer overhead."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RL Task Definition"}),": Tasks require defining observation space, action space, reward function, and reset conditions. Design choices significantly impact learning speed and final policy quality."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Reward Shaping"}),": Dense rewards (e.g., ",(0,r.jsx)(n.code,{children:"-distance"}),") converge faster than sparse rewards (",(0,r.jsx)(n.code,{children:"+1"})," on success). Penalties for collisions and large actions improve safety and smoothness."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Domain Randomization"}),": Varying physics parameters (mass, friction, damping) and sensor noise during training improves sim-to-real transfer by making policies robust to real-world variations."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PPO Algorithm"}),": Proximal Policy Optimization (PPO) is a stable, sample-efficient RL algorithm well-suited for continuous control tasks in robotics."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Deployment"}),": Trained policies can be exported as neural network checkpoints and deployed to real robots, though fine-tuning with real-world data often improves performance."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"navigation",children:"Navigation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,r.jsx)(n.a,{href:"/docs/module-3-isaac/isaac-sim",children:"Isaac Sim for Robotics"}),"\n",(0,r.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,r.jsx)(n.a,{href:"/docs/module-3-isaac/isaac-sdk-overview",children:"Isaac Platform Architecture"})]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var s=i(6540);const r={},a=s.createContext(r);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);