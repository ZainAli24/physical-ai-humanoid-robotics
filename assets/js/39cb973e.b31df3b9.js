"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_01=globalThis.webpackChunkphysical_ai_humanoid_robotics_01||[]).push([[4],{7431:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3-isaac/reinforcement-learning","title":"Advanced RL and Sim-to-Real Transfer","description":"Master advanced reinforcement learning techniques, multi-agent systems, and sim-to-real transfer strategies for deploying trained policies on physical robots","source":"@site/docs/module-3-isaac/reinforcement-learning.md","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/reinforcement-learning","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/reinforcement-learning.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Advanced RL and Sim-to-Real Transfer","description":"Master advanced reinforcement learning techniques, multi-agent systems, and sim-to-real transfer strategies for deploying trained policies on physical robots"},"sidebar":"mainSidebar","previous":{"title":"AI-Powered Perception","permalink":"/physical-ai-humanoid-robotics/docs/module-3-isaac/ai-powered-perception"},"next":{"title":"Introduction to VLA Models","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/intro-vla"}}');var s=i(4848),o=i(8453);const a={sidebar_position:6,title:"Advanced RL and Sim-to-Real Transfer",description:"Master advanced reinforcement learning techniques, multi-agent systems, and sim-to-real transfer strategies for deploying trained policies on physical robots"},r="Advanced RL and Sim-to-Real Transfer",l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Advanced RL Algorithms",id:"advanced-rl-algorithms",level:2},{value:"Hindsight Experience Replay (HER)",id:"hindsight-experience-replay-her",level:3},{value:"Soft Actor-Critic (SAC) for Continuous Control",id:"soft-actor-critic-sac-for-continuous-control",level:3},{value:"Curriculum Learning",id:"curriculum-learning",level:2},{value:"Automatic Curriculum Design",id:"automatic-curriculum-design",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"System Identification",id:"system-identification",level:3},{value:"Deployment on Physical Robots",id:"deployment-on-physical-robots",level:2},{value:"Safety-Critical RL",id:"safety-critical-rl",level:3},{value:"Multi-Agent RL",id:"multi-agent-rl",level:2},{value:"Collaborative Manipulation",id:"collaborative-manipulation",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"advanced-rl-and-sim-to-real-transfer",children:"Advanced RL and Sim-to-Real Transfer"})}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(e.p,{children:"Before starting this chapter, you should have:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"\u2705 Completed all previous Module 3 chapters (especially Isaac Gym RL)"}),"\n",(0,s.jsx)(e.li,{children:"\u2705 Strong understanding of RL algorithms (PPO, SAC, TD3)"}),"\n",(0,s.jsx)(e.li,{children:"\u2705 Experience training policies in Isaac Gym"}),"\n",(0,s.jsx)(e.li,{children:"\u2705 NVIDIA RTX GPU (12GB+ VRAM for large-scale training)"}),"\n",(0,s.jsx)(e.li,{children:"\u2705 Access to physical robot hardware (for deployment sections)"}),"\n",(0,s.jsx)(e.li,{children:"\u2705 Understanding of control theory and system identification"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Estimated Reading Time"}),": 30-35 minutes"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(e.p,{children:["While you've learned to train basic policies in Isaac Gym, ",(0,s.jsx)(e.strong,{children:"production robotics"})," requires:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Advanced RL algorithms"}),": Multi-task learning, hierarchical RL, curriculum learning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sim-to-real transfer"}),": Bridging the reality gap between simulation and physical robots"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety and robustness"}),": Handling failure modes, constraints, and uncertainty"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-agent coordination"}),": Training teams of robots to collaborate"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:["This chapter explores ",(0,s.jsx)(e.strong,{children:"state-of-the-art techniques"})," for deploying RL policies in real-world robotics:"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Why Advanced RL?"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sample efficiency"}),": Learn complex behaviors faster"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Generalization"}),": Policies that work across task variations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robustness"}),": Handle sensor noise, actuator delays, and modeling errors"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Scalability"}),": Train multiple skills simultaneously"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Learning Objectives"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Implement advanced RL algorithms (HER, SAC-X, PEARL)"}),"\n",(0,s.jsx)(e.li,{children:"Design curriculum learning strategies for complex skills"}),"\n",(0,s.jsx)(e.li,{children:"Apply domain randomization and system identification for sim-to-real transfer"}),"\n",(0,s.jsx)(e.li,{children:"Deploy trained policies on physical robots with safety guarantees"}),"\n",(0,s.jsx)(e.li,{children:"Build multi-agent systems for collaborative manipulation"}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"advanced-rl-algorithms",children:"Advanced RL Algorithms"}),"\n",(0,s.jsx)(e.h3,{id:"hindsight-experience-replay-her",children:"Hindsight Experience Replay (HER)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Problem"}),': Sparse rewards make learning difficult (e.g., "reach exact position" only rewards success).']}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Solution"}),": HER treats failures as successes for ",(0,s.jsx)(e.em,{children:"different"})," goals."]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# her_reach_task.py\nfrom omni.isaac.gym.vec_env import VecEnvBase\nimport torch\nimport numpy as np\n\nclass HERReachEnv(VecEnvBase):\n    def __init__(self, num_envs=4096):\n        super().__init__(num_envs=num_envs)\n        self.replay_buffer = HERReplayBuffer(capacity=1000000)\n\n    def compute_reward(self, achieved_goal, desired_goal):\n        """Compute reward: -1 if not at goal, 0 if at goal"""\n        distance = torch.norm(achieved_goal - desired_goal, dim=-1)\n        return -(distance > 0.05).float()  # Binary reward\n\n    def step(self, actions):\n        # Execute actions\n        self.robot.apply_actions(actions)\n        self.world.step()\n\n        # Get observations\n        achieved_goal = self.robot.get_end_effector_position()\n        desired_goal = self.target_position\n        obs = self.get_observations()\n\n        # Compute reward\n        reward = self.compute_reward(achieved_goal, desired_goal)\n\n        done = (reward == 0) | (self.step_count > 200)\n\n        # Store transition\n        self.replay_buffer.add(obs, actions, reward, achieved_goal, desired_goal)\n\n        # HER: Recompute reward for alternative goals\n        if done.any():\n            self.hindsight_relabel()\n\n        return obs, reward, done, {}\n\n    def hindsight_relabel(self):\n        """Relabel past experiences with achieved goals as new goals"""\n        batch = self.replay_buffer.sample(batch_size=256)\n\n        for i in range(len(batch)):\n            original_obs = batch[i]["obs"]\n            original_action = batch[i]["action"]\n            achieved_goal = batch[i]["achieved_goal"]\n\n            # Pretend achieved_goal was the desired goal\n            new_reward = self.compute_reward(achieved_goal, achieved_goal)  # Always 0 (success)\n\n            # Store relabeled transition\n            self.replay_buffer.add(\n                original_obs,\n                original_action,\n                new_reward,\n                achieved_goal,\n                achieved_goal  # New goal\n            )\n\nclass HERReplayBuffer:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.buffer = []\n        self.position = 0\n\n    def add(self, obs, action, reward, achieved_goal, desired_goal):\n        if len(self.buffer) < self.capacity:\n            self.buffer.append(None)\n\n        self.buffer[self.position] = {\n            "obs": obs,\n            "action": action,\n            "reward": reward,\n            "achieved_goal": achieved_goal,\n            "desired_goal": desired_goal\n        }\n        self.position = (self.position + 1) % self.capacity\n\n    def sample(self, batch_size):\n        indices = np.random.randint(0, len(self.buffer), size=batch_size)\n        return [self.buffer[i] for i in indices]\n'})}),"\n",(0,s.jsx)(e.h3,{id:"soft-actor-critic-sac-for-continuous-control",children:"Soft Actor-Critic (SAC) for Continuous Control"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"SAC"})," is more sample-efficient than PPO for robotic manipulation:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# sac_manipulation.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass SACAgent:\n    def __init__(self, obs_dim, action_dim, hidden_dim=256):\n        # Actor network (policy)\n        self.actor = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim * 2)  # Mean and log_std\n        )\n\n        # Critic networks (Q-functions)\n        self.critic1 = self._build_critic(obs_dim, action_dim, hidden_dim)\n        self.critic2 = self._build_critic(obs_dim, action_dim, hidden_dim)\n\n        # Target critics (for stability)\n        self.critic1_target = self._build_critic(obs_dim, action_dim, hidden_dim)\n        self.critic2_target = self._build_critic(obs_dim, action_dim, hidden_dim)\n        self.critic1_target.load_state_dict(self.critic1.state_dict())\n        self.critic2_target.load_state_dict(self.critic2.state_dict())\n\n        # Optimizers\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=3e-4)\n        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=3e-4)\n\n        # Temperature parameter for entropy\n        self.log_alpha = torch.tensor([0.0], requires_grad=True)\n        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=3e-4)\n\n        self.gamma = 0.99  # Discount factor\n        self.tau = 0.005  # Target network update rate\n\n    def _build_critic(self, obs_dim, action_dim, hidden_dim):\n        return nn.Sequential(\n            nn.Linear(obs_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def select_action(self, obs, deterministic=False):\n        """Sample action from policy"""\n        with torch.no_grad():\n            mean, log_std = self.actor(obs).chunk(2, dim=-1)\n            log_std = torch.clamp(log_std, -20, 2)\n            std = log_std.exp()\n\n            if deterministic:\n                return torch.tanh(mean)\n\n            # Reparameterization trick\n            normal = torch.randn_like(mean)\n            action = mean + std * normal\n            return torch.tanh(action)\n\n    def update(self, batch):\n        """Update SAC agent from batch of transitions"""\n        obs, action, reward, next_obs, done = batch\n\n        # Update critics\n        with torch.no_grad():\n            next_action, next_log_prob = self._sample_action(next_obs)\n            q1_target = self.critic1_target(torch.cat([next_obs, next_action], dim=-1))\n            q2_target = self.critic2_target(torch.cat([next_obs, next_action], dim=-1))\n            q_target = torch.min(q1_target, q2_target) - self.log_alpha.exp() * next_log_prob\n            target = reward + self.gamma * (1 - done) * q_target\n\n        q1 = self.critic1(torch.cat([obs, action], dim=-1))\n        q2 = self.critic2(torch.cat([obs, action], dim=-1))\n\n        critic1_loss = nn.MSELoss()(q1, target)\n        critic2_loss = nn.MSELoss()(q2, target)\n\n        self.critic1_optimizer.zero_grad()\n        critic1_loss.backward()\n        self.critic1_optimizer.step()\n\n        self.critic2_optimizer.zero_grad()\n        critic2_loss.backward()\n        self.critic2_optimizer.step()\n\n        # Update actor\n        new_action, log_prob = self._sample_action(obs)\n        q1_new = self.critic1(torch.cat([obs, new_action], dim=-1))\n        q2_new = self.critic2(torch.cat([obs, new_action], dim=-1))\n        q_new = torch.min(q1_new, q2_new)\n\n        actor_loss = (self.log_alpha.exp() * log_prob - q_new).mean()\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # Update temperature\n        alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n        self.alpha_optimizer.zero_grad()\n        alpha_loss.backward()\n        self.alpha_optimizer.step()\n\n        # Soft update target networks\n        self._soft_update()\n\n    def _sample_action(self, obs):\n        """Sample action and compute log probability"""\n        mean, log_std = self.actor(obs).chunk(2, dim=-1)\n        log_std = torch.clamp(log_std, -20, 2)\n        std = log_std.exp()\n\n        normal = torch.randn_like(mean)\n        z = mean + std * normal\n        action = torch.tanh(z)\n\n        # Log probability with tanh correction\n        log_prob = torch.distributions.Normal(mean, std).log_prob(z)\n        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n        log_prob = log_prob.sum(dim=-1, keepdim=True)\n\n        return action, log_prob\n\n    def _soft_update(self):\n        """Soft update target networks"""\n        for param, target_param in zip(self.critic1.parameters(), self.critic1_target.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n        for param, target_param in zip(self.critic2.parameters(), self.critic2_target.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"curriculum-learning",children:"Curriculum Learning"}),"\n",(0,s.jsx)(e.h3,{id:"automatic-curriculum-design",children:"Automatic Curriculum Design"}),"\n",(0,s.jsx)(e.p,{children:"Start with easy tasks, gradually increase difficulty:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# curriculum_learning.py\nimport numpy as np\n\nclass CurriculumManager:\n    def __init__(self, initial_difficulty=0.1, max_difficulty=1.0):\n        self.difficulty = initial_difficulty\n        self.max_difficulty = max_difficulty\n        self.success_rate = 0.0\n        self.success_window = []\n\n    def update(self, success: bool):\n        """Update difficulty based on recent success rate"""\n        self.success_window.append(float(success))\n\n        if len(self.success_window) > 100:\n            self.success_window.pop(0)\n\n        self.success_rate = np.mean(self.success_window)\n\n        # Increase difficulty if success rate > 80%\n        if self.success_rate > 0.8:\n            self.difficulty = min(self.difficulty * 1.05, self.max_difficulty)\n\n        # Decrease difficulty if success rate < 40%\n        elif self.success_rate < 0.4:\n            self.difficulty = max(self.difficulty * 0.95, 0.1)\n\n    def sample_task(self):\n        """Sample task based on current difficulty"""\n        if self.difficulty < 0.3:\n            # Easy: Large objects, close positions\n            object_size = np.random.uniform(0.08, 0.12)\n            target_distance = np.random.uniform(0.2, 0.4)\n\n        elif self.difficulty < 0.7:\n            # Medium: Smaller objects, farther positions\n            object_size = np.random.uniform(0.05, 0.08)\n            target_distance = np.random.uniform(0.4, 0.7)\n\n        else:\n            # Hard: Tiny objects, occluded, far\n            object_size = np.random.uniform(0.02, 0.05)\n            target_distance = np.random.uniform(0.7, 1.0)\n\n        return {\n            "object_size": object_size,\n            "target_distance": target_distance,\n            "difficulty": self.difficulty\n        }\n\n# Usage in training loop\ncurriculum = CurriculumManager()\n\nfor episode in range(10000):\n    task_params = curriculum.sample_task()\n\n    # Setup environment with task parameters\n    env.reset(task_params)\n\n    done = False\n    while not done:\n        action = agent.select_action(obs)\n        obs, reward, done, info = env.step(action)\n\n    # Update curriculum\n    curriculum.update(success=info["success"])\n\n    if episode % 100 == 0:\n        print(f"Episode {episode}: Difficulty={curriculum.difficulty:.2f}, Success Rate={curriculum.success_rate:.2f}")\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,s.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Randomize simulation parameters"})," to create robust policies:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# domain_randomization.py\nfrom omni.isaac.core import World\nimport numpy as np\n\nclass DomainRandomizer:\n    def __init__(self, world: World):\n        self.world = world\n\n    def randomize_physics(self):\n        """Randomize physics parameters"""\n        # Randomize gravity\n        gravity = np.random.uniform(9.5, 10.0)\n        self.world.set_gravity([0, 0, -gravity])\n\n        # Randomize mass\n        for obj in self.world.scene.get_objects():\n            mass_scale = np.random.uniform(0.8, 1.2)\n            obj.set_mass(obj.get_mass() * mass_scale)\n\n        # Randomize friction\n        friction = np.random.uniform(0.3, 0.9)\n        self.world.scene.get_ground_plane().set_friction(friction)\n\n    def randomize_observations(self, obs):\n        """Add noise to observations (sensor noise)"""\n        # Position noise\n        obs["position"] += np.random.normal(0, 0.002, size=obs["position"].shape)\n\n        # Velocity noise\n        obs["velocity"] += np.random.normal(0, 0.01, size=obs["velocity"].shape)\n\n        # Force-torque noise\n        if "force" in obs:\n            obs["force"] += np.random.normal(0, 0.5, size=obs["force"].shape)\n\n        return obs\n\n    def randomize_actions(self, actions):\n        """Add actuator delay and noise"""\n        # Actuator noise\n        actions += np.random.normal(0, 0.02, size=actions.shape)\n\n        # Action clipping (actuator limits)\n        actions = np.clip(actions, -1.0, 1.0)\n\n        return actions\n\n    def randomize_visuals(self):\n        """Randomize visual appearance"""\n        # Random textures (for vision-based policies)\n        for obj in self.world.scene.get_objects():\n            color = np.random.uniform(0, 1, size=3)\n            obj.set_color(color)\n\n        # Random lighting\n        light_intensity = np.random.uniform(500, 2000)\n        self.world.set_light_intensity(light_intensity)\n\n# Training loop with randomization\nrandomizer = DomainRandomizer(world)\n\nfor episode in range(10000):\n    # Randomize at episode start\n    randomizer.randomize_physics()\n    randomizer.randomize_visuals()\n\n    obs = env.reset()\n\n    done = False\n    while not done:\n        # Add observation noise\n        noisy_obs = randomizer.randomize_observations(obs)\n\n        action = agent.select_action(noisy_obs)\n\n        # Add actuator noise\n        noisy_action = randomizer.randomize_actions(action)\n\n        obs, reward, done, info = env.step(noisy_action)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"system-identification",children:"System Identification"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Measure real robot parameters"})," and match simulation:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# system_identification.py\nimport numpy as np\nfrom scipy.optimize import minimize\n\nclass SystemIdentifier:\n    def __init__(self, robot):\n        self.robot = robot\n\n    def identify_mass(self, joint_idx):\n        """Estimate link mass from gravity compensation experiments"""\n        # Move joint to different angles, measure torques\n        angles = np.linspace(-1.0, 1.0, 20)\n        measured_torques = []\n\n        for angle in angles:\n            self.robot.set_joint_position(joint_idx, angle)\n            torque = self.robot.get_joint_torque(joint_idx)\n            measured_torques.append(torque)\n\n        # Fit model: torque = mass * g * length * sin(angle)\n        def model(params, angle):\n            mass, length = params\n            return mass * 9.81 * length * np.sin(angle)\n\n        def loss(params):\n            predicted = [model(params, a) for a in angles]\n            return np.sum((np.array(predicted) - np.array(measured_torques))**2)\n\n        result = minimize(loss, x0=[1.0, 0.3], bounds=[(0.1, 10), (0.1, 1.0)])\n        estimated_mass, estimated_length = result.x\n\n        return {"mass": estimated_mass, "length": estimated_length}\n\n    def identify_friction(self, joint_idx):\n        """Estimate joint friction from velocity experiments"""\n        # Move at constant velocities, measure required torques\n        velocities = np.linspace(-2.0, 2.0, 20)\n        measured_torques = []\n\n        for vel in velocities:\n            self.robot.set_joint_velocity(joint_idx, vel)\n            torque = self.robot.get_joint_torque(joint_idx)\n            measured_torques.append(torque)\n\n        # Fit model: torque = viscous * vel + coulomb * sign(vel)\n        def model(params, vel):\n            viscous, coulomb = params\n            return viscous * vel + coulomb * np.sign(vel)\n\n        def loss(params):\n            predicted = [model(params, v) for v in velocities]\n            return np.sum((np.array(predicted) - np.array(measured_torques))**2)\n\n        result = minimize(loss, x0=[0.1, 0.5], bounds=[(0, 2), (0, 5)])\n        viscous_friction, coulomb_friction = result.x\n\n        return {"viscous": viscous_friction, "coulomb": coulomb_friction}\n\n# Usage: Tune simulation to match real robot\nidentifier = SystemIdentifier(real_robot)\n\nfor joint_idx in range(7):\n    mass_params = identifier.identify_mass(joint_idx)\n    friction_params = identifier.identify_friction(joint_idx)\n\n    # Update simulation\n    sim_robot.set_link_mass(joint_idx, mass_params["mass"])\n    sim_robot.set_joint_friction(joint_idx, friction_params["viscous"], friction_params["coulomb"])\n\nprint("Simulation parameters updated to match real robot")\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"deployment-on-physical-robots",children:"Deployment on Physical Robots"}),"\n",(0,s.jsx)(e.h3,{id:"safety-critical-rl",children:"Safety-Critical RL"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Add safety constraints"})," before deploying:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# safe_deployment.py\nimport torch\n\nclass SafetyFilter:\n    def __init__(self, robot):\n        self.robot = robot\n        self.joint_limits = robot.get_joint_limits()\n        self.velocity_limits = robot.get_velocity_limits()\n        self.workspace_bounds = {"x": [-0.5, 0.5], "y": [-0.5, 0.5], "z": [0, 1.0]}\n\n    def filter_action(self, action):\n        """Apply safety checks to action"""\n        # 1. Joint limit check\n        current_position = self.robot.get_joint_positions()\n        predicted_position = current_position + action * 0.01  # Assuming 100Hz control\n\n        for i, (lower, upper) in enumerate(self.joint_limits):\n            if predicted_position[i] < lower or predicted_position[i] > upper:\n                action[i] = 0  # Stop motion toward limit\n\n        # 2. Velocity limit check\n        action = torch.clamp(action, -self.velocity_limits, self.velocity_limits)\n\n        # 3. Workspace boundary check\n        ee_pos = self.robot.get_end_effector_position()\n        if not self._in_workspace(ee_pos):\n            # Compute direction to workspace center\n            center = torch.tensor([0, 0, 0.5])\n            action = (center - ee_pos) * 0.1  # Move toward center\n\n        return action\n\n    def _in_workspace(self, position):\n        """Check if position is within safe workspace"""\n        x, y, z = position\n        return (self.workspace_bounds["x"][0] <= x <= self.workspace_bounds["x"][1] and\n                self.workspace_bounds["y"][0] <= y <= self.workspace_bounds["y"][1] and\n                self.workspace_bounds["z"][0] <= z <= self.workspace_bounds["z"][1])\n\n# Deploy policy with safety filter\nsafety_filter = SafetyFilter(robot)\n\nobs = env.reset()\nfor step in range(1000):\n    # Policy selects action\n    action = policy.select_action(obs)\n\n    # Safety filter modifies action\n    safe_action = safety_filter.filter_action(action)\n\n    # Execute safe action\n    obs, reward, done, info = env.step(safe_action)\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"multi-agent-rl",children:"Multi-Agent RL"}),"\n",(0,s.jsx)(e.h3,{id:"collaborative-manipulation",children:"Collaborative Manipulation"}),"\n",(0,s.jsx)(e.p,{children:"Train multiple robots to collaborate:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# multi_agent_manipulation.py\nfrom omni.isaac.gym.vec_env import VecEnvBase\nimport torch\n\nclass MultiAgentEnv(VecEnvBase):\n    def __init__(self, num_envs=1024, num_agents=2):\n        super().__init__(num_envs=num_envs)\n        self.num_agents = num_agents\n\n        # Add multiple robots per environment\n        for env_idx in range(num_envs):\n            for agent_idx in range(num_agents):\n                robot_path = f"/World/Env_{env_idx}/Robot_{agent_idx}"\n                self.add_robot(robot_path, position=[agent_idx * 0.5, 0, 0])\n\n    def step(self, actions):\n        """\n        Actions: (num_envs, num_agents, action_dim)\n        """\n        # Apply actions for all agents\n        for agent_idx in range(self.num_agents):\n            agent_actions = actions[:, agent_idx, :]\n            self.robots[agent_idx].apply_actions(agent_actions)\n\n        self.world.step()\n\n        # Compute observations and rewards\n        obs = self.get_observations()  # Shape: (num_envs, num_agents, obs_dim)\n        rewards = self.compute_rewards()  # Shape: (num_envs, num_agents)\n\n        return obs, rewards, self.check_done(), {}\n\n    def compute_rewards(self):\n        """Compute collaborative reward"""\n        # Shared reward: both agents must grasp the object\n        object_pos = self.get_object_position()\n\n        agent_0_dist = torch.norm(self.robots[0].get_ee_position() - object_pos, dim=-1)\n        agent_1_dist = torch.norm(self.robots[1].get_ee_position() - object_pos, dim=-1)\n\n        # Reward when both agents are close to object\n        collaborative_reward = -(agent_0_dist + agent_1_dist)\n\n        # Bonus if object is lifted\n        object_height = object_pos[:, 2]\n        lift_bonus = torch.clamp(object_height - 0.3, 0, 1) * 10\n\n        return collaborative_reward + lift_bonus\n'})}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Advanced RL algorithms"})," (HER, SAC) improve sample efficiency and enable sparse-reward learning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Curriculum learning"})," automatically adjusts task difficulty to accelerate training"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Domain randomization"})," creates robust policies that transfer to real robots despite sim-to-real gap"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"System identification"})," tunes simulation parameters to match real robot dynamics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety filters"})," ensure deployed policies respect joint limits, workspace boundaries, and velocity constraints"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-agent RL"})," enables collaborative manipulation with coordinated policies"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:["Mastering these techniques transforms RL from a research tool into a ",(0,s.jsx)(e.strong,{children:"production-ready"})," technology for deploying intelligent behaviors on physical robots."]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Previous Chapter"}),": ",(0,s.jsx)(e.a,{href:"/physical-ai-humanoid-robotics/docs/module-3-isaac/ai-powered-perception",children:"AI-Powered Perception"}),"\n",(0,s.jsx)(e.strong,{children:"Next Module"}),": ",(0,s.jsx)(e.a,{href:"/docs/module-4-vla/intro-vla",children:"Module 4: Vision-Language-Action Models"})]}),"\n",(0,s.jsx)(e.p,{children:"Congratulations! You've completed Module 3: NVIDIA Isaac. You now have the skills to simulate, train, and deploy intelligent robotic systems. In Module 4, we'll explore cutting-edge Vision-Language-Action models that enable robots to understand natural language instructions and visual scenes."})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>r});var t=i(6540);const s={},o=t.createContext(s);function a(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);