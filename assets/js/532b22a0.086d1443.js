"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_01=globalThis.webpackChunkphysical_ai_humanoid_robotics_01||[]).push([[792],{1274:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4-vla/vla-architectures","title":"VLA Architectures: RT-1, RT-2, PaLM-E","description":"Deep dive into state-of-the-art Vision-Language-Action architectures including RT-1, RT-2, and PaLM-E","source":"@site/docs/module-4-vla/vla-architectures.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/vla-architectures","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/vla-architectures","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/vla-architectures.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"VLA Architectures: RT-1, RT-2, PaLM-E","description":"Deep dive into state-of-the-art Vision-Language-Action architectures including RT-1, RT-2, and PaLM-E"},"sidebar":"mainSidebar","previous":{"title":"Introduction to VLA Models","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/intro-vla"},"next":{"title":"Training and Deploying VLA Systems","permalink":"/physical-ai-humanoid-robotics/docs/module-4-vla/vla-training-deployment"}}');var r=i(4848),t=i(8453);const o={sidebar_position:2,title:"VLA Architectures: RT-1, RT-2, PaLM-E",description:"Deep dive into state-of-the-art Vision-Language-Action architectures including RT-1, RT-2, and PaLM-E"},l="VLA Architectures: RT-1, RT-2, PaLM-E",a={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"RT-1: Robotics Transformer 1",id:"rt-1-robotics-transformer-1",level:2},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Token Learner Module",id:"token-learner-module",level:3},{value:"Transformer Encoder-Decoder",id:"transformer-encoder-decoder",level:3},{value:"Action Tokenization",id:"action-tokenization",level:3},{value:"Training Data",id:"training-data",level:3},{value:"RT-1 Performance",id:"rt-1-performance",level:3},{value:"RT-1 Architecture Diagram",id:"rt-1-architecture-diagram",level:2},{value:"RT-2: Robotics Transformer 2",id:"rt-2-robotics-transformer-2",level:2},{value:"Key Innovation: VLM-to-Robot Transfer",id:"key-innovation-vlm-to-robot-transfer",level:3},{value:"Architecture",id:"architecture",level:3},{value:"Emergent Capabilities",id:"emergent-capabilities",level:3},{value:"Inference",id:"inference",level:3},{value:"RT-2 Architecture Diagram",id:"rt-2-architecture-diagram",level:2},{value:"PaLM-E: Embodied Language Model",id:"palm-e-embodied-language-model",level:2},{value:"Architecture",id:"architecture-1",level:3},{value:"Training Strategy",id:"training-strategy",level:3},{value:"Long-Horizon Planning",id:"long-horizon-planning",level:3},{value:"Multimodal Reasoning",id:"multimodal-reasoning",level:3},{value:"Architecture Comparison Table",id:"architecture-comparison-table",level:2},{value:"Design Trade-offs",id:"design-trade-offs",level:3},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: Read RT-1 Paper in Depth",id:"exercise-1-read-rt-1-paper-in-depth",level:3},{value:"Exercise 2: Compare RT-2 Improvements Over RT-1",id:"exercise-2-compare-rt-2-improvements-over-rt-1",level:3},{value:"Exercise 3: Analyze PaLM-E Multimodal Integration",id:"exercise-3-analyze-palm-e-multimodal-integration",level:3},{value:"Exercise 4: Reproduce Architecture Diagrams",id:"exercise-4-reproduce-architecture-diagrams",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Navigation",id:"navigation",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vla-architectures-rt-1-rt-2-palm-e",children:"VLA Architectures: RT-1, RT-2, PaLM-E"})}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before diving into this chapter, ensure you have:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Completed ",(0,r.jsx)(n.strong,{children:"Chapter 1: Introduction to VLA Models"})," from this module"]}),"\n",(0,r.jsxs)(n.li,{children:["Strong understanding of ",(0,r.jsx)(n.strong,{children:"transformer architecture"})," (self-attention, positional encodings, encoder-decoder)"]}),"\n",(0,r.jsxs)(n.li,{children:["Familiarity with ",(0,r.jsx)(n.strong,{children:"vision transformers (ViT)"})," and token-based image processing"]}),"\n",(0,r.jsxs)(n.li,{children:["Knowledge of ",(0,r.jsx)(n.strong,{children:"language models"})," (BERT, T5, GPT architecture)"]}),"\n",(0,r.jsxs)(n.li,{children:["Experience with ",(0,r.jsx)(n.strong,{children:"PyTorch"})," or ",(0,r.jsx)(n.strong,{children:"TensorFlow"})," for reading model code"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsxs)(n.p,{children:["This chapter explores three groundbreaking VLA architectures that define the state-of-the-art in robot learning: ",(0,r.jsx)(n.strong,{children:"RT-1 (Robotics Transformer 1)"}),", ",(0,r.jsx)(n.strong,{children:"RT-2 (Robotics Transformer 2)"}),", and ",(0,r.jsx)(n.strong,{children:"PaLM-E (Embodied Language Model)"}),". Each model represents a different approach to integrating vision, language, and action:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RT-1"})," pioneered efficient visual tokenization and action discretization for robot control"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RT-2"})," leveraged massive vision-language pretraining to achieve emergent reasoning capabilities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PaLM-E"})," scaled to 562 billion parameters, enabling multimodal planning and long-horizon tasks"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"You'll learn the architectural details of each model, understand their design trade-offs, and compare their strengths and limitations across real-world robotics benchmarks."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Learning Objectives:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand RT-1's token learner and action tokenization mechanisms"}),"\n",(0,r.jsx)(n.li,{children:"Explain how RT-2 leverages VLM pretraining for emergent robotic capabilities"}),"\n",(0,r.jsx)(n.li,{children:"Analyze PaLM-E's multimodal integration and planning architecture"}),"\n",(0,r.jsx)(n.li,{children:"Compare RT-1, RT-2, and PaLM-E across key dimensions (parameters, data, performance)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"rt-1-robotics-transformer-1",children:"RT-1: Robotics Transformer 1"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-1"}),", introduced by Google's Robotics at Google team in December 2022, was the first large-scale VLA model to demonstrate broad generalization across hundreds of real-world robot tasks. Trained on ",(0,r.jsx)(n.strong,{children:"130,000 demonstrations"})," spanning ",(0,r.jsx)(n.strong,{children:"700+ tasks"})," in office kitchens, RT-1 achieved ",(0,r.jsx)(n.strong,{children:"97% success"})," on seen tasks and ",(0,r.jsx)(n.strong,{children:"76% success"})," on novel language instructions."]}),"\n",(0,r.jsx)(n.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,r.jsx)(n.p,{children:"RT-1 consists of three core components:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Token Learner"}),": Compresses 300\xd7300 RGB images into 81 visual tokens (9\xd79 grid)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transformer Encoder"}),": Processes visual tokens + language tokens using 8 layers of self-attention"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action Decoder"}),": Outputs 11-dimensional action tokens (7 arm joints + 3 base velocities + 1 gripper)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The key innovation is ",(0,r.jsx)(n.strong,{children:"visual tokenization"}),"\u2014instead of processing full images (90,000 pixels), RT-1 learns a compact 81-token representation, reducing computational cost by 1000\xd7 while preserving task-relevant visual information (object locations, gripper pose, scene context)."]}),"\n",(0,r.jsx)(n.h3,{id:"token-learner-module",children:"Token Learner Module"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"Token Learner"})," (Ryoo et al., 2021) uses ",(0,r.jsx)(n.strong,{children:"learned attention"})," to select the most relevant spatial regions from image features:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Architecture"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Input: 300\xd7300\xd73 RGB image\n\u2193\nResNet-50 backbone \u2192 19\xd719\xd72048 feature map (6,859 spatial features)\n\u2193\nToken Learner (spatial attention) \u2192 81\xd7512 visual tokens\n\u2193\nOutput: 81 tokens representing key visual regions\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"How it works"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"ResNet-50 extracts convolutional features from the image (19\xd719 spatial grid with 2048 channels)"}),"\n",(0,r.jsx)(n.li,{children:"Token Learner applies a small MLP to each spatial location, producing an attention score"}),"\n",(0,r.jsx)(n.li,{children:"Top-81 spatial regions (9\xd79 grid) are selected based on attention scores"}),"\n",(0,r.jsx)(n.li,{children:"Selected features are projected to 512-dimensional visual tokens"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Benefit"}),": Reduces sequence length from 361 image patches (standard ViT) to 81 tokens, enabling faster inference (5 Hz on GPU) while focusing on task-relevant regions (robot gripper, target objects, obstacles)."]}),"\n",(0,r.jsx)(n.h3,{id:"transformer-encoder-decoder",children:"Transformer Encoder-Decoder"}),"\n",(0,r.jsxs)(n.p,{children:["RT-1 uses a ",(0,r.jsx)(n.strong,{children:"12-layer transformer"})," to process multimodal sequences:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Input sequence"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"81 visual tokens (from Token Learner)"}),"\n",(0,r.jsx)(n.li,{children:'16 language tokens (BERT-encoded instruction like "pick red apple")'}),"\n",(0,r.jsx)(n.li,{children:"Total: 97 tokens"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Encoder"})," (8 layers):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Self-attention across all 97 tokens to fuse vision and language"}),"\n",(0,r.jsx)(n.li,{children:"Each token attends to all others, learning cross-modal alignments"}),"\n",(0,r.jsx)(n.li,{children:'Example: Token representing "red object region" attends to language token "red" and "apple"'}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Decoder"})," (4 layers):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Predicts 11-dimensional action vector autoregressively"}),"\n",(0,r.jsx)(n.li,{children:"Each action dimension is discretized into 256 bins (quantization)"}),"\n",(0,r.jsx)(n.li,{children:"Output: Sequence of 11 discrete tokens representing joint angles and gripper state"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"action-tokenization",children:"Action Tokenization"}),"\n",(0,r.jsxs)(n.p,{children:["RT-1 discretizes continuous robot actions into ",(0,r.jsx)(n.strong,{children:"discrete tokens"})," for classification (instead of regression):"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Continuous actions"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"7 arm joint angles (radians): [-\u03c0, \u03c0]"}),"\n",(0,r.jsx)(n.li,{children:"3 base velocities (m/s): [-0.5, 0.5]"}),"\n",(0,r.jsx)(n.li,{children:"1 gripper command: {open, close}"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tokenization"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Divide each continuous range into 256 bins (8-bit quantization)","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Example: Joint 1 range [-\u03c0, \u03c0] \u2192 bins [0, 255]"}),"\n",(0,r.jsx)(n.li,{children:"Angle 1.57 rad \u2192 bin 192"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Transformer predicts bin index (0-255) for each action dimension"}),"\n",(0,r.jsx)(n.li,{children:"Bin index is converted back to continuous value during execution"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Classification loss"})," (cross-entropy) more stable than regression loss (MSE) for high-dimensional actions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Discretization prevents mode averaging"}),": Avoids predicting average action when multiple valid actions exist (e.g., grasp left side vs. right side of object)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Easier to train"}),": Softmax outputs more robust than unbounded regression outputs"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"training-data",children:"Training Data"}),"\n",(0,r.jsxs)(n.p,{children:["RT-1 was trained on the ",(0,r.jsx)(n.strong,{children:"RT-1 dataset"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"130,000 teleoperation demonstrations"})," collected over 17 months"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"700+ tasks"})," across 13 robots in office kitchens"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Instruction diversity"}),": 1,000+ unique language instructions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object diversity"}),": 200+ household objects (fruits, cans, bottles, utensils)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environment diversity"}),": 5 different kitchen layouts with varying lighting and clutter"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Data collection process"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Human operator teleoperates robot using VR controller (Oculus Quest)"}),"\n",(0,r.jsx)(n.li,{children:"Robot executes task while recording: camera images (10 Hz), language instruction, joint states, actions"}),"\n",(0,r.jsx)(n.li,{children:"Successful demonstrations are labeled and added to dataset"}),"\n",(0,r.jsx)(n.li,{children:"Failed attempts are discarded (no negative examples)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"rt-1-performance",children:"RT-1 Performance"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Benchmark results"})," (Google's office kitchen robots, 2022):"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Task Category"}),(0,r.jsx)(n.th,{children:"Success Rate"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Seen tasks (trained)"}),(0,r.jsx)(n.td,{children:"97%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Novel instructions (unseen)"}),(0,r.jsx)(n.td,{children:"76%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Novel objects (seen category)"}),(0,r.jsx)(n.td,{children:"83%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Novel objects (unseen category)"}),(0,r.jsx)(n.td,{children:"62%"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example capabilities"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Move Coke can to the top drawer" \u2192 95% success'}),"\n",(0,r.jsx)(n.li,{children:'"Place apple in the blue bowl" \u2192 89% success (novel bowl color)'}),"\n",(0,r.jsx)(n.li,{children:'"Pick the fruit" \u2192 78% success (generalizes to unseen fruits: mango, kiwi)'}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"rt-1-architecture-diagram",children:"RT-1 Architecture Diagram"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"flowchart TD\n    A[Camera Image<br/>300\xd7300 RGB] --\x3e B[ResNet-50<br/>Backbone]\n    B --\x3e C[Token Learner<br/>19\xd719\xd72048 \u2192 81\xd7512]\n    D[Language Instruction<br/>'Pick up the apple'] --\x3e E[BERT Encoder<br/>16 tokens]\n    C --\x3e F[Transformer Encoder<br/>8 layers, 97 tokens]\n    E --\x3e F\n    F --\x3e G[Transformer Decoder<br/>4 layers]\n    G --\x3e H[Action Tokens<br/>11 bins \xd7 256 classes]\n    H --\x3e I[Detokenize<br/>Bin \u2192 Continuous]\n    I --\x3e J[Robot Execution<br/>7 joints + 3 base + 1 gripper]\n\n    style A fill:#e1f5ff\n    style D fill:#fff4e1\n    style H fill:#e8f5e9\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Alt text"}),": RT-1 architecture flowchart showing image processing through ResNet-50 and Token Learner, language encoding through BERT, multimodal fusion in transformer encoder, action prediction in decoder, discretization into 256-bin tokens, and execution on robot."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"rt-2-robotics-transformer-2",children:"RT-2: Robotics Transformer 2"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-2"}),", released in July 2023, dramatically improved RT-1's generalization by ",(0,r.jsx)(n.strong,{children:"co-fine-tuning"})," a pretrained vision-language model (VLM) on robot data. Instead of training from scratch, RT-2 leverages ",(0,r.jsx)(n.strong,{children:"PaLI-X"})," (55 billion parameters) pretrained on 10 billion image-text pairs from the internet, enabling ",(0,r.jsx)(n.strong,{children:"emergent capabilities"})," like reasoning about object properties, spatial relationships, and semantic categories."]}),"\n",(0,r.jsx)(n.h3,{id:"key-innovation-vlm-to-robot-transfer",children:"Key Innovation: VLM-to-Robot Transfer"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hypothesis"}),': Vision-language models (VLMs) that understand "a red apple is a fruit" from web images can transfer this knowledge to robotic manipulation when fine-tuned on robot demonstrations.']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Approach"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Start with ",(0,r.jsx)(n.strong,{children:"PaLI-X"})," (55B parameters), a VLM pretrained on 10B image-text pairs (web images + captions)"]}),"\n",(0,r.jsx)(n.li,{children:"Extend output vocabulary: Add robot action tokens (256 bins \xd7 11 dimensions = 2,816 new tokens)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Co-fine-tune"})," on both:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Web data: Image-caption pairs (e.g., "a red apple on a table")'}),"\n",(0,r.jsx)(n.li,{children:"Robot data: Image-action pairs (e.g., camera image \u2192 joint angles)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["Model learns to generate ",(0,r.jsx)(n.strong,{children:"text"})," for language tasks and ",(0,r.jsx)(n.strong,{children:"action tokens"})," for robot tasks"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Result"}),": RT-2 can perform ",(0,r.jsx)(n.strong,{children:"zero-shot reasoning"})," unavailable to RT-1:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Pick the extinct animal" \u2192 Grasps toy dinosaur (never explicitly trained on "extinct" + "dinosaur")'}),"\n",(0,r.jsx)(n.li,{children:'"Move the drink to the person" \u2192 Identifies beverage cans and navigates toward humans'}),"\n",(0,r.jsx)(n.li,{children:'"Pick the item you\'d use to hammer in a nail" \u2192 Selects rock (tool affordance reasoning)'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,r.jsxs)(n.p,{children:["RT-2 is based on ",(0,r.jsx)(n.strong,{children:"PaLI-X"}),", a vision-language model with:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision Encoder"}),": ViT-e (4B parameters) processes 224\xd7224 images into 256 visual tokens"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language Model"}),": UL2 (51B parameters), a T5-style encoder-decoder transformer"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multimodal Fusion"}),": Cross-attention in decoder attends to visual tokens conditioned on language"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Modifications for robotics"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Add action tokenizer: Maps 11D continuous actions \u2192 2,816 discrete action tokens"}),"\n",(0,r.jsxs)(n.li,{children:["Train with mixed objectives:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Language modeling"}),": Predict next word given image + text prefix"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Action prediction"}),": Predict action token given image + instruction"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Training"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pretraining"}),": 10B image-text pairs from web (Google internal dataset, similar to LAION-5B)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fine-tuning"}),": RT-1 dataset (130K demos) + RT-2 dataset (50K new demos)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Co-fine-tuning"}),": Alternate batches of web data (image captioning) and robot data (action prediction)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"emergent-capabilities",children:"Emergent Capabilities"}),"\n",(0,r.jsxs)(n.p,{children:["RT-2 demonstrates capabilities ",(0,r.jsx)(n.strong,{children:"never explicitly trained"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Semantic Reasoning"}),': "Pick the extinct animal" \u2192 Correctly identifies toy dinosaur among multiple objects']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Spatial Reasoning"}),': "Move the cup to the left of the plate" \u2192 Understands spatial prepositions from language pretraining']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Category Generalization"}),': "Pick the fruit" \u2192 Generalizes to 15+ fruit types vs. 5 in RT-1']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Affordance Reasoning"}),': "Pick something to cut the apple" \u2192 Selects knife (tool-use reasoning)']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance vs. RT-1"}),":"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Metric"}),(0,r.jsx)(n.th,{children:"RT-1"}),(0,r.jsx)(n.th,{children:"RT-2"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Seen tasks"}),(0,r.jsx)(n.td,{children:"97%"}),(0,r.jsx)(n.td,{children:"98%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Novel objects (seen category)"}),(0,r.jsx)(n.td,{children:"83%"}),(0,r.jsx)(n.td,{children:"91%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Novel objects (unseen category)"}),(0,r.jsx)(n.td,{children:"62%"}),(0,r.jsx)(n.td,{children:"79%"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Symbolic reasoning tasks"}),(0,r.jsx)(n.td,{children:"0%"}),(0,r.jsx)(n.td,{children:"74%"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symbolic reasoning examples"})," (new capability):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'"Pick the bag of chips" \u2192 85% success (understands "bag" packaging)'}),"\n",(0,r.jsx)(n.li,{children:'"Move the smallest object" \u2192 72% success (visual size comparison)'}),"\n",(0,r.jsx)(n.li,{children:'"Pick the emergency equipment" \u2192 68% success (fire extinguisher selected)'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"inference",children:"Inference"}),"\n",(0,r.jsxs)(n.p,{children:["RT-2 runs inference at ",(0,r.jsx)(n.strong,{children:"3 Hz"})," on NVIDIA A100 GPU:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Latency breakdown"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Vision encoding (ViT-e): 180ms"}),"\n",(0,r.jsx)(n.li,{children:"Language encoding (UL2): 50ms"}),"\n",(0,r.jsx)(n.li,{children:"Cross-attention + action decoding: 80ms"}),"\n",(0,r.jsx)(n.li,{children:"Total: 310ms per action prediction"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Deployment"}),": RT-2 is deployed on Google's mobile manipulator robots with:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"NVIDIA Jetson AGX Orin (edge GPU) running vision encoder (7 Hz)"}),"\n",(0,r.jsx)(n.li,{children:"Cloud server running full RT-2 (3 Hz) via 5G connection"}),"\n",(0,r.jsx)(n.li,{children:"Robot controller interpolates actions between predictions for smooth 20Hz execution"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"rt-2-architecture-diagram",children:"RT-2 Architecture Diagram"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"flowchart TD\n    A[Vision-Language Model<br/>PaLI-X 55B params] --\x3e B[Vision Encoder<br/>ViT-e 4B]\n    A --\x3e C[Language Model<br/>UL2 51B]\n    D[Pretraining<br/>10B image-text pairs] --\x3e A\n    E[Robot Camera<br/>224\xd7224 RGB] --\x3e F[ViT-e<br/>256 visual tokens]\n    G[Instruction<br/>'Pick the fruit'] --\x3e H[UL2 Encoder<br/>Language tokens]\n    F --\x3e I[UL2 Decoder<br/>Cross-attention]\n    H --\x3e I\n    I --\x3e J{Output Type}\n    J --\x3e|Language Task| K[Text Tokens<br/>'A red apple']\n    J --\x3e|Robot Task| L[Action Tokens<br/>11D discretized]\n    L --\x3e M[Robot Execution]\n\n    style D fill:#ffe6e6\n    style A fill:#e6f3ff\n    style L fill:#e8f5e9\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Alt text"}),": RT-2 architecture flowchart showing PaLI-X VLM pretrained on 10B image-text pairs, then co-fine-tuned to output either text tokens (language tasks) or action tokens (robot tasks) through shared vision encoder and language model."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"palm-e-embodied-language-model",children:"PaLM-E: Embodied Language Model"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PaLM-E"})," (March 2023) is the largest VLA model to date, integrating ",(0,r.jsx)(n.strong,{children:"PaLM"})," language model (540B parameters) with visual observations to enable ",(0,r.jsx)(n.strong,{children:"multimodal planning"}),", ",(0,r.jsx)(n.strong,{children:"long-horizon reasoning"}),", and ",(0,r.jsx)(n.strong,{children:"state estimation"})," for embodied agents."]}),"\n",(0,r.jsx)(n.h3,{id:"architecture-1",children:"Architecture"}),"\n",(0,r.jsx)(n.p,{children:"PaLM-E combines:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PaLM"})," (540B parameters): Large language model pretrained on 780B text tokens"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision Encoder"}),": ViT-22B processes images into 256 visual tokens"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Encoders"}),": Additional encoders for depth, LiDAR, proprioception (joint states)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multimodal Integration"}),": Inject vision/sensor tokens into PaLM's text sequence"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Multimodal sequence"})," (example):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'[TEXT] "The robot should" [IMAGE_TOKEN_1] ... [IMAGE_TOKEN_256] [TEXT] "pick up the mug because" [IMAGE_TOKEN_1] ... [TEXT] "the user requested coffee."\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Visual tokens are ",(0,r.jsx)(n.strong,{children:"interleaved"})," with text tokens, allowing PaLM to:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Reason about visual observations using language priors"}),"\n",(0,r.jsx)(n.li,{children:'Ground language in physical states (e.g., "the mug is to the left" requires spatial grounding)'}),"\n",(0,r.jsx)(n.li,{children:'Generate action plans as text sequences (e.g., "Step 1: Grasp mug. Step 2: Move to coffee machine.")'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"training-strategy",children:"Training Strategy"}),"\n",(0,r.jsxs)(n.p,{children:["PaLM-E uses ",(0,r.jsx)(n.strong,{children:"multi-task co-training"})," across diverse modalities:"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision-language"}),": Image captioning, VQA (Visual Question Answering)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robotics"}),": Manipulation tasks (RT-1 dataset + 20,000 long-horizon demos)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embodied QA"}),': "What object is on the table?" given robot camera feed']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"State estimation"}),": Predict object poses from images"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Planning"}),': Generate task plans given goals ("make coffee" \u2192 multi-step plan)']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Training details"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"562B parameters"})," (largest embodied model)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"100,000 robot demos"})," across 20 robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"10M image-text pairs"})," for vision-language grounding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mixed-precision training"})," on 2048 TPU v4 chips for 1 month"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"long-horizon-planning",children:"Long-Horizon Planning"}),"\n",(0,r.jsxs)(n.p,{children:["PaLM-E can decompose high-level goals into ",(0,r.jsx)(n.strong,{children:"multi-step action sequences"}),":"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example"}),': "Prepare a meal"']}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:'Understand goal: Parse "prepare a meal" \u2192 sequence of sub-tasks'}),"\n",(0,r.jsx)(n.li,{children:"Visual grounding: Identify ingredients on counter (tomato, bread, cheese)"}),"\n",(0,r.jsxs)(n.li,{children:["Generate plan:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Step 1: Grasp knife\nStep 2: Cut tomato into slices\nStep 3: Open bread bag\nStep 4: Place tomato slices on bread\nStep 5: Add cheese\nStep 6: Close sandwich\nStep 7: Serve to user\n"})}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Execute: Convert each step into low-level actions using RT-1-style action decoder"}),"\n",(0,r.jsx)(n.li,{children:"Monitor: After each step, re-perceive scene and adjust plan if needed"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Closed-loop execution"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'After "Step 1: Grasp knife", PaLM-E checks: "Did I successfully grasp the knife?"'}),"\n",(0,r.jsx)(n.li,{children:"If yes: Proceed to Step 2"}),"\n",(0,r.jsx)(n.li,{children:'If no: Replan: "The knife is out of reach. Step 1b: Navigate closer to the counter."'}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Success rate"})," on long-horizon tasks (5-20 steps):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Simple tasks (3-5 steps): 87% success"}),"\n",(0,r.jsx)(n.li,{children:"Medium tasks (6-10 steps): 68% success"}),"\n",(0,r.jsx)(n.li,{children:"Complex tasks (11-20 steps): 52% success"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"multimodal-reasoning",children:"Multimodal Reasoning"}),"\n",(0,r.jsxs)(n.p,{children:["PaLM-E can answer questions requiring ",(0,r.jsx)(n.strong,{children:"cross-modal reasoning"}),":"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example interactions"}),":"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"User"}),': "Why is the drawer stuck?" (given robot camera showing drawer)\n',(0,r.jsx)(n.strong,{children:"PaLM-E"}),': "The spoon is blocking the drawer from closing fully. The handle is wedged against the drawer frame."']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"User"}),': "Can I stack the glass on the plate?"\n',(0,r.jsx)(n.strong,{children:"PaLM-E"}),': "No, the glass is too large and heavy. It would likely slide off or tip over. Place it next to the plate instead."']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"User"}),': "What\'s the fastest way to sort these items by color?"\n',(0,r.jsx)(n.strong,{children:"PaLM-E"}),': "Group by primary color first (reds together, blues together), then fine-tune within groups. Start with the largest items to save space."']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Reasoning types"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Physical reasoning"}),": Weight, balance, friction, stability"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Spatial reasoning"}),": Proximity, containment, occlusion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Causal reasoning"}),': "If I move X, then Y will happen"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Temporal reasoning"}),': "Do X before Y" (order constraints)']}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"architecture-comparison-table",children:"Architecture Comparison Table"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"RT-1"}),(0,r.jsx)(n.th,{children:"RT-2"}),(0,r.jsx)(n.th,{children:"PaLM-E"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Parameters"})}),(0,r.jsx)(n.td,{children:"35M"}),(0,r.jsx)(n.td,{children:"55B (4B vision + 51B language)"}),(0,r.jsx)(n.td,{children:"562B (22B vision + 540B language)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Pretraining"})}),(0,r.jsx)(n.td,{children:"None (trained from scratch)"}),(0,r.jsx)(n.td,{children:"PaLI-X on 10B image-text pairs"}),(0,r.jsx)(n.td,{children:"PaLM on 780B text tokens + ViT-22B"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Training Data"})}),(0,r.jsx)(n.td,{children:"130K robot demos"}),(0,r.jsx)(n.td,{children:"180K robot demos + web data"}),(0,r.jsx)(n.td,{children:"100K robot demos + 10M image-text pairs"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Tasks"})}),(0,r.jsx)(n.td,{children:"Single-step manipulation"}),(0,r.jsx)(n.td,{children:"Single-step + reasoning"}),(0,r.jsx)(n.td,{children:"Long-horizon planning (5-20 steps)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Generalization"})}),(0,r.jsx)(n.td,{children:"Novel objects (same category)"}),(0,r.jsx)(n.td,{children:"Novel objects + symbolic reasoning"}),(0,r.jsx)(n.td,{children:"Cross-domain transfer + planning"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Inference Speed"})}),(0,r.jsx)(n.td,{children:"5 Hz (GPU)"}),(0,r.jsx)(n.td,{children:"3 Hz (A100 GPU)"}),(0,r.jsx)(n.td,{children:"0.5 Hz (2048 TPU)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Novel Capabilities"})}),(0,r.jsx)(n.td,{children:"Action tokenization"}),(0,r.jsx)(n.td,{children:"Emergent reasoning from VLM"}),(0,r.jsx)(n.td,{children:"Multimodal QA, state estimation"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Use Cases"})}),(0,r.jsx)(n.td,{children:"Warehouse picking, assembly"}),(0,r.jsx)(n.td,{children:"Household robots, retail"}),(0,r.jsx)(n.td,{children:"Research, complex planning"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Deployment"})}),(0,r.jsx)(n.td,{children:"Edge devices (Jetson Orin)"}),(0,r.jsx)(n.td,{children:"Cloud + edge hybrid"}),(0,r.jsx)(n.td,{children:"Cloud-only (TPU inference)"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"design-trade-offs",children:"Design Trade-offs"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-1"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Fast inference (5 Hz)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Compact model (35M params) for edge deployment"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Limited generalization to novel categories"}),"\n",(0,r.jsx)(n.li,{children:"\u274c No reasoning about object properties"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-2"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Emergent reasoning (pick extinct animal, smallest object)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Strong zero-shot generalization (79% on novel objects)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Slow inference (3 Hz) requires powerful GPU"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Cannot handle long-horizon tasks (>1 step)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PaLM-E"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Long-horizon planning (20+ steps)"}),"\n",(0,r.jsx)(n.li,{children:'\u2705 Multimodal reasoning (answer "why" questions)'}),"\n",(0,r.jsx)(n.li,{children:"\u274c Extremely slow (0.5 Hz inference)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Requires TPU cluster (not deployable on robots)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Production systems"}),": RT-1 or RT-2 (deployable, real-time performance)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Research"}),": PaLM-E (push boundaries of what's possible)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hybrid approach"}),": PaLM-E for high-level planning, RT-2 for low-level execution"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-read-rt-1-paper-in-depth",children:"Exercise 1: Read RT-1 Paper in Depth"}),"\n",(0,r.jsxs)(n.p,{children:["Access the original RT-1 paper: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2212.06817",children:"RT-1: Robotics Transformer for Real-World Control at Scale"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Explain the Token Learner architecture in detail. How does it reduce sequence length from 361 to 81 tokens?"}),"\n",(0,r.jsx)(n.li,{children:"Why did the authors choose action discretization (256 bins) over continuous action regression?"}),"\n",(0,r.jsx)(n.li,{children:"Analyze Table 2 (generalization results). What factors contribute to the 21% drop in success rate for novel instructions (76% vs. 97%)?"}),"\n",(0,r.jsx)(n.li,{children:"Reproduce the RT-1 architecture diagram in Figure 2. Label all components and data dimensions."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected time"}),": 60 minutes\n",(0,r.jsx)(n.strong,{children:"Learning goal"}),": Deep understanding of RT-1 design decisions and their empirical justification."]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-compare-rt-2-improvements-over-rt-1",children:"Exercise 2: Compare RT-2 Improvements Over RT-1"}),"\n",(0,r.jsx)(n.p,{children:"Create a detailed comparison document covering:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Technical improvements"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Vision encoder: ResNet-50 + Token Learner (RT-1) vs. ViT-e (RT-2)"}),"\n",(0,r.jsx)(n.li,{children:"Language model: BERT (RT-1) vs. UL2 (RT-2)"}),"\n",(0,r.jsx)(n.li,{children:"Training data: 130K robot demos (RT-1) vs. 180K + 10B web pairs (RT-2)"}),"\n",(0,r.jsx)(n.li,{children:"Model size: 35M (RT-1) vs. 55B (RT-2)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capability improvements"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Quantify generalization gains (use Tables 3-4 from RT-2 paper)"}),"\n",(0,r.jsx)(n.li,{children:"List 5 emergent capabilities in RT-2 absent in RT-1"}),"\n",(0,r.jsx)(n.li,{children:"Analyze failure modes: What tasks still fail for both models?"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Trade-offs"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Inference speed: RT-1 (5 Hz) vs. RT-2 (3 Hz)"}),"\n",(0,r.jsx)(n.li,{children:"Deployment: RT-1 on Jetson Orin vs. RT-2 requiring A100"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected time"}),": 45 minutes\n",(0,r.jsx)(n.strong,{children:"Learning goal"}),": Understand how VLM pretraining enables emergent robotic capabilities."]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-analyze-palm-e-multimodal-integration",children:"Exercise 3: Analyze PaLM-E Multimodal Integration"}),"\n",(0,r.jsxs)(n.p,{children:["Read Section 3.2 of the PaLM-E paper (",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2303.03378",children:"PaLM-E: An Embodied Multimodal Language Model"}),")"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"How are visual tokens interleaved with text tokens in PaLM-E's input sequence?"}),"\n",(0,r.jsx)(n.li,{children:"What is the advantage of interleaving over concatenation (vision first, then text)?"}),"\n",(0,r.jsx)(n.li,{children:"Analyze Figure 5 (long-horizon planning example). How does PaLM-E handle execution failures mid-plan?"}),"\n",(0,r.jsx)(n.li,{children:"Why does PaLM-E use 562B parameters vs. RT-2's 55B? What capabilities require the extra scale?"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected time"}),": 50 minutes\n",(0,r.jsx)(n.strong,{children:"Learning goal"}),": Understand multimodal integration strategies for embodied AI."]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-4-reproduce-architecture-diagrams",children:"Exercise 4: Reproduce Architecture Diagrams"}),"\n",(0,r.jsx)(n.p,{children:"Using Mermaid or draw.io, create architecture diagrams for:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-1 full pipeline"}),": Camera \u2192 ResNet \u2192 Token Learner \u2192 Transformer \u2192 Actions"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Include tensor dimensions at each stage"}),"\n",(0,r.jsx)(n.li,{children:"Show Token Learner's spatial attention mechanism"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-2 co-fine-tuning"}),": Web data + robot data \u2192 PaLI-X \u2192 text OR actions"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Show branching between language tasks and robot tasks"}),"\n",(0,r.jsx)(n.li,{children:"Label pretrained components vs. fine-tuned components"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PaLM-E interleaved sequence"}),": Text + images \u2192 PaLM \u2192 planning"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Show example sequence with [TEXT] and [IMAGE] tokens"}),"\n",(0,r.jsx)(n.li,{children:"Illustrate closed-loop execution with replanning"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected time"}),": 60 minutes\n",(0,r.jsx)(n.strong,{children:"Learning goal"}),": Solidify understanding through visual representation."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-1 pioneered VLA models"})," by introducing token learner for efficient visual compression (19\xd719 \u2192 9\xd79 tokens), action discretization into 256 bins to prevent mode averaging, and training on 130K real-world robot demonstrations across 700 tasks, achieving 97% success on seen tasks and 76% on novel instructions."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RT-2 leveraged vision-language pretraining"}),' by co-fine-tuning PaLI-X (55B parameters) on 10B web image-text pairs and robot data, enabling emergent capabilities like symbolic reasoning ("pick extinct animal"), affordance understanding ("item to hammer a nail"), and improved generalization (79% on novel objects vs. 62% for RT-1).']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PaLM-E scaled to 562B parameters"}),' by integrating PaLM language model (540B) with ViT-22B vision encoder, supporting long-horizon planning (5-20 step tasks at 52-87% success), multimodal reasoning (answering "why" questions about physical states), and state estimation from visual observations.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Token learner reduces computational cost"})," by learning spatial attention over ResNet features to select 81 most task-relevant tokens instead of processing all 361 image patches, achieving 1000\xd7 reduction in FLOPs while preserving object locations, gripper pose, and scene context needed for manipulation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Action tokenization outperforms regression"})," by discretizing continuous robot actions into 256 bins per dimension and framing control as classification (cross-entropy loss), preventing mode averaging when multiple valid actions exist and improving training stability for high-dimensional action spaces (11D for RT-1)."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"VLM transfer enables emergent reasoning"}),' because vision-language models pretrained on internet data (e.g., "red apple is fruit") transfer semantic knowledge to robotics when fine-tuned on robot demonstrations, allowing RT-2 to generalize to unseen categories ("extinct animal" \u2192 dinosaur) without explicit training on category labels.']}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Architecture trade-offs span inference speed, generalization, and planning"})," with RT-1 optimized for edge deployment (5 Hz, 35M params), RT-2 balancing reasoning and speed (3 Hz, 55B params), and PaLM-E prioritizing capabilities over efficiency (0.5 Hz, 562B params), requiring different deployment strategies (edge vs. cloud hybrid vs. cloud-only)."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"navigation",children:"Navigation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous Chapter"}),": ",(0,r.jsx)(n.a,{href:"/docs/module-4-vla/intro-vla",children:"Introduction to VLA Models"}),"\n",(0,r.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,r.jsx)(n.a,{href:"/docs/module-4-vla/vla-training-deployment",children:"Training and Deploying VLA Systems"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);